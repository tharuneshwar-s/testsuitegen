====================
path: README.md

# TestSuiteGen Core Library

![Python](https://img.shields.io/badge/python-3.11+-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Pydantic](https://img.shields.io/badge/Pydantic-v2-E92063?style=for-the-badge&logo=pydantic&logoColor=white)
![Jinja2](https://img.shields.io/badge/jinja-3.0+-C63D14?style=for-the-badge&logo=jinja&logoColor=white)
![Tree-sitter](https://img.shields.io/badge/Tree%20Sitter-Parsing-43B02A?style=for-the-badge&logo=treesitter&logoColor=white)
![AST](https://img.shields.io/badge/Python%20AST-Parsing-306998?style=for-the-badge&logo=python&logoColor=white)
![Httpx](https://img.shields.io/badge/httpx-Async_HTTP-005571?style=for-the-badge)
![Google Gemini](https://img.shields.io/badge/Google%20Gemini-8E75B2?style=for-the-badge&logo=google%20gemini&logoColor=white)
![Groq](https://img.shields.io/badge/Groq-F55036?style=for-the-badge&logo=groq&logoColor=white)
![vLLM](https://img.shields.io/badge/vLLM-Supported-blue?style=for-the-badge)
![LM Studio](https://img.shields.io/badge/LM_Studio-Local-purple?style=for-the-badge)

The core logic engine for generating test suites. It handles parsing OpenAPI/Python specifications, generating Intermediate Representations (IR), enhancing payloads via LLMs, and rendering test code.

## Installation

1.  Navigate to the directory:

    ```bash
    cd testsuitegen
    ```

2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Folder Structure

```text
testsuitegen/
├── requirements.txt            # Project dependencies
├── src/                        # Source code
│   ├── config/
│   │   ├── settings.py         # Global settings & Enums (LLMProviders, API Keys)
│   │   └── __init__.py
│   ├── exceptions/
│   │   ├── exceptions.py       # Custom error classes (LLMError, ParsingError)
│   │   └── __init__.py
│   ├── generators/             # Core Generation Logic
│   │   ├── intent_generator/   # 1. Intent Analysis
│   │   │   ├── openapi_intent/ # OpenAPI specific strategies
│   │   │   │   ├── enums.py
│   │   │   │   └── generator.py
│   │   │   ├── python_intent/  # Python specific strategies
│   │   │   │   ├── enums.py
│   │   │   │   └── generator.py
│   │   │   ├── typescript_intent/ # TypeScript specific strategies
│   │   │   │   ├── enums.py
│   │   │   │   └── generator.py
│   │   │   └── generator.py    # Main Intent Strategy
│   │   ├── ir_generator/       # 2. Intermediate Representation
│   │   │   ├── builder.py      # IR Construction logic
│   │   │   └── validator.py    # Schema validation
│   │   └── payloads_generator/ # 3. Payload Generation
│   │       ├── openapi_mutator/
│   │       │   └── mutator.py
│   │       ├── python_mutator/
│   │       │   └── mutator.py
│   │       ├── typescript_mutator/
│   │       │   └── mutator.py
│   │       ├── generator.py
│   │       └── mutator.py      # Base mutator logic
│   ├── llm_enhancer/           # AI Enhancement Layer
│   │   ├── payload_enhancer/   # Data realism enhancement
│   │   │   ├── enhancer.py
│   │   │   ├── prompts.py
│   │   │   └── validator.py
│   │   ├── providers/          # LLM Integration
│   │   │   ├── airllm.py
│   │   │   ├── base.py
│   │   │   ├── config.py
│   │   │   ├── factory.py
│   │   │   ├── gemini.py
│   │   │   ├── groq.py
│   │   │   ├── lmstudio.py
│   │   │   ├── openrouter.py
│   │   │   └── vllm.py
│   │   ├── python_enhancer/    # Python Code Enhancement
│   │   │   ├── ir_enhancer/
│   │   │   │   ├── enhancer.py
│   │   │   │   └── prompts.py
│   │   │   └── test_suite_enhancer/
│   │   │       └── enhancer.py
│   │   ├── typescript_enhancer/# TypeScript Code Enhancement
│   │   │   ├── ir_enhancer/
│   │   │   │   └── enhancer.py
│   │   │   └── test_suite_enhancer/
│   │   │       └── enhancer.py
│   │   ├── circuit_breaker.py  # Error resilience
│   │   └── client.py           # Unified client wrapper
│   └── testsuite/              # 4. Final Code Rendering
│       ├── generator.py        # Orchestrates template rendering
│       └── templates.py        # Jinja2 templates (Pytest, Jest)
└── __init__.py
```

## Environment Configuration

Create a `.env` file in the project root (or ensure these variables are available).

| Variable                            | Description                      | Required        | Example                    |
| :---------------------------------- | :------------------------------- | :-------------- | :------------------------- |
| `GEMINI_API_KEY`                    | API Key for Google Gemini        | If using Gemini | `AIza...`                  |
| `OPENAI_API_KEY`                    | API Key for OpenAI               | If using OpenAI | `sk-...`                   |
| `GROQ_API_KEY`                      | API Key for Groq                 | If using Groq   | `gsk_...`                  |
| `LMSTUDIO_BASE_URL`                 | Base URL for LM Studio (Local)   | If using Local  | `http://localhost:1234/v1` |
| `VLLM_BASE_URL`                     | Base URL for VLLM (Local/Remote) | If using VLLM   | `http://localhost:8000/v1` |
| `AIRLLM_MODEL_PATH`                 | Path/Name for AirLLM Model       | If using AirLLM | `Qwen/Qwen2.5-Coder-1.5B`  |
| `CIRCUIT_BREAKER_FAILURE_THRESHOLD` | Failures before blocking LLM     | No (Default: 5) | `5`                        |
| `MAX_LLM_RETRIES`                   | Max retry attempts for LLM       | No (Default: 3) | `3`                        |
| `EXPONENTIAL_BACKOFF_BASE`          | Base delay for retries (seconds) | No (Default: 2) | `2`                        |
| `DEFAULT_OUTPUT_DIR`                | Directory for artifacts          | No              | `artifacts`                |

## Settings Configuration

The file `src/config/settings.py` controls the application behavior. Key values you might want to adjust include:

- **`LLMProviders` (Enum)**: Defines available providers and their default models/parameters (temperature, max_tokens).
- **`DEFAULT_LLM_PROVIDER`**: Logic to select the best available provider (Preference: Local > Cloud).
- **`CIRCUIT_BREAKER_FAILURE_THRESHOLD`**: Controls how sensitive the system is to LLM failures.
- **`EXPONENTIAL_BACKOFF_BASE`**: Base seconds for retry delays (2s, 4s, 8s...).

## Adding a New LLM Provider

To add a new LLM provider (e.g., "Anthropic"):

1.  **Create Provider Class**:
    Add a new file `src/llm_enhancer/providers/anthropic.py`. Use `src/llm_enhancer/providers/base.py` as a base class.

    ```python
    from .base import BaseLLMProvider
    class AnthropicProvider(BaseLLMProvider):
        def generate(self, prompt: str, **kwargs) -> str:
            # Implement API call logic
            pass
    ```

2.  **Register in Factory**:
    Update `src/llm_enhancer/providers/factory.py` to include the new provider in `create_provider`.

3.  **Update Settings**:
    In `src/config/settings.py`:
    - Add `ANTHROPIC` to the `LLMProviders` Enum with default config (model, tokens).
    - Add it to `get_by_name()` method.

4.  **Environment Variable**:
    Add the necessary API key (e.g., `ANTHROPIC_API_KEY`) to your `.env` and `settings.py` loading logic.

===========================

====================
path: requirements.txt

# requirements.txt
pyyaml
jsonschema
tree-sitter
tree-sitter-python
tree-sitter-javascript
tree-sitter-typescript
jinja2
black
python-dotenv
fastapi
requests
uvicorn
pydantic

# LLM Provider SDKs
google-generativeai
groq
openai
airllm
transformers==4.48.0
optimum==1.17.0 
vllm
===========================

====================
path: src\config\settings.py

"""
Application-wide configuration settings.

This module loads environment variables and provides configuration
constants for all components of the Test Suite Generator.
"""

import os
from enum import Enum
from pathlib import Path
from dotenv import load_dotenv
from typing import Optional

# Import ProviderConfig from providers module
from testsuitegen.src.llm_enhancer.providers.config import ProviderConfig


env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(dotenv_path=env_path)


class LLMProviders(Enum):
    """
    Enumeration of available LLM providers with their default configurations.

    Each provider includes:
    - Model selection
    - Temperature for deterministic output
    - Token limits
    - Timeout settings
    """

    GEMINI = ProviderConfig(
        name="gemini",
        model="gemini-2.0-flash",
        temperature=0.01,
        max_tokens=8000,
        timeout=90,
    )

    GROQ = ProviderConfig(
        name="groq",
        model="llama-3.3-70b-versatile",
        temperature=0.01,
        max_tokens=8000,
        timeout=90,
    )

    LMSTUDIO = ProviderConfig(
        name="lmstudio",
        model="meta-llama-3.1-8b-instruct",
        # model="versatile-llama-3-8b",
        temperature=0.001,
        max_tokens=4096 * 2,
        timeout=90,
        base_url="http://localhost:1234/v1",
    )
    VLLM = ProviderConfig(
        name="vllm",
        model="Qwen/Qwen2.5-Coder-1.5B-Instruct",
        temperature=0.01,
        max_tokens=8000,
        timeout=int(os.getenv("VLLM_TIMEOUT", "300")),
        base_url=os.getenv("VLLM_BASE_URL", ""),
    )
    AIRLLM = ProviderConfig(
        name="airllm",
        model="",
        temperature=0.01,
        max_tokens=8000,
        timeout=90,
    )

    @classmethod
    def get_by_name(cls, name: str) -> Optional["LLMProviders"]:
        """
        Get provider enum by name (case-insensitive).

        Args:
            name: Provider name string

        Returns:
            LLMProviders enum or None if not found
        """
        name = name.lower()
        for provider in cls:
            if provider.value.name == name:
                return provider
        return None

    @classmethod
    def get_available_providers(cls) -> list["LLMProviders"]:
        """
        Get list of all configured and available providers.

        Returns:
            List of LLMProviders that have valid API keys
        """
        return [p for p in cls if p.value.is_available]

    @classmethod
    def get_default_provider(cls) -> Optional["LLMProviders"]:
        """
        Get default provider based on availability and preference order.

        Preference: lmstudio > airllm > vllm > gemini > groq
        (Local models preferred over cloud for privacy/cost)

        Returns:
            First available provider in preference order, or None
        """
        preference_order = [
            cls.LMSTUDIO,
            cls.AIRLLM,
            cls.VLLM,
            cls.GEMINI,
            cls.GROQ,
        ]
        for provider in preference_order:
            if provider.value.is_available:
                return provider
        return None


# Default provider/model (mandatory LLM enabled by design)
DEFAULT_LLM_PROVIDER = LLMProviders.get_default_provider()
if DEFAULT_LLM_PROVIDER:
    DEFAULT_LLM_MODEL = DEFAULT_LLM_PROVIDER.value.model
else:
    DEFAULT_LLM_MODEL = None

# Global flag: Is LLM enhancement enabled? (any provider available)
LLM_ENABLED = len(LLMProviders.get_available_providers()) > 0


# ==============================================================================
# CIRCUIT BREAKER CONFIGURATION
# ==============================================================================

# Number of consecutive failures before circuit breaker trips
# This prevents cascading failures and saves API costs
CIRCUIT_BREAKER_FAILURE_THRESHOLD = int(
    os.getenv("CIRCUIT_BREAKER_FAILURE_THRESHOLD", "5")
)


# ==============================================================================
# RETRY CONFIGURATION
# ==============================================================================

# Maximum number of retry attempts for LLM calls
# Each retry uses exponential backoff
MAX_LLM_RETRIES = int(os.getenv("MAX_LLM_RETRIES", "3"))

# Base delay for exponential backoff (in seconds)
# Actual delays: base^1, base^2, base^3 (e.g., 2s, 4s, 8s)
EXPONENTIAL_BACKOFF_BASE = int(os.getenv("EXPONENTIAL_BACKOFF_BASE", "2"))


# ==============================================================================
# APPLICATION SETTINGS
# ==============================================================================

# Default output directory for generated artifacts
DEFAULT_OUTPUT_DIR = os.getenv("DEFAULT_OUTPUT_DIR", "artifacts")

# Default base URL for API testing
DEFAULT_BASE_URL = os.getenv("DEFAULT_BASE_URL", "http://localhost:8000")


# ==============================================================================
# CONFIGURATION SUMMARY (for debugging)
# ==============================================================================


def print_config_summary():
    """
    Print a summary of the current configuration.
    Useful for debugging and verification.
    """
    print("=" * 80)
    print("TEST SUITE GENERATOR - CONFIGURATION SUMMARY")
    print("=" * 80)
    print(f"Default LLM Provider: {DEFAULT_LLM_PROVIDER.value.name}")
    print(f"Default LLM Model: {DEFAULT_LLM_MODEL}")
    print(f"LLM Enhancement Enabled: {LLM_ENABLED}")

    available_providers = LLMProviders.get_available_providers()
    if available_providers:
        print(
            f"Available Providers: {', '.join(p.value.name for p in available_providers)}"
        )
        print(f"Default Provider: {DEFAULT_LLM_PROVIDER.value.name}")
        print(f"Model: {DEFAULT_LLM_PROVIDER.value.model}")
        print(f"Temperature: {DEFAULT_LLM_PROVIDER.value.temperature}")
        print(f"Max Tokens: {DEFAULT_LLM_PROVIDER.value.max_tokens}")
        print(f"Timeout: {DEFAULT_LLM_PROVIDER.value.timeout}s")
        if DEFAULT_LLM_PROVIDER.value.base_url:
            print(f"Base URL: {DEFAULT_LLM_PROVIDER.value.base_url}")
    else:
        print("Available Providers: None")
        print("No providers configured - check your .env file")
    print("-" * 80)
    print(f"Circuit Breaker Threshold: {CIRCUIT_BREAKER_FAILURE_THRESHOLD} failures")
    print(f"Max LLM Retries: {MAX_LLM_RETRIES}")
    print(f"Exponential Backoff Base: {EXPONENTIAL_BACKOFF_BASE}s")
    print("-" * 80)
    print(f"Default Output Directory: {DEFAULT_OUTPUT_DIR}")
    print(f"Default Base URL: {DEFAULT_BASE_URL}")
    print("=" * 80)

===========================

====================
path: src\config\__init__.py


===========================

====================
path: src\exceptions\exceptions.py

"""Custom Exception Classes for TestSuiteGen."""


class TestGenError(Exception):
    """Base class for all testsuitegen exceptions."""

    def __init__(self, message: str, details: str = None):
        self.message = message
        self.details = details
        super().__init__(self.message)

    def __str__(self):
        msg = f"Error: {self.message}"
        if self.details:
            msg += f"\n   Details: {self.details}"
        return msg


class FileError(TestGenError):
    """Raised when the input file cannot be accessed or is invalid."""

    pass


class InvalidSpecError(TestGenError):
    """Raised when the specification (OpenAPI/Python) is structurally invalid."""

    pass


class ValidationError(TestGenError):
    """Raised when the Intermediate Representation (IR) fails validation."""

    pass


class LLMError(TestGenError):
    """Raised when LLM operations fail."""

    pass


class LLMFatalError(LLMError):
    """Raised when an LLM error is non-retryable (e.g., policy or configuration).

    Use this to signal that retries will not help and enhancement should fallback
    immediately to non-LLM behavior.
    """

    pass


class ParsingError(TestGenError):
    """Raised when parsing operations fail."""

    pass

===========================

====================
path: src\exceptions\__init__.py


===========================

====================
path: src\generators\intent_generator\generator.py

from testsuitegen.src.generators.intent_generator.python_intent.generator import (
    PythonIntentGenerator,
)
from testsuitegen.src.generators.intent_generator.openapi_intent.generator import (
    IntentGenerator as OpenAPIIntentGenerator,
)
from testsuitegen.src.generators.intent_generator.typescript_intent.generator import (
    TypeScriptIntentGenerator,
)


def generate_intents(ir_operation: dict) -> list[dict]:
    """
    Main dispatcher for intent generation.
    Routes to appropriate generator based on operation kind.

    Args:
        ir_operation: The IR operation dict with 'kind' field

    Returns:
        List of intent dictionaries
    """
    kind = ir_operation.get("kind", "http")

    # Dispatch based on kind
    if kind == "function":
        generator = PythonIntentGenerator(ir_operation)
        return generator.generate()

    elif kind == "typescript_function":
        generator = TypeScriptIntentGenerator(ir_operation)
        return generator.generate()

    else:
        generator = OpenAPIIntentGenerator(ir_operation)
        return generator.generate()

===========================

====================
path: src\generators\intent_generator\schema.json

{
  "type": "object",
  "required": ["intent", "target", "expected"],
  "properties": {
    "intent": { "type": "string" },
    "target": { "type": "string" },
    "field": { "type": "string" },
    "expected": { "type": "string" },
    "notes": { "type": "string" }
  },
  "additionalProperties": false
}
===========================

====================
path: src\generators\intent_generator\__init__.py


===========================

====================
path: src\generators\intent_generator\openapi_intent\enums.py

from enum import Enum


class OpenAPISpecIntentType(str, Enum):
    # General intent types
    HAPPY_PATH = "HAPPY_PATH"

    # Field-related violations
    REQUIRED_FIELD_MISSING = "REQUIRED_FIELD_MISSING"
    NULL_NOT_ALLOWED = "NULL_NOT_ALLOWED"
    TYPE_VIOLATION = "TYPE_VIOLATION"
    # Path Parameter Semantics (404 vs 400)
    RESOURCE_NOT_FOUND = "RESOURCE_NOT_FOUND"  # Valid format, wrong ID -> 404
    FORMAT_INVALID_PATH_PARAM = "FORMAT_INVALID_PATH_PARAM"  # Invalid format -> 400
    # Array-related violations
    ARRAY_ITEM_TYPE_VIOLATION = "ARRAY_ITEM_TYPE_VIOLATION"
    ARRAY_SHAPE_VIOLATION = "ARRAY_SHAPE_VIOLATION"
    NESTED_ARRAY_ITEM_TYPE_VIOLATION = "NESTED_ARRAY_ITEM_TYPE_VIOLATION"

    # Union-related violations
    UNION_NO_MATCH = "UNION_NO_MATCH"

    # Property-related violations
    ADDITIONAL_PROPERTY_NOT_ALLOWED = "ADDITIONAL_PROPERTY_NOT_ALLOWED"
    OBJECT_VALUE_TYPE_VIOLATION = "OBJECT_VALUE_TYPE_VIOLATION"
    ARRAY_ITEM_OBJECT_VALUE_TYPE_VIOLATION = "ARRAY_ITEM_OBJECT_VALUE_TYPE_VIOLATION"

    # String constraint violations
    ENUM_MISMATCH = "ENUM_MISMATCH"
    STRING_TOO_SHORT = "STRING_TOO_SHORT"
    STRING_TOO_LONG = "STRING_TOO_LONG"
    PATTERN_MISMATCH = "PATTERN_MISMATCH"
    FORMAT_INVALID = "FORMAT_INVALID"

    # Numeric constraint violations
    NUMBER_TOO_SMALL = "NUMBER_TOO_SMALL"
    NUMBER_TOO_LARGE = "NUMBER_TOO_LARGE"
    NOT_MULTIPLE_OF = "NOT_MULTIPLE_OF"

    # Array constraint violations
    ARRAY_TOO_SHORT = "ARRAY_TOO_SHORT"
    ARRAY_TOO_LONG = "ARRAY_TOO_LONG"
    ARRAY_NOT_UNIQUE = "ARRAY_NOT_UNIQUE"

    # Object constraint violations
    OBJECT_TOO_FEW_PROPERTIES = "OBJECT_TOO_FEW_PROPERTIES"
    OBJECT_TOO_MANY_PROPERTIES = "OBJECT_TOO_MANY_PROPERTIES"

    # Precise Boundaries (Off-by-one)
    BOUNDARY_MIN_MINUS_ONE = "BOUNDARY_MIN_MINUS_ONE"  # e.g., min=10, send 9
    BOUNDARY_MAX_PLUS_ONE = "BOUNDARY_MAX_PLUS_ONE"  # e.g., max=10, send 11
    BOUNDARY_MIN_LENGTH_MINUS_ONE = "BOUNDARY_MIN_LENGTH_MINUS_ONE"
    BOUNDARY_MAX_LENGTH_PLUS_ONE = "BOUNDARY_MAX_LENGTH_PLUS_ONE"
    BOUNDARY_MIN_ITEMS_MINUS_ONE = "BOUNDARY_MIN_ITEMS_MINUS_ONE"
    BOUNDARY_MAX_ITEMS_PLUS_ONE = "BOUNDARY_MAX_ITEMS_PLUS_ONE"

    # Data Edge Cases
    EMPTY_STRING = "EMPTY_STRING"  # "" (Different from NULL)
    WHITESPACE_ONLY = "WHITESPACE_ONLY"  # "   "

    # Security Fuzzing
    SQL_INJECTION = "SQL_INJECTION"
    XSS_INJECTION = "XSS_INJECTION"
    COMMAND_INJECTION = "COMMAND_INJECTION"

    # Polymorphism & Logic Dependencies
    DISCRIMINATOR_VIOLATION = "DISCRIMINATOR_VIOLATION"  # Invalid discriminator value
    DEPENDENCY_VIOLATION = "DEPENDENCY_VIOLATION"  # Missing dependent field
    CONDITIONAL_REQUIRED_MISSING = "CONDITIONAL_REQUIRED_MISSING"  # Conditional logic: if A=1, B required but missing

    # Header Fuzzing
    HEADER_MISSING = "HEADER_MISSING"
    HEADER_ENUM_MISMATCH = "HEADER_ENUM_MISMATCH"
    HEADER_INJECTION = "HEADER_INJECTION"  # CRLF Injection


# ----- New: intents grouped by category -----
INTENTS_BY_CATEGORY = {
    "Functional": [
        OpenAPISpecIntentType.HAPPY_PATH.value,
    ],
    "Structure": [
        OpenAPISpecIntentType.REQUIRED_FIELD_MISSING.value,
        OpenAPISpecIntentType.NULL_NOT_ALLOWED.value,
        OpenAPISpecIntentType.TYPE_VIOLATION.value,
        OpenAPISpecIntentType.RESOURCE_NOT_FOUND.value,
        OpenAPISpecIntentType.FORMAT_INVALID_PATH_PARAM.value,
        OpenAPISpecIntentType.ADDITIONAL_PROPERTY_NOT_ALLOWED.value,
        OpenAPISpecIntentType.OBJECT_VALUE_TYPE_VIOLATION.value,
    ],
    "Constraints": [
        OpenAPISpecIntentType.ENUM_MISMATCH.value,
        OpenAPISpecIntentType.STRING_TOO_SHORT.value,
        OpenAPISpecIntentType.STRING_TOO_LONG.value,
        OpenAPISpecIntentType.PATTERN_MISMATCH.value,
        OpenAPISpecIntentType.NUMBER_TOO_SMALL.value,
        OpenAPISpecIntentType.NUMBER_TOO_LARGE.value,
        OpenAPISpecIntentType.NOT_MULTIPLE_OF.value,
        OpenAPISpecIntentType.BOUNDARY_MIN_MINUS_ONE.value,
        OpenAPISpecIntentType.BOUNDARY_MAX_PLUS_ONE.value,
    ],
    "Array": [
        OpenAPISpecIntentType.ARRAY_ITEM_TYPE_VIOLATION.value,
        OpenAPISpecIntentType.ARRAY_SHAPE_VIOLATION.value,
        OpenAPISpecIntentType.ARRAY_TOO_SHORT.value,
        OpenAPISpecIntentType.ARRAY_TOO_LONG.value,
        OpenAPISpecIntentType.ARRAY_NOT_UNIQUE.value,
    ],
    "Security": [
        OpenAPISpecIntentType.SQL_INJECTION.value,
        OpenAPISpecIntentType.XSS_INJECTION.value,
        OpenAPISpecIntentType.COMMAND_INJECTION.value,
        OpenAPISpecIntentType.HEADER_INJECTION.value,
    ],
}

# Flattened list and reverse map for quick lookup
ALL_OPENAPI_INTENTS = [i for cat in INTENTS_BY_CATEGORY.values() for i in cat]
INTENT_TO_CATEGORY = {
    intent: cat for cat, lst in INTENTS_BY_CATEGORY.items() for intent in lst
}

===========================

====================
path: src\generators\intent_generator\openapi_intent\generator.py

from testsuitegen.src.generators.intent_generator.openapi_intent.enums import (
    OpenAPISpecIntentType,
)


class IntentGenerator:
    """
    Intent generator.
    Generates test intents covering:
    - Headers, Path params, Query params, Body fields
    - Boundaries, Constraints, Security fuzzing
    - Polymorphism (discriminators), Logic dependencies
    """

    def __init__(self, ir: dict):
        self.ir = ir
        self.op_id = ir["id"]
        self.inputs = ir["inputs"]
        self.body = self.inputs.get("body")
        self.schema = self.body["schema"] if self.body else None

        self.OK = self._get_success_status()
        self.ERR = self._get_error_status()

        self.intents = []

    def generate(self) -> list[dict]:
        """Main entry point - generates all intents and returns deduplicated list."""
        self._emit_happy_path()
        self._process_headers()
        self._process_path_params()
        self._process_query_params()

        if self.schema:
            self._process_body_schema()

        return self._deduplicate()

    # ========== STATUS CODE HELPERS ==========

    def _get_success_status(self) -> str:
        """Determine the success status code from outputs."""
        for output in self.ir.get("outputs", []):
            if output["status"] in range(200, 301):
                return str(output["status"])
        return "200"

    def _get_error_status(self) -> str:
        """Determine the error status code from errors."""
        for error in self.ir.get("errors", []):
            if error["status"] in range(400, 501):
                return str(error["status"])
        return "422"

    # ========== INTENT EMITTER ==========

    def _determine_expected_status(self, intent_type: str, location: str) -> str:
        """
        Intelligently derives expected HTTP status based on context.
        Uses the OpenAPI spec's defined error status for validation errors.

        Args:
            intent_type: The type of intent being generated
            location: Where the parameter is (path/query/header/body)

        Returns:
            Expected HTTP status code as string
        """
        # 1. Path Parameters -> 404 Not Found
        if location == "path":
            return "404"

        # 2. Auth/Headers -> 400 Bad Request
        if location == "header":
            return "400"

        return self.ERR

    def _is_security_test_applicable(self, prop: dict) -> bool:
        """
        Prevents Intent Explosion by filtering security tests.
        Only tests injection if the field is 'open' enough to accept malicious input.

        Args:
            prop: The property schema

        Returns:
            True if security testing is applicable, False otherwise
        """
        # 1. Skip if Enum (Input strictly validated against a list)
        if "enum" in prop:
            return False

        # 2. Skip if UUID/Date (Strict format validation blocks injection)
        if prop.get("format") in ["uuid", "date", "date-time", "ipv4", "ipv6"]:
            return False

        # 3. Skip if very short maxLength (Injection payloads won't fit)
        # Typical SQLi is ~10-15 chars minimum to be useful
        if prop.get("maxLength", 100) < 10:
            return False

        # 4. Skip if strict pattern (Regex likely blocks injection)
        if "pattern" in prop:
            return False

        return True

    def _emit(
        self,
        intent: str,
        target: str,
        field: str = None,
        expected: str = None,
        notes: str = None,
    ) -> None:
        """Adds an intent to the list with intelligent status code determination."""
        # Determine location from target string
        location = "body"
        if ".path." in target:
            location = "path"
        elif ".query." in target:
            location = "query"
        elif ".headers." in target:
            location = "header"

        # Auto-calculate status if not provided
        if not expected:
            expected = self._determine_expected_status(intent, location)

        self.intents.append(
            {
                "operation_id": self.op_id,
                "intent": intent,
                "target": target,
                "expected": expected,
                **({"field": field} if field else {}),
                **({"notes": notes} if notes else {}),
            }
        )

    # ========== HAPPY PATH ==========

    def _emit_happy_path(self) -> None:
        """Emits happy path intent(s), with polymorphic variant support."""
        base = f"{self.op_id}.inputs.body" if self.body else f"{self.op_id}.inputs"

        # 1. Check Root Polymorphism
        if self.schema and "oneOf" in self.schema:
            # Generate a Happy Path for EACH variant
            for idx, variant in enumerate(self.schema["oneOf"]):
                self._emit(
                    OpenAPISpecIntentType.HAPPY_PATH.value,
                    base,
                    notes=f"Root Variant {idx}",
                    expected=self.OK,
                )
            return

        # 2. Check Nested Polymorphism
        if self.schema and "properties" in self.schema:
            for name, prop in self.schema["properties"].items():
                if "oneOf" in prop:
                    # Found nested polymorphism - generate variant for each option
                    for idx, variant in enumerate(prop["oneOf"]):
                        # Try to extract discriminator value for better naming
                        desc = f"Variant {idx}"
                        # If variant has a discriminator field with enum, use it
                        if "properties" in variant and "mode" in variant["properties"]:
                            mode_enum = variant["properties"]["mode"].get("enum", [])
                            if mode_enum:
                                desc = f"Variant: {mode_enum[0]}"

                        self._emit(
                            OpenAPISpecIntentType.HAPPY_PATH.value,
                            base,
                            notes=f"Nested {name} -> {desc}",
                            expected=self.OK,
                        )
                    return  # Stop after finding the first major polymorphic switch

        # 3. Fallback - Standard Happy Path
        self._emit(
            OpenAPISpecIntentType.HAPPY_PATH.value,
            base,
            expected=self.OK,
            notes="Standard Valid Request",
        )

    # ========== HEADER ANALYSIS ==========

    def _process_headers(self) -> None:
        """Processes header parameters for required, enum, and injection tests."""
        for header in self.inputs.get("headers", []):
            header_path = f"{self.op_id}.inputs.headers.{header['name']}"
            header_schema = header.get("schema", {})

            # Required header missing
            if header.get("required"):
                self._emit(
                    OpenAPISpecIntentType.HEADER_MISSING.value,
                    header_path,
                    field=header["name"],
                )

            # CRLF injection test - only for strings without enum constraints
            if header_schema.get("type") == "string" and "enum" not in header_schema:
                self._emit(
                    OpenAPISpecIntentType.HEADER_INJECTION.value,
                    header_path,
                    field=header["name"],
                )

            # Enum mismatch
            if "enum" in header_schema:
                self._emit(
                    OpenAPISpecIntentType.HEADER_ENUM_MISMATCH.value,
                    header_path,
                    field=header["name"],
                )

            # CRLF injection test
            self._emit(
                OpenAPISpecIntentType.HEADER_INJECTION.value,
                header_path,
                field=header["name"],
            )

            # Enum mismatch
            if "enum" in header_schema:
                self._emit(
                    OpenAPISpecIntentType.HEADER_ENUM_MISMATCH.value,
                    header_path,
                    field=header["name"],
                )

    # ========== PATH PARAMETER ANALYSIS ==========

    def _process_path_params(self) -> None:
        """Processes path parameters for type, boundary, pattern, and security tests."""
        for param in self.inputs.get("path", []):
            param_path = f"{self.op_id}.inputs.path.{param['name']}"
            param_schema = param.get("schema", {})

            # 1. Resource Not Found (Valid format, wrong ID) -> 404
            self._emit(
                OpenAPISpecIntentType.RESOURCE_NOT_FOUND.value,
                param_path,
                field=param["name"],
                expected="404",
                notes="Valid format, nonexistent resource",
            )

            # 2. Format Invalid (Invalid format) -> 400
            # Only if format is specified (uuid, int, etc.)
            if param_schema.get("format") or param_schema.get("type") in [
                "integer",
                "number",
            ]:
                self._emit(
                    OpenAPISpecIntentType.FORMAT_INVALID_PATH_PARAM.value,
                    param_path,
                    field=param["name"],
                    expected="400",
                    notes="Invalid format",
                )

            # String-specific constraints
            if param_schema.get("type") == "string":
                self._process_string_path_param(param_path, param["name"], param_schema)

            # Numeric-specific constraints
            if param_schema.get("type") in ["integer", "number"]:
                self._process_numeric_path_param(
                    param_path, param["name"], param_schema
                )

    def _process_string_path_param(self, path: str, name: str, schema: dict) -> None:
        """Processes string path parameter constraints."""
        # Pattern mismatch
        if "pattern" in schema:
            self._emit(
                OpenAPISpecIntentType.PATTERN_MISMATCH.value,
                path,
                field=name,
                notes=f"Pattern: {schema['pattern']}",
            )

        # Length boundaries
        if "minLength" in schema:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MIN_LENGTH_MINUS_ONE.value,
                path,
                field=name,
                notes=f"Len: {schema['minLength']}-1",
            )

        if "maxLength" in schema:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MAX_LENGTH_PLUS_ONE.value,
                path,
                field=name,
                notes=f"Len: {schema['maxLength']}+1",
            )

        # Security fuzzing (skip for UUIDs and dates)
        if schema.get("format") not in ["uuid", "date", "date-time"]:
            self._emit(OpenAPISpecIntentType.SQL_INJECTION.value, path, field=name)
            self._emit(OpenAPISpecIntentType.XSS_INJECTION.value, path, field=name)

    def _process_numeric_path_param(self, path: str, name: str, schema: dict) -> None:
        """Processes numeric path parameter constraints."""
        if "minimum" in schema:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MIN_MINUS_ONE.value,
                path,
                field=name,
                notes=f"Val: {schema['minimum']}-1",
            )

        if "maximum" in schema:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MAX_PLUS_ONE.value,
                path,
                field=name,
                notes=f"Val: {schema['maximum']}+1",
            )

    # ========== QUERY PARAMETER ANALYSIS ==========

    def _process_query_params(self) -> None:
        """Processes query parameters for required and type violations."""
        for param in self.inputs.get("query", []):
            param_path = f"{self.op_id}.inputs.query.{param['name']}"
            param_schema = param.get("schema", {})

            # Required field missing
            if param.get("required"):
                self._emit(
                    OpenAPISpecIntentType.REQUIRED_FIELD_MISSING.value,
                    param_path,
                    field=param["name"],
                )

            # Type violation - only for types where sending wrong type causes validation error
            # String fields accept any string, so TYPE_VIOLATION doesn't apply
            param_type = param_schema.get("type")
            if param_type in ["integer", "number", "boolean"] or "enum" in param_schema:
                self._emit(
                    OpenAPISpecIntentType.TYPE_VIOLATION.value,
                    param_path,
                    field=param["name"],
                )

    # ========== BODY SCHEMA ANALYSIS ==========

    def _process_body_schema(self) -> None:
        """Processes the request body schema for all constraints and validations."""
        base = f"{self.op_id}.inputs.body"

        # Required fields
        self._process_required_fields(base)

        # Schema dependencies
        self._process_schema_dependencies(base)

        # Property-level constraints (recursive)
        self._process_properties(self.schema.get("properties", {}), base)

        # Root-level constraints
        self._process_root_constraints(base)

    def _process_required_fields(self, base: str) -> None:
        """Emits intents for required field violations."""
        for required_field in self.schema.get("required", []):
            self._emit(
                OpenAPISpecIntentType.REQUIRED_FIELD_MISSING.value,
                f"{base}.{required_field}",
                field=required_field,
            )

    def _process_schema_dependencies(self, base: str) -> None:
        """Processes OpenAPI schema dependencies (field X requires field Y)."""
        dependencies = self.schema.get("dependencies", {})
        for field, deps in dependencies.items():
            if isinstance(deps, list):
                for dep in deps:
                    self._emit(
                        OpenAPISpecIntentType.DEPENDENCY_VIOLATION.value,
                        base,
                        field=f"{field}_requires_{dep}",
                        notes=f"Field {field} requires {dep}",
                    )

    def _process_conditional_dependencies(self, base: str) -> None:
        """Processes conditional dependencies (if field A exists, field B is required)."""
        # Support both OpenAPI 3.0 'dependencies' and 3.1 'dependentRequired'
        deps = self.schema.get("dependencies", {}) or self.schema.get(
            "dependentRequired", {}
        )

        for field, requirements in deps.items():
            if isinstance(requirements, list):
                for req_field in requirements:
                    self._emit(
                        OpenAPISpecIntentType.CONDITIONAL_REQUIRED_MISSING.value,
                        base,
                        field=f"{field}_requires_{req_field}",
                        notes=f"When {field} is present, {req_field} is required",
                    )

    def _process_root_constraints(self, base: str) -> None:
        """Processes root-level object constraints."""
        # Additional properties
        if self.schema.get("additionalProperties") is False:
            self._emit(
                OpenAPISpecIntentType.ADDITIONAL_PROPERTY_NOT_ALLOWED.value, base
            )

        # Min/max properties
        if "minProperties" in self.schema:
            self._emit(
                OpenAPISpecIntentType.OBJECT_TOO_FEW_PROPERTIES.value,
                base,
                notes=f"Min properties: {self.schema['minProperties']}",
            )

        if "maxProperties" in self.schema:
            self._emit(
                OpenAPISpecIntentType.OBJECT_TOO_MANY_PROPERTIES.value,
                base,
                notes=f"Max properties: {self.schema['maxProperties']}",
            )

    # ========== RECURSIVE PROPERTY ANALYSIS ==========

    def _process_properties(self, properties: dict, parent_path: str) -> None:
        """Recursively processes properties and their constraints."""
        for name, prop in properties.items():
            field_path = f"{parent_path}.{name}"

            # Common validations
            self._process_common_validations(field_path, name, prop)

            # Type-specific validations
            prop_type = prop.get("type")

            if prop_type == "string":
                self._process_string_field(field_path, name, prop)
            elif prop_type in ["integer", "number"]:
                self._process_numeric_field(field_path, name, prop)
            elif prop_type == "array":
                self._process_array_field(field_path, name, prop)
            elif prop_type == "object":
                self._process_object_field(field_path, name, prop)

    def _process_common_validations(
        self, field_path: str, name: str, prop: dict
    ) -> None:
        """Processes validations common to all field types."""
        # Nullability
        if prop.get("nullable") is False:
            self._emit(
                OpenAPISpecIntentType.NULL_NOT_ALLOWED.value, field_path, field=name
            )

        # Type violation - only for types where sending wrong type causes validation error
        # String fields accept any string, so TYPE_VIOLATION doesn't apply to them
        prop_type = prop.get("type")
        if prop_type in ["integer", "number", "boolean"] or "enum" in prop:
            self._emit(OpenAPISpecIntentType.TYPE_VIOLATION.value, field_path, field=name)

        # Union types
        if "oneOf" in prop:
            self._emit(
                OpenAPISpecIntentType.UNION_NO_MATCH.value, field_path, field=name
            )

        # Polymorphic discriminator
        if "oneOf" in prop or "anyOf" in prop:
            self._process_discriminator(field_path, prop)

    def _process_discriminator(self, field_path: str, prop: dict) -> None:
        """Processes polymorphic discriminator if present."""
        discriminator = prop.get("discriminator")
        if discriminator and "propertyName" in discriminator:
            disc_prop = discriminator["propertyName"]
            self._emit(
                OpenAPISpecIntentType.DISCRIMINATOR_VIOLATION.value,
                f"{field_path}.{disc_prop}",
                field=disc_prop,
                notes="Invalid discriminator value",
            )

    # ========== STRING FIELD PROCESSING ==========

    def _process_string_field(self, field_path: str, name: str, prop: dict) -> None:
        """Processes string-specific constraints and validations."""
        # Enum
        if "enum" in prop:
            self._emit(
                OpenAPISpecIntentType.ENUM_MISMATCH.value,
                field_path,
                field=name,
                notes=f"Value not in enum: {prop['enum']}",
            )

        # Length boundaries
        self._process_string_length_boundaries(field_path, name, prop)

        # Pattern
        if "pattern" in prop:
            self._emit(
                OpenAPISpecIntentType.PATTERN_MISMATCH.value,
                field_path,
                field=name,
                notes=f"Pattern: {prop['pattern']}",
            )

        # Format
        if "format" in prop:
            self._emit(
                OpenAPISpecIntentType.FORMAT_INVALID.value,
                field_path,
                field=name,
                notes=f"Format: {prop['format']}",
            )

        # Security fuzzing
        self._process_string_security_tests(field_path, name, prop)

    def _process_string_length_boundaries(
        self, field_path: str, name: str, prop: dict
    ) -> None:
        """Processes minLength and maxLength boundaries."""
        if "minLength" in prop:
            if prop["minLength"] > 0:
                self._emit(
                    OpenAPISpecIntentType.BOUNDARY_MIN_LENGTH_MINUS_ONE.value,
                    field_path,
                    field=name,
                    notes=f"Len: {prop['minLength']}-1",
                )
            else:
                self._emit(
                    OpenAPISpecIntentType.EMPTY_STRING.value, field_path, field=name
                )
        elif prop.get("nullable") is not False:
            self._emit(OpenAPISpecIntentType.EMPTY_STRING.value, field_path, field=name)

        if "maxLength" in prop:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MAX_LENGTH_PLUS_ONE.value,
                field_path,
                field=name,
                notes=f"Len: {prop['maxLength']}+1",
            )

    def _process_string_security_tests(
        self, field_path: str, name: str, prop: dict
    ) -> None:
        """Processes security fuzzing tests for strings with noise reduction."""
        # Use intelligent filter to avoid test explosion
        if self._is_security_test_applicable(prop):
            self._emit(
                OpenAPISpecIntentType.SQL_INJECTION.value, field_path, field=name
            )
            self._emit(
                OpenAPISpecIntentType.XSS_INJECTION.value, field_path, field=name
            )
            self._emit(
                OpenAPISpecIntentType.WHITESPACE_ONLY.value, field_path, field=name
            )

    # ========== NUMERIC FIELD PROCESSING ==========

    def _process_numeric_field(self, field_path: str, name: str, prop: dict) -> None:
        """Processes numeric-specific constraints (integer/number)."""
        # Minimum boundary
        if "minimum" in prop:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MIN_MINUS_ONE.value,
                field_path,
                field=name,
                notes=f"Boundary: {prop['minimum']} - 1",
            )

        # Maximum boundary
        if "maximum" in prop:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MAX_PLUS_ONE.value,
                field_path,
                field=name,
                notes=f"Boundary: {prop['maximum']} + 1",
            )

        # Multiple of
        if "multipleOf" in prop:
            self._emit(
                OpenAPISpecIntentType.NOT_MULTIPLE_OF.value,
                field_path,
                field=name,
                notes=f"Multiple of: {prop['multipleOf']}",
            )

    # ========== ARRAY FIELD PROCESSING ==========

    def _process_array_field(self, field_path: str, name: str, prop: dict) -> None:
        """Processes array-specific constraints and nested items."""
        # Array item type violation
        self._emit(OpenAPISpecIntentType.ARRAY_ITEM_TYPE_VIOLATION.value, field_path)

        # Item count boundaries
        if "minItems" in prop:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MIN_ITEMS_MINUS_ONE.value,
                field_path,
                field=name,
                notes=f"Items: {prop['minItems']}-1",
            )

        if "maxItems" in prop:
            self._emit(
                OpenAPISpecIntentType.BOUNDARY_MAX_ITEMS_PLUS_ONE.value,
                field_path,
                field=name,
                notes=f"Items: {prop['maxItems']}+1",
            )

        # Unique items
        if prop.get("uniqueItems"):
            self._emit(
                OpenAPISpecIntentType.ARRAY_NOT_UNIQUE.value,
                field_path,
                field=name,
                notes="Array must have unique items",
            )

        # Process array items
        self._process_array_items(field_path, prop.get("items", {}))

    def _process_array_items(self, field_path: str, items: dict) -> None:
        """Processes array item schemas recursively."""
        item_path = f"{field_path}[]"

        # Union in items
        if "oneOf" in items:
            self._emit(OpenAPISpecIntentType.UNION_NO_MATCH.value, item_path)

        # Object items
        if items.get("type") == "object":
            self._emit(
                OpenAPISpecIntentType.OBJECT_VALUE_TYPE_VIOLATION.value, item_path
            )

            # Recurse into object properties
            if "properties" in items:
                self._process_properties(items["properties"], item_path)

            # Nested arrays in object items
            for key, value in items.get("properties", {}).items():
                if value.get("type") == "array":
                    self._emit(
                        OpenAPISpecIntentType.ARRAY_ITEM_TYPE_VIOLATION.value,
                        f"{item_path}.{key}",
                    )
                    if "oneOf" in value.get("items", {}):
                        self._emit(
                            OpenAPISpecIntentType.UNION_NO_MATCH.value,
                            f"{item_path}.{key}[]",
                        )

    # ========== OBJECT FIELD PROCESSING ==========

    def _process_object_field(self, field_path: str, name: str, prop: dict) -> None:
        """Processes nested object properties and additionalProperties."""
        # Recurse into nested properties
        if "properties" in prop:
            self._process_properties(prop["properties"], field_path)

        # Additional properties
        if "additionalProperties" in prop:
            self._process_additional_properties(
                field_path, prop["additionalProperties"]
            )

    def _process_additional_properties(self, field_path: str, additional_props) -> None:
        """Processes additionalProperties schema."""
        # Only process if it's a schema (dict), not a boolean
        if not isinstance(additional_props, dict):
            return

        self._emit(OpenAPISpecIntentType.OBJECT_VALUE_TYPE_VIOLATION.value, field_path)

        # Union in additional properties
        if "oneOf" in additional_props:
            self._emit(OpenAPISpecIntentType.UNION_NO_MATCH.value, f"{field_path}.*")

        # Array in additional properties
        elif additional_props.get("type") == "array":
            self._emit(
                OpenAPISpecIntentType.ARRAY_ITEM_TYPE_VIOLATION.value, f"{field_path}.*"
            )

            # Object in array items
            ap_items = additional_props.get("items", {})
            if ap_items.get("type") == "object":
                self._emit(
                    OpenAPISpecIntentType.OBJECT_VALUE_TYPE_VIOLATION.value,
                    f"{field_path}.*[]",
                )

    # ========== DEDUPLICATION ==========

    def _deduplicate(self) -> list[dict]:
        """Removes duplicate intents based on (intent, target) combination."""
        seen = set()
        final = []

        for intent in self.intents:
            key = (intent["intent"], intent["target"])
            if key not in seen:
                seen.add(key)
                final.append(intent)

        return final


def generate_intents(ir: dict) -> list[dict]:
    """
    Main entry point for intent generation.
    Maintains backward compatibility with function-based API.
    """
    generator = IntentGenerator(ir)
    return generator.generate()

===========================

====================
path: src\generators\intent_generator\openapi_intent\__init__.py


===========================

====================
path: src\generators\intent_generator\python_intent\enums.py

from enum import Enum


class PythonIntentType(str, Enum):
    # --- 1. Structural (The Contract) ---
    HAPPY_PATH = "HAPPY_PATH"
    REQUIRED_ARG_MISSING = "REQUIRED_ARG_MISSING"
    UNEXPECTED_ARGUMENT = "UNEXPECTED_ARGUMENT"
    TOO_MANY_POS_ARGS = "TOO_MANY_POS_ARGS"

    # --- 2. Type System (Static Typing) ---
    TYPE_VIOLATION = "TYPE_VIOLATION"
    NULL_NOT_ALLOWED = "NULL_NOT_ALLOWED"
    ARRAY_ITEM_TYPE_VIOLATION = "ARRAY_ITEM_TYPE_VIOLATION"
    DICT_KEY_TYPE_VIOLATION = "DICT_KEY_TYPE_VIOLATION"
    DICT_VALUE_TYPE_VIOLATION = "DICT_VALUE_TYPE_VIOLATION"
    UNION_NO_MATCH = "UNION_NO_MATCH"

    # --- 3. Constraints & Boundaries (The Logic) ---
    BOUNDARY_MIN_MINUS_ONE = "BOUNDARY_MIN_MINUS_ONE"
    BOUNDARY_MAX_PLUS_ONE = "BOUNDARY_MAX_PLUS_ONE"
    STRING_TOO_SHORT = "STRING_TOO_SHORT"
    STRING_TOO_LONG = "STRING_TOO_LONG"
    PATTERN_MISMATCH = "PATTERN_MISMATCH"
    ENUM_MISMATCH = "ENUM_MISMATCH"
    NOT_MULTIPLE_OF = "NOT_MULTIPLE_OF"

    # --- 4. Data Edge Cases (Robustness) ---
    EMPTY_STRING = "EMPTY_STRING"
    WHITESPACE_ONLY = "WHITESPACE_ONLY"
    ZERO_VALUE = "ZERO_VALUE"
    NEGATIVE_VALUE = "NEGATIVE_VALUE"
    EMPTY_COLLECTION = "EMPTY_COLLECTION"

    # --- 5. Complex Objects ---
    OBJECT_MISSING_FIELD = "OBJECT_MISSING_FIELD"
    OBJECT_EXTRA_FIELD = "OBJECT_EXTRA_FIELD"

    # --- 6. Security Fuzzing ---
    SQL_INJECTION = "SQL_INJECTION"
    XSS_INJECTION = "XSS_INJECTION"
    PATH_TRAVERSAL = "PATH_TRAVERSAL"
    COMMAND_INJECTION = "COMMAND_INJECTION"

    # --- 7. Python Runtime Specifics ---
    MUTABLE_DEFAULT_TRAP = "MUTABLE_DEFAULT_TRAP"


# ----- New: intents grouped by category -----
INTENTS_BY_CATEGORY = {
    "Functional": [
        PythonIntentType.HAPPY_PATH.value,
    ],
    "Structure": [
        PythonIntentType.REQUIRED_ARG_MISSING.value,
        PythonIntentType.UNEXPECTED_ARGUMENT.value,
        PythonIntentType.TOO_MANY_POS_ARGS.value,
        PythonIntentType.OBJECT_MISSING_FIELD.value,
        PythonIntentType.OBJECT_EXTRA_FIELD.value,
    ],
    "Type": [
        PythonIntentType.TYPE_VIOLATION.value,
        PythonIntentType.NULL_NOT_ALLOWED.value,
        PythonIntentType.ARRAY_ITEM_TYPE_VIOLATION.value,
        PythonIntentType.DICT_KEY_TYPE_VIOLATION.value,
        PythonIntentType.DICT_VALUE_TYPE_VIOLATION.value,
        PythonIntentType.UNION_NO_MATCH.value,
    ],
    "Constraints": [
        PythonIntentType.BOUNDARY_MIN_MINUS_ONE.value,
        PythonIntentType.BOUNDARY_MAX_PLUS_ONE.value,
        PythonIntentType.STRING_TOO_SHORT.value,
        PythonIntentType.STRING_TOO_LONG.value,
        PythonIntentType.PATTERN_MISMATCH.value,
        PythonIntentType.ENUM_MISMATCH.value,
        PythonIntentType.NOT_MULTIPLE_OF.value,
    ],
    "Robustness": [
        PythonIntentType.EMPTY_STRING.value,
        PythonIntentType.WHITESPACE_ONLY.value,
        PythonIntentType.ZERO_VALUE.value,
        PythonIntentType.NEGATIVE_VALUE.value,
        PythonIntentType.EMPTY_COLLECTION.value,
    ],
    "Security": [
        PythonIntentType.SQL_INJECTION.value,
        PythonIntentType.XSS_INJECTION.value,
        PythonIntentType.PATH_TRAVERSAL.value,
        PythonIntentType.COMMAND_INJECTION.value,
    ],
    "Runtime": [
        PythonIntentType.MUTABLE_DEFAULT_TRAP.value,
    ],
}

# Flattened list and reverse map for quick lookup
ALL_PYTHON_INTENTS = [i for cat in INTENTS_BY_CATEGORY.values() for i in cat]
INTENT_TO_CATEGORY = {
    intent: cat for cat, lst in INTENTS_BY_CATEGORY.items() for intent in lst
}

===========================

====================
path: src\generators\intent_generator\python_intent\generator.py

from testsuitegen.src.generators.intent_generator.python_intent.enums import (
    PythonIntentType,
)


class PythonIntentGenerator:
    """
    Generates test intents for Python functions based on their IR schema.
    """

    def __init__(self, ir_operation: dict):
        self.op = ir_operation
        self.op_id = ir_operation["id"]
        # In Python IR, arguments are mapped to inputs.body.schema
        self.body = ir_operation["inputs"].get("body", {})
        self.schema = self.body.get("schema", {})
        self.intents = []

    def generate(self) -> list[dict]:
        if not self.schema:
            return []

        # 1. Happy Path
        self._emit_happy_path()

        # 2. Argument Analysis
        properties = self.schema.get("properties", {})
        self._process_arguments(properties, parent_path="body")

        # 3. Structural Limits
        if self.schema.get("additionalProperties") is False:
            self._emit(PythonIntentType.UNEXPECTED_ARGUMENT, "body", expected="400")

        return self.intents

    def _emit(self, intent, target, field=None, expected="422", notes=None):
        """
        Emits an intent.
        Note on 'expected':
        - 200: Success / Return Value
        - 400: TypeError (Structural/Missing args)
        - 422: ValueError (Constraint violation)
        """
        self.intents.append(
            {
                "operation_id": self.op_id,
                "intent": intent,
                "target": target,
                "expected": expected,
                **({"field": field} if field else {}),
                **({"notes": notes} if notes else {}),
            }
        )

    def _emit_happy_path(self):
        self._emit(
            PythonIntentType.HAPPY_PATH, "body", expected="200", notes="Valid arguments"
        )

    def _process_arguments(self, properties: dict, parent_path: str):
        required_fields = self.schema.get("required", [])

        for name, prop in properties.items():
            field_path = f"{parent_path}.{name}"

            # --- 1. Structural Checks ---
            if name in required_fields:
                self._emit(
                    PythonIntentType.REQUIRED_ARG_MISSING,
                    field_path,
                    field=name,
                    expected="400",
                )

            # --- 2. Type Checks ---
            # TYPE_VIOLATION only applies to types where sending wrong type causes error
            # String fields accept any string, so TYPE_VIOLATION doesn't apply
            prop_type = prop.get("type")
            if prop_type in ["integer", "number", "boolean"] or "enum" in prop:
                self._emit(
                    PythonIntentType.TYPE_VIOLATION,
                    field_path,
                    field=name,
                    expected="400",
                )

            if not prop.get("nullable", False):
                self._emit(
                    PythonIntentType.NULL_NOT_ALLOWED,
                    field_path,
                    field=name,
                    expected="400",
                )

            # --- 3. Numeric Constraints ---
            if prop.get("type") in ["integer", "number"]:
                if "minimum" in prop:
                    self._emit(
                        PythonIntentType.BOUNDARY_MIN_MINUS_ONE, field_path, field=name
                    )
                    # Also test negative if min is 0 or positive
                    if prop["minimum"] >= 0:
                        self._emit(
                            PythonIntentType.NEGATIVE_VALUE, field_path, field=name
                        )

                if "maximum" in prop:
                    self._emit(
                        PythonIntentType.BOUNDARY_MAX_PLUS_ONE, field_path, field=name
                    )

                if "multipleOf" in prop:
                    self._emit(PythonIntentType.NOT_MULTIPLE_OF, field_path, field=name)

                # Edge case: Zero
                
                if prop.get("minimum", 0) > 0 or prop.get("exclusiveMinimum", False):
                    self._emit(PythonIntentType.ZERO_VALUE, field_path, field=name)

            # --- 4. String Constraints ---
            if prop.get("type") == "string":
                # Check explicitly for enum list OR x-enum-type marker
                if "enum" in prop or "x-enum-type" in prop:
                    self._emit(PythonIntentType.ENUM_MISMATCH, field_path, field=name)

                if "minLength" in prop:
                    self._emit(
                        PythonIntentType.STRING_TOO_SHORT, field_path, field=name
                    )
                    self._emit(PythonIntentType.EMPTY_STRING, field_path, field=name)

                if "maxLength" in prop:
                    self._emit(PythonIntentType.STRING_TOO_LONG, field_path, field=name)

                if "pattern" in prop:
                    self._emit(
                        PythonIntentType.PATTERN_MISMATCH, field_path, field=name
                    )

                # Security Fuzzing (Skip if strict format/enum exists)
                if (
                    not prop.get("enum")
                    and "x-enum-type" not in prop
                    and prop.get("format")
                    not in [
                        "uuid",
                        "date",
                        "date-time",
                    ]
                ):
                    self._emit(PythonIntentType.SQL_INJECTION, field_path, field=name)
                    self._emit(PythonIntentType.XSS_INJECTION, field_path, field=name)
                    self._emit(PythonIntentType.WHITESPACE_ONLY, field_path, field=name)

            # --- 5. Collections (Arrays) ---
            if prop.get("type") == "array":
                self._emit(
                    PythonIntentType.ARRAY_ITEM_TYPE_VIOLATION, field_path, field=name
                )

                # Empty collection check
                if prop.get("minItems", 0) > 0:
                    self._emit(
                        PythonIntentType.EMPTY_COLLECTION, field_path, field=name
                    )

            # --- 6. Complex Objects (Recursion) ---
            if prop.get("type") == "object":
                if "properties" in prop:
                    # Recurse for nested objects
                    self._process_arguments(prop["properties"], field_path)

                # Check for extra fields if strict
                if prop.get("additionalProperties") is False:
                    self._emit(
                        PythonIntentType.OBJECT_EXTRA_FIELD, field_path, field=name
                    )

===========================

====================
path: src\generators\intent_generator\python_intent\__init__.py


===========================

====================
path: src\generators\intent_generator\typescript_intent\enums.py

"""
TypeScript Intent Type Definitions

Defines all test intent types for TypeScript function testing.
These intents are specifically designed for TypeScript type system validation.
"""

from enum import Enum


class TypeScriptIntentType(str, Enum):
    # --- 1. Functional (Happy Path) ---
    HAPPY_PATH = "HAPPY_PATH"

    # --- 2. Structural (The Contract) ---
    REQUIRED_ARG_MISSING = "REQUIRED_ARG_MISSING"
    UNEXPECTED_ARGUMENT = "UNEXPECTED_ARGUMENT"
    OBJECT_MISSING_FIELD = "OBJECT_MISSING_FIELD"
    OBJECT_EXTRA_FIELD = "OBJECT_EXTRA_FIELD"

    # --- 3. Type System (Static Typing) ---
    TYPE_VIOLATION = "TYPE_VIOLATION"
    NULL_NOT_ALLOWED = "NULL_NOT_ALLOWED"
    ARRAY_ITEM_TYPE_VIOLATION = "ARRAY_ITEM_TYPE_VIOLATION"
    UNION_NO_MATCH = "UNION_NO_MATCH"
    GENERIC_TYPE_VIOLATION = "GENERIC_TYPE_VIOLATION"
    INTERFACE_MISSING_PROPERTY = "INTERFACE_MISSING_PROPERTY"

    # --- 4. Constraints & Boundaries ---
    BOUNDARY_MIN_MINUS_ONE = "BOUNDARY_MIN_MINUS_ONE"
    BOUNDARY_MAX_PLUS_ONE = "BOUNDARY_MAX_PLUS_ONE"
    STRING_TOO_SHORT = "STRING_TOO_SHORT"
    STRING_TOO_LONG = "STRING_TOO_LONG"
    PATTERN_MISMATCH = "PATTERN_MISMATCH"
    ENUM_MISMATCH = "ENUM_MISMATCH"

    # --- 5. Data Edge Cases (Robustness) ---
    EMPTY_STRING = "EMPTY_STRING"
    WHITESPACE_ONLY = "WHITESPACE_ONLY"
    ZERO_VALUE = "ZERO_VALUE"
    NEGATIVE_VALUE = "NEGATIVE_VALUE"
    EMPTY_COLLECTION = "EMPTY_COLLECTION"

    # --- 6. Security Fuzzing ---
    SQL_INJECTION = "SQL_INJECTION"
    XSS_INJECTION = "XSS_INJECTION"
    PATH_TRAVERSAL = "PATH_TRAVERSAL"
    COMMAND_INJECTION = "COMMAND_INJECTION"


# Intents grouped by category
INTENTS_BY_CATEGORY = {
    "Functional": [
        TypeScriptIntentType.HAPPY_PATH.value,
    ],
    "Structure": [
        TypeScriptIntentType.REQUIRED_ARG_MISSING.value,
        TypeScriptIntentType.UNEXPECTED_ARGUMENT.value,
        TypeScriptIntentType.OBJECT_MISSING_FIELD.value,
        TypeScriptIntentType.OBJECT_EXTRA_FIELD.value,
    ],
    "Type": [
        TypeScriptIntentType.TYPE_VIOLATION.value,
        TypeScriptIntentType.NULL_NOT_ALLOWED.value,
        TypeScriptIntentType.ARRAY_ITEM_TYPE_VIOLATION.value,
        TypeScriptIntentType.UNION_NO_MATCH.value,
        TypeScriptIntentType.GENERIC_TYPE_VIOLATION.value,
        TypeScriptIntentType.INTERFACE_MISSING_PROPERTY.value,
    ],
    "Constraints": [
        TypeScriptIntentType.BOUNDARY_MIN_MINUS_ONE.value,
        TypeScriptIntentType.BOUNDARY_MAX_PLUS_ONE.value,
        TypeScriptIntentType.STRING_TOO_SHORT.value,
        TypeScriptIntentType.STRING_TOO_LONG.value,
        TypeScriptIntentType.PATTERN_MISMATCH.value,
        TypeScriptIntentType.ENUM_MISMATCH.value,
    ],
    "Robustness": [
        TypeScriptIntentType.EMPTY_STRING.value,
        TypeScriptIntentType.WHITESPACE_ONLY.value,
        TypeScriptIntentType.ZERO_VALUE.value,
        TypeScriptIntentType.NEGATIVE_VALUE.value,
        TypeScriptIntentType.EMPTY_COLLECTION.value,
    ],
    "Security": [
        TypeScriptIntentType.SQL_INJECTION.value,
        TypeScriptIntentType.XSS_INJECTION.value,
        TypeScriptIntentType.PATH_TRAVERSAL.value,
        TypeScriptIntentType.COMMAND_INJECTION.value,
    ],
}

# Flattened list and reverse map for quick lookup
ALL_TYPESCRIPT_INTENTS = [i for cat in INTENTS_BY_CATEGORY.values() for i in cat]
INTENT_TO_CATEGORY = {
    intent: cat for cat, lst in INTENTS_BY_CATEGORY.items() for intent in lst
}

===========================

====================
path: src\generators\intent_generator\typescript_intent\generator.py

"""
TypeScript Intent Generator

Generates test intents for TypeScript function signatures.
Specialized for TypeScript-specific type system including generics, interfaces, and union types.
"""

from typing import List, Dict, Any, Optional


# TypeScript-specific intent definitions
TYPESCRIPT_INTENT_DEFINITIONS = {
    # Functional
    "HAPPY_PATH": {
        "category": "functional",
        "description": "Valid call with all correct arguments",
        "expected_status": 200,
    },
    # Structural
    "REQUIRED_ARG_MISSING": {
        "category": "structure",
        "description": "Required parameter is missing",
        "expected_status": 400,
    },
    "UNEXPECTED_ARGUMENT": {
        "category": "structure",
        "description": "Extra unexpected parameter passed",
        "expected_status": 400,
    },
    "OBJECT_MISSING_FIELD": {
        "category": "structure",
        "description": "Required property missing from object",
        "expected_status": 400,
    },
    "OBJECT_EXTRA_FIELD": {
        "category": "structure",
        "description": "Unexpected property in strict object",
        "expected_status": 400,
    },
    # Type Violations
    "TYPE_VIOLATION": {
        "category": "type",
        "description": "Wrong type for parameter",
        "expected_status": 400,
    },
    "NULL_NOT_ALLOWED": {
        "category": "type",
        "description": "Null passed to non-nullable parameter",
        "expected_status": 400,
    },
    "ARRAY_ITEM_TYPE_VIOLATION": {
        "category": "type",
        "description": "Array contains items of wrong type",
        "expected_status": 400,
    },
    "UNION_NO_MATCH": {
        "category": "type",
        "description": "Value matches no variant of union type",
        "expected_status": 400,
    },
    "GENERIC_TYPE_VIOLATION": {
        "category": "type",
        "description": "Value violates generic type constraint",
        "expected_status": 400,
    },
    "INTERFACE_MISSING_PROPERTY": {
        "category": "type",
        "description": "Object missing required interface property",
        "expected_status": 400,
    },
    # Constraints
    "BOUNDARY_MIN_MINUS_ONE": {
        "category": "constraint",
        "description": "Numeric value below minimum",
        "expected_status": 400,
    },
    "BOUNDARY_MAX_PLUS_ONE": {
        "category": "constraint",
        "description": "Numeric value above maximum",
        "expected_status": 400,
    },
    "STRING_TOO_SHORT": {
        "category": "constraint",
        "description": "String below minimum length",
        "expected_status": 400,
    },
    "STRING_TOO_LONG": {
        "category": "constraint",
        "description": "String exceeds maximum length",
        "expected_status": 400,
    },
    "PATTERN_MISMATCH": {
        "category": "constraint",
        "description": "String doesn't match required pattern",
        "expected_status": 400,
    },
    "ENUM_MISMATCH": {
        "category": "constraint",
        "description": "Value not in allowed enum options",
        "expected_status": 400,
    },
    # Robustness
    "EMPTY_STRING": {
        "category": "robustness",
        "description": "Empty string input",
        "expected_status": 400,
    },
    "WHITESPACE_ONLY": {
        "category": "robustness",
        "description": "String with only whitespace",
        "expected_status": 400,
    },
    "ZERO_VALUE": {
        "category": "robustness",
        "description": "Zero numeric value",
        "expected_status": 400,
    },
    "NEGATIVE_VALUE": {
        "category": "robustness",
        "description": "Negative numeric value",
        "expected_status": 400,
    },
    "EMPTY_COLLECTION": {
        "category": "robustness",
        "description": "Empty array or object",
        "expected_status": 400,
    },
    # Security
    "SQL_INJECTION": {
        "category": "security",
        "description": "SQL injection attempt",
        "expected_status": 400,
    },
    "XSS_INJECTION": {
        "category": "security",
        "description": "Cross-site scripting attempt",
        "expected_status": 400,
    },
    "PATH_TRAVERSAL": {
        "category": "security",
        "description": "Path traversal attempt",
        "expected_status": 400,
    },
    "COMMAND_INJECTION": {
        "category": "security",
        "description": "Command injection attempt",
        "expected_status": 400,
    },
}


class TypeScriptIntentGenerator:
    """
    Generates test intents for TypeScript function operations.
    """

    def __init__(self, operation: dict):
        self.operation = operation
        self.op_id = operation.get("id", "unknown_op")
        self.inputs = operation.get("inputs", {})
        self.body_schema = self.inputs.get("body", {}).get("schema", {})
        self.properties = self.body_schema.get("properties", {})
        self.required = self.body_schema.get("required", [])

    def generate(self) -> List[dict]:
        """Generate all applicable intents for this TypeScript operation."""
        intents = []

        # Always generate HAPPY_PATH
        intents.append(self._create_intent("HAPPY_PATH", "inputs.body", None))

        # Generate intents for each parameter
        for param_name, param_schema in self.properties.items():
            target = f"inputs.body.{param_name}"
            param_intents = self._generate_param_intents(
                param_name, param_schema, target
            )
            intents.extend(param_intents)

        return intents

    def _create_intent(
        self, intent_type: str, target: str, field: Optional[str], **kwargs
    ) -> dict:
        """Create a standardized intent dict."""
        intent_def = TYPESCRIPT_INTENT_DEFINITIONS.get(intent_type, {})

        intent = {
            "intent": intent_type,
            "operation_id": self.op_id,
            "target": target,
            "field": field,
            "category": intent_def.get("category", "unknown"),
            "description": intent_def.get("description", ""),
            "expected": str(intent_def.get("expected_status", 400)),
        }
        intent.update(kwargs)
        return intent

    def _generate_param_intents(
        self, param_name: str, schema: dict, target: str
    ) -> List[dict]:
        """Generate intents applicable to a specific parameter."""
        intents = []
        param_type = schema.get("type", "")
        is_required = param_name in self.required
        is_nullable = schema.get("nullable", False)

        # Structural intents
        if is_required:
            intents.append(
                self._create_intent("REQUIRED_ARG_MISSING", target, param_name)
            )

        # Type violations
        if param_type:
            intents.append(self._create_intent("TYPE_VIOLATION", target, param_name))

        if not is_nullable:
            intents.append(self._create_intent("NULL_NOT_ALLOWED", target, param_name))

        # Array-specific
        if param_type == "array":
            intents.append(
                self._create_intent("ARRAY_ITEM_TYPE_VIOLATION", target, param_name)
            )
            intents.append(self._create_intent("EMPTY_COLLECTION", target, param_name))

        # Object-specific
        if param_type == "object":
            intents.append(
                self._create_intent("OBJECT_MISSING_FIELD", target, param_name)
            )
            intents.append(
                self._create_intent("OBJECT_EXTRA_FIELD", target, param_name)
            )
            intents.append(
                self._create_intent("INTERFACE_MISSING_PROPERTY", target, param_name)
            )

        # Union types
        if "oneOf" in schema or "anyOf" in schema:
            intents.append(self._create_intent("UNION_NO_MATCH", target, param_name))

        # Numeric constraints
        if param_type in ("number", "integer"):
            if "minimum" in schema:
                intents.append(
                    self._create_intent("BOUNDARY_MIN_MINUS_ONE", target, param_name)
                )
            if "maximum" in schema:
                intents.append(
                    self._create_intent("BOUNDARY_MAX_PLUS_ONE", target, param_name)
                )
            intents.append(self._create_intent("ZERO_VALUE", target, param_name))
            intents.append(self._create_intent("NEGATIVE_VALUE", target, param_name))

        # String constraints
        if param_type == "string":
            if "minLength" in schema:
                intents.append(
                    self._create_intent("STRING_TOO_SHORT", target, param_name)
                )
            if "maxLength" in schema:
                intents.append(
                    self._create_intent("STRING_TOO_LONG", target, param_name)
                )
            if "pattern" in schema:
                intents.append(
                    self._create_intent("PATTERN_MISMATCH", target, param_name)
                )
            if "enum" in schema:
                intents.append(self._create_intent("ENUM_MISMATCH", target, param_name))

            # Robustness
            intents.append(self._create_intent("EMPTY_STRING", target, param_name))
            intents.append(self._create_intent("WHITESPACE_ONLY", target, param_name))

            # Security
            intents.append(self._create_intent("SQL_INJECTION", target, param_name))
            intents.append(self._create_intent("XSS_INJECTION", target, param_name))
            intents.append(self._create_intent("PATH_TRAVERSAL", target, param_name))
            intents.append(self._create_intent("COMMAND_INJECTION", target, param_name))

        return intents


def get_typescript_intent_definitions() -> dict:
    """Return all TypeScript intent definitions."""
    return TYPESCRIPT_INTENT_DEFINITIONS.copy()

===========================

====================
path: src\generators\intent_generator\typescript_intent\__init__.py


===========================

====================
path: src\generators\ir_generator\builder.py

import hashlib


def build_ir(
    source_type: str,
    source_name: str,
    source_payload: str,
    operations: list,
    types: list = None,
    version: str = "1.0",
    metadata: dict = None,
) -> dict:
    ir = {
        "ir_version": version,
        "source": {
            "type": source_type,
            "name": source_name,
            "hash": f"sha256:{_hash(source_payload)}",
        },
        "operations": operations,
        "types": types or [],
    }

    # Add metadata if provided
    if metadata:
        ir["metadata"] = metadata

    return ir


def _hash(text: str) -> str:
    return hashlib.sha256(text.encode()).hexdigest()

===========================

====================
path: src\generators\ir_generator\schema.json

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["ir_version", "source", "operations"],
  "properties": {
    "ir_version": { "type": "string" },
    "source": {
      "type": "object",
      "required": ["type", "name", "hash"],
      "properties": {
        "type": { "type": "string" },
        "name": { "type": "string" },
        "hash": { "type": "string" }
      }
    },
    "operations": {
      "type": "array",
      "items": { "$ref": "#/definitions/operation" }
    },
    "types": {
      "type": "array",
      "items": { "type": "object" }
    },
    "metadata": {
      "type": "object"
    },
    "warnings": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["code", "message"],
        "properties": {
          "code": { "type": "string" },
          "param": { "type": "string" },
          "message": { "type": "string" }
        }
      }
    }
  },
  "definitions": {
    "operation": {
      "type": "object",
      "required": ["id", "kind", "inputs", "outputs", "errors"],
      "properties": {
        "id": { "type": "string" },
        "kind": { "type": "string" },
        "method": { "type": "string" },
        "path": { "type": "string" },
        "inputs": { "type": "object" },
        "outputs": { "type": "array" },
        "errors": { "type": "array" },
        "async": { "type": "boolean" },
        "description": { "type": "string" },
        "metadata": { "type": "object" }
      }
    }
  }
}

===========================

====================
path: src\generators\ir_generator\validator.py

# validator.py
# This file will contain functions to validate IR against the schema.

import json
from jsonschema import validate, ValidationError as JsonSchemaValidationError
from pathlib import Path
from testsuitegen.src.exceptions.exceptions import ValidationError


SCHEMA_PATH = Path(__file__).parent / "schema.json"


def validate_ir(ir: dict) -> bool:
    """
    Validate IR data using the schema defined in schema.json.

    Raises:
        ValidationError: If the IR structure is invalid.
    """
    schema = json.loads(SCHEMA_PATH.read_text())
    try:
        validate(instance=ir, schema=schema)
    except JsonSchemaValidationError as e:

        error_path = " -> ".join([str(p) for p in e.path]) if e.path else "Root"
        raise ValidationError(
            f"IR Structure Invalid at [{error_path}]",
            (
                f"{e.message}\n   Expected: {e.schema.get('type', 'object')}"
                if hasattr(e, "schema")
                else e.message
            ),
        )
        
    return True

===========================

====================
path: src\generators\ir_generator\__init__.py

# IR (Intermediate Representation) module
# This module handles the construction and validation of the intermediate representation

===========================

====================
path: src\generators\payloads_generator\generator.py

import uuid
from copy import deepcopy
from typing import List, Any, Optional

# Import all mutators
from testsuitegen.src.generators.payloads_generator.openapi_mutator.mutator import (
    OpenAPIMutator,
)
from testsuitegen.src.generators.payloads_generator.python_mutator.mutator import (
    PythonMutator,
)
from testsuitegen.src.generators.payloads_generator.typescript_mutator.mutator import (
    TypeScriptMutator,
)




def get_mutator_for_kind(kind: str):
    """
    Factory function to get the appropriate mutator based on IR operation kind.
    
    Args:
        kind: The kind of operation ('http', 'function', 'typescript_function')
        
    Returns:
        An instance of the appropriate mutator class
    """
    if kind == "function":
        return PythonMutator()
    elif kind == "typescript_function":
        return TypeScriptMutator()
    else:  # Default to OpenAPI for 'http' and unknown kinds
        return OpenAPIMutator()


class PayloadGenerator:
    """
    Payload Generator.
    Converts IR and Intents into concrete test cases using a mutation-based approach.
    Supports 'Golden Record' optimization to maintain high-fidelity data across tests.
    Uses the appropriate mutator based on the source type (OpenAPI, Python, TypeScript).
    """

    def __init__(self, ir: dict, base_payload_override: Optional[dict] = None):
        self.ir = ir
        self.base_path_params = self._build_path_params()
        self.base_headers = self._build_valid_headers()
        
        # Select the appropriate mutator based on IR kind
        kind = ir.get("kind", "http")
        self.mutator = get_mutator_for_kind(kind)

        # Initialize the base payload (Golden Record)
        if base_payload_override:
            self.base_payload = deepcopy(base_payload_override)
            body_schema = self.ir["inputs"].get("body", {}).get("schema", {})
            self._recursive_sanitize(self.base_payload, body_schema)
        else:
            self.base_payload = self._build_valid_payload()

    def generate(self, intents: List[dict]) -> List[dict]:
        """
        Iterates through intents and applies mutations
        to the base Golden Record.
        """
        results = []

        for intent in intents:
            # Step 1: Start every test case with a clean copy of the Golden Record
            payload = deepcopy(self.base_payload)
            path_params = deepcopy(self.base_path_params)
            headers = deepcopy(self.base_headers)

            intent_type = intent["intent"]
            target = intent["target"]
            field = intent.get("field")
            field_schema = self._get_schema_for_field(target)

            # Step 2: Handle Happy Path
            if intent_type == "HAPPY_PATH":
                self._handle_happy_path(payload, path_params)
                results.append(self._wrap(intent, payload, path_params, headers))
                continue

            # Step 3: Handle Header Mutations
            if "inputs.headers" in target:
                self.mutator.mutate_headers(headers, intent_type, field)
                results.append(self._wrap(intent, payload, path_params, headers))
                continue

            # Step 4: Handle Path Parameter Mutations
            if "inputs.path" in target:
                self.mutator.mutate_path_params(
                    path_params, intent_type, field, field_schema, self.ir
                )
                results.append(self._wrap(intent, payload, path_params, headers))
                continue

            # Step 5: Handle Query Parameter Mutations
            if "inputs.query" in target:
                if self.mutator.mutate_query_params(
                    payload, intent_type, field, field_schema
                ):
                    results.append(self._wrap(intent, payload, path_params, headers))
                continue

            # Step 6: Handle Body Mutations (The bulk of the logic)
            self.mutator.mutate_body(payload, intent_type, target, field, field_schema)

            # Step 7: Wrap and append the result
            results.append(self._wrap(intent, payload, path_params, headers))

        # Step 8: Return all generated results
        return results

    def _wrap(
        self,
        intent: dict,
        payload: dict,
        path_params: dict = None,
        headers: dict = None,
    ):
        expected_status = int(intent["expected"])
        result = {
            "operation_id": intent["operation_id"],
            "intent": intent["intent"],
            "expected_status": expected_status,
            "payload": payload,
            "response": self._generate_mock_response(expected_status),
        }
        if path_params:
            result["path_params"] = path_params
        if headers:
            result["headers"] = headers
        return result

    def _generate_mock_response(self, status_code: int) -> dict:
        """Generate a mock response based on the operation's output schemas."""
        outputs = self.ir.get("outputs", [])
        
        # Find matching output schema for the expected status code
        matching_output = None
        for output in outputs:
            if output.get("status") == status_code:
                matching_output = output
                break
        
        # If no exact match, try to find a 2xx response for happy path
        if not matching_output and 200 <= status_code < 300:
            for output in outputs:
                if 200 <= output.get("status", 0) < 300:
                    matching_output = output
                    break
        
        if not matching_output or not matching_output.get("schema"):
            return {"status": status_code, "body": None}
        
        schema = matching_output["schema"]
        mock_body = self._generate_value_from_schema(schema)
        
        return {
            "status": status_code,
            "content_type": matching_output.get("content_type", "application/json"),
            "body": mock_body,
        }

    def _generate_value_from_schema(self, schema: dict) -> Any:
        """Recursively generate mock data from a schema."""
        if not isinstance(schema, dict):
            return None
        
        # Handle oneOf/anyOf by picking first option
        if "oneOf" in schema:
            return self._generate_value_from_schema(schema["oneOf"][0])
        if "anyOf" in schema:
            return self._generate_value_from_schema(schema["anyOf"][0])
        
        schema_type = schema.get("type", "object")
        
        # Use example if provided
        if "example" in schema:
            return schema["example"]
        
        # Use default if provided
        if "default" in schema:
            return schema["default"]
        
        # Use enum first value if provided
        if "enum" in schema:
            return schema["enum"][0]
        
        if schema_type == "object":
            result = {}
            properties = schema.get("properties", {})
            for prop_name, prop_schema in properties.items():
                result[prop_name] = self._generate_value_from_schema(prop_schema)
            return result
        
        elif schema_type == "array":
            items_schema = schema.get("items", {})
            return [self._generate_value_from_schema(items_schema)]
        
        elif schema_type == "string":
            fmt = schema.get("format")
            if fmt == "uuid":
                return "550e8400-e29b-41d4-a716-446655440000"
            elif fmt == "date-time":
                return "2024-01-15T10:30:00Z"
            elif fmt == "date":
                return "2024-01-15"
            elif fmt == "email":
                return "user@example.com"
            elif fmt == "uri":
                return "https://example.com/resource"
            return "sample_string"
        
        elif schema_type == "integer":
            return 1
        
        elif schema_type == "number":
            return 1.0
        
        elif schema_type == "boolean":
            return True
        
        return None

    def _handle_happy_path(self, payload: dict, path_params: dict):
        """Ensure required query and path params are valid."""
        for query_param in self.ir["inputs"].get("query", []):
            if query_param.get("required", False):
                name = query_param["name"]
                if name not in payload:
                    payload[name] = self._valid_value(
                        query_param.get("schema", {}), name
                    )

        self._fill_happy_path_params(path_params)

    def _recursive_sanitize(self, data: Any, schema: dict):
        """Remove invalid fields based on schema."""
        if not isinstance(data, dict) or not schema:
            return
        if schema.get("additionalProperties") is False:
            allowed = set(schema.get("properties", {}).keys())
            for k in list(data.keys()):
                if k not in allowed:
                    del data[k]
        for k, v in data.items():
            if k in schema.get("properties", {}):
                self._recursive_sanitize(v, schema["properties"][k])

    def _calculate_boundary_value(self, intent_type: str, schema: dict) -> Any:
        """Calculate boundary values for schema constraints."""
        if not schema:
            return "INVALID"
        stype = schema.get("type")
        if intent_type == "BOUNDARY_MIN_MINUS_ONE":
            mn = schema.get("minimum", 0)
            return (mn - 1) if stype == "integer" else (mn - 0.01)
        if intent_type == "BOUNDARY_MAX_PLUS_ONE":
            mx = schema.get("maximum", 100)
            return (mx + 1) if stype == "integer" else (mx + 0.01)
        if intent_type == "BOUNDARY_MIN_LENGTH_MINUS_ONE":
            return "x" * max(0, schema.get("minLength", 1) - 1)
        if intent_type == "BOUNDARY_MAX_LENGTH_PLUS_ONE":
            return "x" * (schema.get("maxLength", 10) + 1)
        return "INVALID"

    def _get_schema_for_field(self, target: str) -> dict:
        """Retrieve schema for a specific field in the target."""
        parts = target.split(".")
        current = self.ir["inputs"]
        if "body" in parts:
            current = current.get("body", {}).get("schema", {})
            for part in parts[parts.index("body") + 1 :]:
                if not part or part in ["[]", "*"]:
                    continue
                if "properties" in current:
                    current = current["properties"].get(part, {})
                elif "items" in current:
                    current = current["items"]
                    if "properties" in current:
                        current = current["properties"].get(part, {})
        return current if isinstance(current, dict) else {}

    def _valid_value(self, spec: dict, field_name: str = None) -> Any:
        """Generate a valid value based on the schema."""
        if not isinstance(spec, dict):
            return "__PLACEHOLDER_ANY__"
        if "oneOf" in spec:
            return self._valid_value(spec["oneOf"][0], field_name)
        if "anyOf" in spec:
            return self._valid_value(spec["anyOf"][0], field_name)

        t = spec.get("type")
        if t == "string":
            if "enum" in spec:
                return spec["enum"][0]
            fmt = spec.get("format")
            if fmt == "uuid":
                return f"__PLACEHOLDER_UUID_{field_name}__"
            if fmt == "date-time":
                return f"__PLACEHOLDER_DATETIME_{field_name}__"
            return f"__PLACEHOLDER_STRING_{field_name}__"
        if t == "integer":
            # Respect minimum/maximum constraints
            min_val = spec.get("minimum", spec.get("exclusiveMinimum"))
            if min_val is not None:
                # For exclusiveMinimum, add 1
                if "exclusiveMinimum" in spec:
                    return int(min_val) + 1
                return int(min_val)
            return 1
        if t == "number":
            # Respect minimum/maximum constraints
            min_val = spec.get("minimum", spec.get("exclusiveMinimum"))
            if min_val is not None:
                # For exclusiveMinimum, add a small delta
                if "exclusiveMinimum" in spec:
                    return float(min_val) + 0.01
                return float(min_val)
            return 1.0
        if t == "boolean":
            return True
        if t == "array":
            return [self._valid_value(spec.get("items", {}), field_name)]
        if t == "object":
            # Recursively generate valid values for object properties
            properties = spec.get("properties", {})
            if properties:
                result = {}
                for prop_name, prop_schema in properties.items():
                    result[prop_name] = self._valid_value(prop_schema, prop_name)
                return result
            return {}
        return "__PLACEHOLDER_ANY__"

    def _invalid_value(self, spec: dict) -> Any:
        """Generate an invalid value for the schema."""
        t = spec.get("type", "string")
        if t == "integer":
            return "not_an_int"
        if t == "boolean":
            return "not_a_bool"
        return 12345

    def _should_skip_query_type_violation(self, schema: dict) -> bool:
        """Check if query type violation should be skipped."""
        if schema.get("type") == "string" and not any(
            k in schema for k in ["minLength", "pattern", "enum"]
        ):
            return True
        return False

    def _fill_happy_path_params(self, path_params: dict, exclude: List[str] = None):
        """Fill valid values for path parameters."""
        exclude = exclude or []
        for param in self.ir["inputs"].get("path", []):
            name = param["name"]
            if name in exclude:
                continue
            schema = param.get("schema", {})
            if schema.get("format") == "uuid":
                path_params[name] = str(uuid.uuid4())
            elif schema.get("type") == "integer":
                path_params[name] = 1
            else:
                path_params[name] = "test_val"

    def _build_valid_payload(self) -> dict:
        """Build a valid payload based on the schema."""
        payload = {}
        body = self.ir["inputs"].get("body", {})
        if body and body.get("schema"):
            for f, s in body["schema"].get("properties", {}).items():
                payload[f] = self._valid_value(s, f)
        return payload

    def _build_path_params(self) -> dict:
        """Build default path parameters."""
        return {p["name"]: None for p in self.ir["inputs"].get("path", [])}

    def _build_valid_headers(self) -> dict:
        """Build valid headers based on the schema."""
        return {
            h["name"]: self._valid_value(h.get("schema", {}), h["name"])
            for h in self.ir["inputs"].get("headers", [])
        }

    def _get_target_field(self, target: str):
        """Extract the target field from the path."""
        parts = target.split(".")
        if "body" in parts:
            idx = parts.index("body")
            if idx + 1 < len(parts):
                return parts[idx + 1].replace("[]", "").replace("*", "")
        return None


def generate_payloads(
    ir: dict, intents: list, base_payload_override: dict = None
) -> list:
    """Functional wrapper for the PayloadGenerator class."""
    generator = PayloadGenerator(ir, base_payload_override)
    return generator.generate(intents)

===========================

====================
path: src\generators\payloads_generator\mutator.py

from typing import Any, Dict, List
import uuid


class PayloadMutator:
    """
    Handles all mutation logic for payloads, headers, path parameters, and query parameters.
    """

    INVALID_TYPE = "__INVALID_TYPE__"

    def mutate_headers(self, headers: dict, intent_type: str, field: str):
        """Applies mutations to request headers."""
        if intent_type == "HEADER_MISSING":
            headers.pop(field, None)
        elif intent_type == "HEADER_INJECTION":
            headers[field] = "ValidValue\r\nSet-Cookie: evil=true"
        elif intent_type == "HEADER_ENUM_MISMATCH":
            headers[field] = "INVALID_HEADER_ENUM"

    def mutate_path_params(
        self, path_params: dict, intent_type: str, field: str, schema: dict, ir: dict
    ):
        """Applies mutations to URL path parameters."""
        if intent_type == "TYPE_VIOLATION":
            path_params[field] = self.INVALID_TYPE
        elif intent_type == "RESOURCE_NOT_FOUND":
            if schema.get("format") == "uuid":
                path_params[field] = str(uuid.uuid4())
            elif schema.get("type") == "integer":
                path_params[field] = 999999
            else:
                path_params[field] = "nonexistent_resource"
        elif intent_type == "FORMAT_INVALID_PATH_PARAM":
            path_params[field] = "not-a-valid-format"
        elif intent_type in [
            "BOUNDARY_MIN_MINUS_ONE",
            "BOUNDARY_MAX_PLUS_ONE",
            "BOUNDARY_MIN_LENGTH_MINUS_ONE",
            "BOUNDARY_MAX_LENGTH_PLUS_ONE",
            "PATTERN_MISMATCH",
            "SQL_INJECTION",
            "XSS_INJECTION",
        ]:
            val = self._calculate_boundary_value(intent_type, schema)
            if intent_type == "SQL_INJECTION":
                val = "' OR '1'='1"
            elif intent_type == "XSS_INJECTION":
                val = "<script>alert(1)</script>"
            elif intent_type == "PATTERN_MISMATCH":
                val = "!!!invalid!!!"
            path_params[field] = val

        # Ensure other path params (not the one we are testing) are valid
        self._fill_happy_path_params(path_params, ir, exclude=[field])

    def _fill_happy_path_params(
        self, path_params: dict, ir: dict, exclude: list = None
    ):
        """Fills all path parameters with valid values except those in exclude."""
        exclude = exclude or []
        for param in ir["inputs"].get("path", []):
            name = param["name"]
            if name in exclude:
                continue
            schema = param.get("schema", {})
            path_params[name] = self._valid_value(schema, name)

    def _valid_value(self, spec: dict, field_name: str = None) -> Any:
        """Generates a valid placeholder value based on schema type."""
        if not isinstance(spec, dict):
            return "__PLACEHOLDER_ANY__"

        t = spec.get("type", "string")
        if t == "string":
            if "enum" in spec:
                return spec["enum"][0]
            fmt = spec.get("format")
            if fmt == "uuid":
                return str(uuid.uuid4())
            return f"__PLACEHOLDER_STRING_{field_name}__"
        if t == "integer":
            return 1
        if t == "number":
            return 1.0
        if t == "boolean":
            return True
        if t == "array":
            return []
        if t == "object":
            return {}
        return "__PLACEHOLDER_ANY__"

    def mutate_query_params(
        self, payload: dict, intent_type: str, field: str, schema: dict
    ) -> bool:
        """Applies mutations to query parameters stored in the payload."""
        if intent_type == "REQUIRED_FIELD_MISSING":
            payload.pop(field, None)
            return True
        elif intent_type == "TYPE_VIOLATION":
            if self._should_skip_query_type_violation(schema):
                return False
            payload[field] = self.INVALID_TYPE
            return True
        return True

    def mutate_body(
        self, payload: dict, intent_type: str, target: str, field: str, schema: dict
    ):
        """Applies mutations to the JSON request body."""
        # Fallback: if field is missing, extract from target
        if not field and target:
            field = target.split(".")[-1].replace("[]", "")

        # Structural - Python Function Specific
        if intent_type == "REQUIRED_ARG_MISSING" and field:
            self._delete_field(payload, field)
        elif intent_type == "UNEXPECTED_ARGUMENT":
            payload["__unexpected_arg__"] = "unexpected_value"
        elif intent_type == "TOO_MANY_POS_ARGS":
            # Add extra positional arguments indicator
            payload["__extra_positional__"] = ["extra1", "extra2"]

        # Structural - General
        elif intent_type == "REQUIRED_FIELD_MISSING" and field:
            self._delete_field(payload, field)
        elif intent_type == "NULL_NOT_ALLOWED" and field:
            self._set_value(payload, target, field, None)
        elif intent_type == "TYPE_VIOLATION":
            self._apply_type_violation(payload, target, field)
        elif intent_type == "ADDITIONAL_PROPERTY_NOT_ALLOWED":
            payload["__extra__"] = "boom"

        # Type System
        elif intent_type == "ARRAY_ITEM_TYPE_VIOLATION":
            if field:
                # Determine the correct invalid type based on expected item type
                item_schema = schema.get("items", {})
                item_type = item_schema.get("type", "")
                
                # Use a value that actually violates the expected type
                if item_type == "string":
                    invalid_value = 12345  # Use integer to violate string
                elif item_type in ("integer", "number"):
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate number
                elif item_type == "object":
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate object
                elif item_type == "boolean":
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate boolean
                else:
                    invalid_value = self.INVALID_TYPE  # Default fallback
                
                self._set_value(payload, target, field, [invalid_value])
        elif intent_type == "DICT_KEY_TYPE_VIOLATION":
            if field:
                # Create dict with invalid key type (represented as marker)
                self._set_value(
                    payload, target, field, {"__INVALID_KEY_TYPE__": "value"}
                )
        elif intent_type == "DICT_VALUE_TYPE_VIOLATION":
            if field:
                # Create dict with invalid value type
                self._set_value(payload, target, field, {"key": self.INVALID_TYPE})
        elif intent_type == "OBJECT_VALUE_TYPE_VIOLATION":
            if field:
                # Set object field to a non-object type (string)
                self._set_value(payload, target, field, self.INVALID_TYPE)
        elif intent_type == "ARRAY_ITEM_OBJECT_VALUE_TYPE_VIOLATION":
            if field:
                # Set array item's object field to wrong type
                self._set_value(payload, target, field, [{"__nested_type_violation__": self.INVALID_TYPE}])
        elif intent_type == "NESTED_ARRAY_ITEM_TYPE_VIOLATION":
            if field:
                # Set nested array item to wrong type
                self._set_value(payload, target, field, [[self.INVALID_TYPE]])
        elif intent_type == "ARRAY_SHAPE_VIOLATION":
            if field:
                # Change array to non-array type
                self._set_value(payload, target, field, "not_an_array")

        # Boundaries & Constraints - Numeric
        elif intent_type in [
            "BOUNDARY_MIN_MINUS_ONE",
            "BOUNDARY_MAX_PLUS_ONE",
            "BOUNDARY_MIN_LENGTH_MINUS_ONE",
            "BOUNDARY_MAX_LENGTH_PLUS_ONE",
        ]:
            val = self._calculate_boundary_value(intent_type, schema)
            self._set_value(payload, target, field, val)

        # Boundaries & Constraints - Array Items
        elif intent_type == "BOUNDARY_MIN_ITEMS_MINUS_ONE":
            min_items = schema.get("minItems", 1)
            item_template = self._get_array_item_template(schema, target, payload)
            count = max(0, min_items - 1)
            self._set_value(payload, target, field, [item_template] * count if count > 0 else [])
        elif intent_type == "BOUNDARY_MAX_ITEMS_PLUS_ONE":
            max_items = schema.get("maxItems", 10)
            item_template = self._get_array_item_template(schema, target, payload)
            self._set_value(payload, target, field, [item_template] * (max_items + 1))

        elif intent_type == "ENUM_MISMATCH":
            self._set_value(payload, target, field, "INVALID_ENUM_VALUE")
        elif intent_type == "STRING_TOO_SHORT":
            self._set_value(payload, target, field, "")
        elif intent_type == "STRING_TOO_LONG":
            self._set_value(payload, target, field, "x" * 1000)
        elif intent_type == "PATTERN_MISMATCH":
            self._set_value(payload, target, field, "!!!invalid_pattern!!!")
        elif intent_type == "FORMAT_INVALID":
            self._set_value(payload, target, field, "invalid_format_value")
        elif intent_type == "NUMBER_TOO_SMALL":
            self._set_value(payload, target, field, -999999)
        elif intent_type == "NUMBER_TOO_LARGE":
            self._set_value(payload, target, field, 999999)
        elif intent_type == "NOT_MULTIPLE_OF":
            self._set_value(payload, target, field, 7)

        # Data Edge Cases
        elif intent_type == "EMPTY_STRING":
            self._set_value(payload, target, field, "")
        elif intent_type == "WHITESPACE_ONLY":
            self._set_value(payload, target, field, "   ")
        elif intent_type == "ZERO_VALUE":
            self._set_value(payload, target, field, 0)
        elif intent_type == "NEGATIVE_VALUE":
            self._set_value(payload, target, field, -1)
        elif intent_type == "EMPTY_COLLECTION":
            # Set to empty list or dict based on schema
            if schema.get("type") == "array":
                self._set_value(payload, target, field, [])
            else:
                self._set_value(payload, target, field, {})
        elif intent_type == "ARRAY_TOO_SHORT":
            self._set_value(payload, target, field, [])
        elif intent_type == "ARRAY_TOO_LONG":
            self._set_value(payload, target, field, [1] * 1000)
        elif intent_type == "ARRAY_NOT_UNIQUE":
            self._make_array_not_unique(payload, target)
        elif intent_type == "OBJECT_TOO_FEW_PROPERTIES":
            payload.clear()
        elif intent_type == "OBJECT_TOO_MANY_PROPERTIES":
            for i in range(100):
                payload[f"extra_{i}"] = i

        # Complex Objects
        elif intent_type == "OBJECT_MISSING_FIELD":
            if field:
                self._delete_field(payload, field)
        elif intent_type == "OBJECT_EXTRA_FIELD":
            if field and field in payload and isinstance(payload[field], dict):
                payload[field]["__unexpected_field__"] = "unexpected"
            else:
                payload["__unexpected_field__"] = "unexpected"

        # Security
        elif intent_type == "SQL_INJECTION":
            self._set_value(payload, target, field, "' OR '1'='1")
        elif intent_type == "XSS_INJECTION":
            self._set_value(payload, target, field, "<script>alert(1)</script>")
        elif intent_type == "PATH_TRAVERSAL":
            self._set_value(payload, target, field, "../../etc/passwd")
        elif intent_type == "COMMAND_INJECTION":
            self._set_value(payload, target, field, "; cat /etc/passwd")

        # Python Runtime Specifics
        elif intent_type == "MUTABLE_DEFAULT_TRAP":
            # Test mutable default by passing same list/dict reference indicator
            if field:
                self._set_value(
                    payload, target, field, {"__mutable_default_marker__": True}
                )

        # Polymorphism & Logic
        elif intent_type == "DISCRIMINATOR_VIOLATION":
            self._set_value(payload, target, field, "INVALID_DISCRIMINATOR_TYPE")
        elif intent_type in ["DEPENDENCY_VIOLATION", "CONDITIONAL_REQUIRED_MISSING"]:
            if field and "_requires_" in field:
                trigger, missing = field.split("_requires_")
                self._set_value(payload, target, trigger, "present")
                self._delete_field(payload, missing)

        elif intent_type == "UNION_NO_MATCH":
            self._apply_union_no_match(payload, target)

    # Helper methods
    def _set_value(self, payload: dict, target: str, field: str, value: Any):
        """Sets a value deep inside the payload based on target path."""
        if not field:
            return
        if field in payload:
            payload[field] = value
            return

        parts = target.split(".")
        try:
            body_idx = parts.index("body")
        except ValueError:
            return

        current = payload
        path_parts = parts[body_idx + 1 :]
        for part in path_parts[:-1]:
            part_clean = part.replace("[]", "").replace("*", "")
            if part_clean in current:
                current = current[part_clean]
                if isinstance(current, list) and len(current) > 0:
                    current = current[0]

        if isinstance(current, dict):
            current[field] = value

    def _delete_field(self, payload: dict, field: str):
        """Recursively deletes a field from the payload."""
        if field in payload:
            del payload[field]
            return
        for v in payload.values():
            if isinstance(v, dict):
                self._delete_field(v, field)
            elif isinstance(v, list):
                for item in v:
                    if isinstance(item, dict):
                        self._delete_field(item, field)

    def _apply_type_violation(self, payload: dict, target: str, field: str):
        if field:
            self._set_value(payload, target, field, self.INVALID_TYPE)
        else:
            parent = self._get_target_field(target)
            if parent and parent in payload:
                payload[parent] = self.INVALID_TYPE

    def _apply_union_no_match(self, payload: dict, target: str):
        parent = self._get_target_field(target)
        if not parent:
            if "mode" in payload:
                payload["mode"] = "INVALID_VARIANT_XYZ"
            payload["__FORCE_UNION_MISMATCH__"] = True
            return
        if parent in payload and isinstance(payload[parent], dict):
            payload[parent]["__FORCE_UNION_MISMATCH__"] = True
            if "mode" in payload[parent]:
                payload[parent]["mode"] = "INVALID_VARIANT_XYZ"

    def _make_array_not_unique(self, payload: dict, target: str):
        parent = self._get_target_field(target)
        if parent in payload and isinstance(payload[parent], list):
            if len(payload[parent]) > 0:
                payload[parent].append(payload[parent][0])
            else:
                payload[parent] = [1, 1]

    def _calculate_boundary_value(self, intent_type: str, schema: dict) -> Any:
        if not schema:
            return "INVALID"
        stype = schema.get("type")
        if intent_type == "BOUNDARY_MIN_MINUS_ONE":
            mn = schema.get("minimum", 0)
            return (mn - 1) if stype == "integer" else (mn - 0.01)
        if intent_type == "BOUNDARY_MAX_PLUS_ONE":
            mx = schema.get("maximum", 100)
            return (mx + 1) if stype == "integer" else (mx + 0.01)
        if intent_type == "BOUNDARY_MIN_LENGTH_MINUS_ONE":
            return "x" * max(0, schema.get("minLength", 1) - 1)
        if intent_type == "BOUNDARY_MAX_LENGTH_PLUS_ONE":
            return "x" * (schema.get("maxLength", 10) + 1)
        return "INVALID"

    def _get_target_field(self, target: str):
        parts = target.split(".")
        if "body" in parts:
            idx = parts.index("body")
            if idx + 1 < len(parts):
                return parts[idx + 1].replace("[]", "").replace("*", "")
        return None

    def _should_skip_query_type_violation(self, schema: dict) -> bool:
        if schema.get("type") == "string" and not any(
            k in schema for k in ["minLength", "pattern", "enum"]
        ):
            return True
        return False

    def _get_array_item_template(self, schema: dict, target: str, payload: dict) -> Any:
        """Get a valid item template from existing payload or generate one."""
        field = self._get_target_field(target)
        if field and field in payload and isinstance(payload[field], list) and len(payload[field]) > 0:
            return payload[field][0]
        items_schema = schema.get("items", {})
        return self._valid_value(items_schema)

===========================

====================
path: src\generators\payloads_generator\openapi_mutator\mutator.py

from typing import Any, Dict, List
import uuid


class OpenAPIMutator:
    """
    Handles all mutation logic for OpenAPI/HTTP payloads, headers, path parameters, and query parameters.
    This mutator is specifically designed for REST API testing based on OpenAPI specifications.
    """

    INVALID_TYPE = "__INVALID_TYPE__"

    def mutate_headers(self, headers: dict, intent_type: str, field: str):
        """Applies mutations to request headers."""
        if intent_type == "HEADER_MISSING":
            headers.pop(field, None)
        elif intent_type == "HEADER_INJECTION":
            headers[field] = "ValidValue\r\nSet-Cookie: evil=true"
        elif intent_type == "HEADER_ENUM_MISMATCH":
            headers[field] = "INVALID_HEADER_ENUM"

    def mutate_path_params(
        self, path_params: dict, intent_type: str, field: str, schema: dict, ir: dict
    ):
        """Applies mutations to URL path parameters."""
        if intent_type == "TYPE_VIOLATION":
            path_params[field] = self.INVALID_TYPE
        elif intent_type == "RESOURCE_NOT_FOUND":
            if schema.get("format") == "uuid":
                path_params[field] = str(uuid.uuid4())
            elif schema.get("type") == "integer":
                path_params[field] = 999999
            else:
                path_params[field] = "nonexistent_resource"
        elif intent_type == "FORMAT_INVALID_PATH_PARAM":
            path_params[field] = "not-a-valid-format"
        elif intent_type in [
            "BOUNDARY_MIN_MINUS_ONE",
            "BOUNDARY_MAX_PLUS_ONE",
            "BOUNDARY_MIN_LENGTH_MINUS_ONE",
            "BOUNDARY_MAX_LENGTH_PLUS_ONE",
            "PATTERN_MISMATCH",
            "SQL_INJECTION",
            "XSS_INJECTION",
        ]:
            val = self._calculate_boundary_value(intent_type, schema)
            if intent_type == "SQL_INJECTION":
                val = "' OR '1'='1"
            elif intent_type == "XSS_INJECTION":
                val = "<script>alert(1)</script>"
            elif intent_type == "PATTERN_MISMATCH":
                val = "!!!invalid!!!"
            path_params[field] = val

        # Ensure other path params (not the one we are testing) are valid
        self._fill_happy_path_params(path_params, ir, exclude=[field])

    def _fill_happy_path_params(
        self, path_params: dict, ir: dict, exclude: list = None
    ):
        """Fills all path parameters with valid values except those in exclude."""
        exclude = exclude or []
        for param in ir["inputs"].get("path", []):
            name = param["name"]
            if name in exclude:
                continue
            schema = param.get("schema", {})
            path_params[name] = self._valid_value(schema, name)

    def _valid_value(self, spec: dict, field_name: str = None) -> Any:
        """Generates a valid placeholder value based on schema type."""
        if not isinstance(spec, dict):
            return "__PLACEHOLDER_ANY__"

        t = spec.get("type", "string")
        if t == "string":
            if "enum" in spec:
                return spec["enum"][0]
            fmt = spec.get("format")
            if fmt == "uuid":
                return str(uuid.uuid4())
            return f"__PLACEHOLDER_STRING_{field_name}__"
        if t == "integer":
            return 1
        if t == "number":
            return 1.0
        if t == "boolean":
            return True
        if t == "array":
            return []
        if t == "object":
            return {}
        return "__PLACEHOLDER_ANY__"

    def mutate_query_params(
        self, payload: dict, intent_type: str, field: str, schema: dict
    ) -> bool:
        """Applies mutations to query parameters stored in the payload."""
        if intent_type == "REQUIRED_FIELD_MISSING":
            payload.pop(field, None)
            return True
        elif intent_type == "TYPE_VIOLATION":
            if self._should_skip_query_type_violation(schema):
                return False
            payload[field] = self.INVALID_TYPE
            return True
        return True

    def mutate_body(
        self, payload: dict, intent_type: str, target: str, field: str, schema: dict
    ):
        """Applies mutations to the JSON request body."""
        # Fallback: if field is missing, extract from target
        if not field and target:
            field = target.split(".")[-1].replace("[]", "")

        # Structural - Python Function Specific
        if intent_type == "REQUIRED_ARG_MISSING" and field:
            self._delete_field(payload, field)
        elif intent_type == "UNEXPECTED_ARGUMENT":
            payload["__unexpected_arg__"] = "unexpected_value"
        elif intent_type == "TOO_MANY_POS_ARGS":
            # Add extra positional arguments indicator
            payload["__extra_positional__"] = ["extra1", "extra2"]

        # Structural - General
        elif intent_type == "REQUIRED_FIELD_MISSING" and field:
            self._delete_field(payload, field)
        elif intent_type == "NULL_NOT_ALLOWED" and field:
            self._set_value(payload, target, field, None)
        elif intent_type == "TYPE_VIOLATION":
            self._apply_type_violation(payload, target, field)
        elif intent_type == "ADDITIONAL_PROPERTY_NOT_ALLOWED":
            payload["__extra__"] = "boom"

        # Type System
        elif intent_type == "ARRAY_ITEM_TYPE_VIOLATION":
            if field:
                # Determine the correct invalid type based on expected item type
                item_schema = schema.get("items", {})
                item_type = item_schema.get("type", "")
                
                # Use a value that actually violates the expected type
                if item_type == "string":
                    invalid_value = 12345  # Use integer to violate string
                elif item_type in ("integer", "number"):
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate number
                elif item_type == "object":
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate object
                elif item_type == "boolean":
                    invalid_value = "__INVALID_TYPE__"  # Use string to violate boolean
                else:
                    invalid_value = self.INVALID_TYPE  # Default fallback
                
                self._set_value(payload, target, field, [invalid_value])
        elif intent_type == "DICT_KEY_TYPE_VIOLATION":
            if field:
                # Create dict with invalid key type (represented as marker)
                self._set_value(
                    payload, target, field, {"__INVALID_KEY_TYPE__": "value"}
                )
        elif intent_type == "DICT_VALUE_TYPE_VIOLATION":
            if field:
                # Create dict with invalid value type
                self._set_value(payload, target, field, {"key": self.INVALID_TYPE})
        elif intent_type == "OBJECT_VALUE_TYPE_VIOLATION":
            if field:
                # Set object field to a non-object type (string)
                self._set_value(payload, target, field, self.INVALID_TYPE)
        elif intent_type == "ARRAY_ITEM_OBJECT_VALUE_TYPE_VIOLATION":
            if field:
                # Set array item's object field to wrong type
                self._set_value(payload, target, field, [{"__nested_type_violation__": self.INVALID_TYPE}])
        elif intent_type == "NESTED_ARRAY_ITEM_TYPE_VIOLATION":
            if field:
                # Set nested array item to wrong type
                self._set_value(payload, target, field, [[self.INVALID_TYPE]])
        elif intent_type == "ARRAY_SHAPE_VIOLATION":
            if field:
                # Change array to non-array type
                self._set_value(payload, target, field, "not_an_array")

        # Boundaries & Constraints - Numeric
        elif intent_type in [
            "BOUNDARY_MIN_MINUS_ONE",
            "BOUNDARY_MAX_PLUS_ONE",
            "BOUNDARY_MIN_LENGTH_MINUS_ONE",
            "BOUNDARY_MAX_LENGTH_PLUS_ONE",
        ]:
            val = self._calculate_boundary_value(intent_type, schema)
            self._set_value(payload, target, field, val)

        # Boundaries & Constraints - Array Items
        elif intent_type == "BOUNDARY_MIN_ITEMS_MINUS_ONE":
            min_items = schema.get("minItems", 1)
            item_template = self._get_array_item_template(schema, target, payload)
            count = max(0, min_items - 1)
            self._set_value(payload, target, field, [item_template] * count if count > 0 else [])
        elif intent_type == "BOUNDARY_MAX_ITEMS_PLUS_ONE":
            max_items = schema.get("maxItems", 10)
            item_template = self._get_array_item_template(schema, target, payload)
            self._set_value(payload, target, field, [item_template] * (max_items + 1))

        elif intent_type == "ENUM_MISMATCH":
            self._set_value(payload, target, field, "INVALID_ENUM_VALUE")
        elif intent_type == "STRING_TOO_SHORT":
            self._set_value(payload, target, field, "")
        elif intent_type == "STRING_TOO_LONG":
            self._set_value(payload, target, field, "x" * 1000)
        elif intent_type == "PATTERN_MISMATCH":
            self._set_value(payload, target, field, "!!!invalid_pattern!!!")
        elif intent_type == "FORMAT_INVALID":
            self._set_value(payload, target, field, "invalid_format_value")
        elif intent_type == "NUMBER_TOO_SMALL":
            self._set_value(payload, target, field, -999999)
        elif intent_type == "NUMBER_TOO_LARGE":
            self._set_value(payload, target, field, 999999)
        elif intent_type == "NOT_MULTIPLE_OF":
            self._set_value(payload, target, field, 7)

        # Data Edge Cases
        elif intent_type == "EMPTY_STRING":
            self._set_value(payload, target, field, "")
        elif intent_type == "WHITESPACE_ONLY":
            self._set_value(payload, target, field, "   ")
        elif intent_type == "ZERO_VALUE":
            self._set_value(payload, target, field, 0)
        elif intent_type == "NEGATIVE_VALUE":
            self._set_value(payload, target, field, -1)
        elif intent_type == "EMPTY_COLLECTION":
            # Set to empty list or dict based on schema
            if schema.get("type") == "array":
                self._set_value(payload, target, field, [])
            else:
                self._set_value(payload, target, field, {})
        elif intent_type == "ARRAY_TOO_SHORT":
            self._set_value(payload, target, field, [])
        elif intent_type == "ARRAY_TOO_LONG":
            self._set_value(payload, target, field, [1] * 1000)
        elif intent_type == "ARRAY_NOT_UNIQUE":
            self._make_array_not_unique(payload, target)
        elif intent_type == "OBJECT_TOO_FEW_PROPERTIES":
            payload.clear()
        elif intent_type == "OBJECT_TOO_MANY_PROPERTIES":
            for i in range(100):
                payload[f"extra_{i}"] = i

        # Complex Objects
        elif intent_type == "OBJECT_MISSING_FIELD":
            if field:
                self._delete_field(payload, field)
        elif intent_type == "OBJECT_EXTRA_FIELD":
            if field and field in payload and isinstance(payload[field], dict):
                payload[field]["__unexpected_field__"] = "unexpected"
            else:
                payload["__unexpected_field__"] = "unexpected"

        # Security
        elif intent_type == "SQL_INJECTION":
            self._set_value(payload, target, field, "' OR '1'='1")
        elif intent_type == "XSS_INJECTION":
            self._set_value(payload, target, field, "<script>alert(1)</script>")
        elif intent_type == "PATH_TRAVERSAL":
            self._set_value(payload, target, field, "../../etc/passwd")
        elif intent_type == "COMMAND_INJECTION":
            self._set_value(payload, target, field, "; cat /etc/passwd")

        # Python Runtime Specifics
        elif intent_type == "MUTABLE_DEFAULT_TRAP":
            # Test mutable default by passing same list/dict reference indicator
            if field:
                self._set_value(
                    payload, target, field, {"__mutable_default_marker__": True}
                )

        # Polymorphism & Logic
        elif intent_type == "DISCRIMINATOR_VIOLATION":
            self._set_value(payload, target, field, "INVALID_DISCRIMINATOR_TYPE")
        elif intent_type in ["DEPENDENCY_VIOLATION", "CONDITIONAL_REQUIRED_MISSING"]:
            if field and "_requires_" in field:
                trigger, missing = field.split("_requires_")
                self._set_value(payload, target, trigger, "present")
                self._delete_field(payload, missing)

        elif intent_type == "UNION_NO_MATCH":
            self._apply_union_no_match(payload, target)

    # Helper methods
    def _set_value(self, payload: dict, target: str, field: str, value: Any):
        """Sets a value deep inside the payload based on target path."""
        if not field:
            return
        if field in payload:
            payload[field] = value
            return

        parts = target.split(".")
        try:
            body_idx = parts.index("body")
        except ValueError:
            return

        current = payload
        path_parts = parts[body_idx + 1 :]
        for part in path_parts[:-1]:
            part_clean = part.replace("[]", "").replace("*", "")
            if part_clean in current:
                current = current[part_clean]
                if isinstance(current, list) and len(current) > 0:
                    current = current[0]

        if isinstance(current, dict):
            current[field] = value

    def _delete_field(self, payload: dict, field: str):
        """Recursively deletes a field from the payload."""
        if field in payload:
            del payload[field]
            return
        for v in payload.values():
            if isinstance(v, dict):
                self._delete_field(v, field)
            elif isinstance(v, list):
                for item in v:
                    if isinstance(item, dict):
                        self._delete_field(item, field)

    def _apply_type_violation(self, payload: dict, target: str, field: str):
        if field:
            self._set_value(payload, target, field, self.INVALID_TYPE)
        else:
            parent = self._get_target_field(target)
            if parent and parent in payload:
                payload[parent] = self.INVALID_TYPE

    def _apply_union_no_match(self, payload: dict, target: str):
        parent = self._get_target_field(target)
        if not parent:
            if "mode" in payload:
                payload["mode"] = "INVALID_VARIANT_XYZ"
            payload["__FORCE_UNION_MISMATCH__"] = True
            return
        if parent in payload and isinstance(payload[parent], dict):
            payload[parent]["__FORCE_UNION_MISMATCH__"] = True
            if "mode" in payload[parent]:
                payload[parent]["mode"] = "INVALID_VARIANT_XYZ"

    def _make_array_not_unique(self, payload: dict, target: str):
        parent = self._get_target_field(target)
        if parent in payload and isinstance(payload[parent], list):
            if len(payload[parent]) > 0:
                payload[parent].append(payload[parent][0])
            else:
                payload[parent] = [1, 1]

    def _calculate_boundary_value(self, intent_type: str, schema: dict) -> Any:
        if not schema:
            return "INVALID"
        stype = schema.get("type")
        if intent_type == "BOUNDARY_MIN_MINUS_ONE":
            mn = schema.get("minimum", 0)
            return (mn - 1) if stype == "integer" else (mn - 0.01)
        if intent_type == "BOUNDARY_MAX_PLUS_ONE":
            mx = schema.get("maximum", 100)
            return (mx + 1) if stype == "integer" else (mx + 0.01)
        if intent_type == "BOUNDARY_MIN_LENGTH_MINUS_ONE":
            return "x" * max(0, schema.get("minLength", 1) - 1)
        if intent_type == "BOUNDARY_MAX_LENGTH_PLUS_ONE":
            return "x" * (schema.get("maxLength", 10) + 1)
        return "INVALID"

    def _get_target_field(self, target: str):
        parts = target.split(".")
        if "body" in parts:
            idx = parts.index("body")
            if idx + 1 < len(parts):
                return parts[idx + 1].replace("[]", "").replace("*", "")
        return None

    def _should_skip_query_type_violation(self, schema: dict) -> bool:
        if schema.get("type") == "string" and not any(
            k in schema for k in ["minLength", "pattern", "enum"]
        ):
            return True
        return False

    def _get_array_item_template(self, schema: dict, target: str, payload: dict) -> Any:
        """Get a valid item template from existing payload or generate one."""
        field = self._get_target_field(target)
        if field and field in payload and isinstance(payload[field], list) and len(payload[field]) > 0:
            return payload[field][0]
        items_schema = schema.get("items", {})
        return self._valid_value(items_schema)

===========================

====================
path: src\generators\payloads_generator\openapi_mutator\__init__.py


===========================

====================
path: src\generators\payloads_generator\python_mutator\mutator.py

"""
Python Function Mutator

Handles all mutation logic for Python function arguments.
This mutator is specifically designed for Python function testing
based on type hints, docstrings, and function signatures.
"""

from typing import Any, Dict, List, Optional
from copy import deepcopy


class PythonMutator:
    """
    Handles all mutation logic for Python function arguments and parameters.
    Generates mutated argument sets for testing Python functions.
    """

    INVALID_TYPE = "__INVALID_TYPE__"

    # --- Interface compatibility with OpenAPIMutator ---
    def mutate_body(
        self, payload: dict, intent_type: str, target: str, field: str, schema: dict
    ):
        """Alias for mutate_args to maintain interface compatibility."""
        return self.mutate_args(payload, intent_type, target, field, schema)

    def mutate_headers(self, headers: dict, intent_type: str, field: str):
        """No-op for Python functions (no HTTP headers)."""
        pass

    def mutate_path_params(
        self, path_params: dict, intent_type: str, field: str, schema: dict, ir: dict
    ):
        """No-op for Python functions (no URL path params)."""
        pass

    def mutate_query_params(
        self, payload: dict, intent_type: str, field: str, schema: dict
    ) -> bool:
        """No-op for Python functions (no query params)."""
        return True

    # --- Python-specific mutation logic ---
    def mutate_args(
        self,
        args: dict,
        intent_type: str,
        target: str,
        field: str,
        schema: dict,
    ) -> dict:
        """
        Applies mutations to Python function arguments.
        
        Args:
            args: Dictionary of argument name -> value
            intent_type: The type of mutation to apply
            target: The target path (e.g., "body.param_name")
            field: The specific field/argument to mutate
            schema: The schema for the field being mutated
            
        Returns:
            The mutated arguments dictionary
        """
        # Fallback: if field is missing, extract from target
        if not field and target:
            field = target.split(".")[-1].replace("[]", "")

        # --- 1. Structural Mutations ---
        if intent_type == "REQUIRED_ARG_MISSING" and field:
            # Remove a required argument
            args.pop(field, None)

        elif intent_type == "UNEXPECTED_ARGUMENT":
            # Add an unexpected keyword argument
            args["__unexpected_kwarg__"] = "unexpected_value"

        elif intent_type == "TOO_MANY_POS_ARGS":
            # Marker to indicate too many positional arguments
            args["__extra_positional__"] = ["extra_arg_1", "extra_arg_2"]

        # --- 2. Type Violations ---
        elif intent_type == "TYPE_VIOLATION" and field:
            # Set argument to wrong type
            self._set_invalid_type(args, field, schema)

        elif intent_type == "NULL_NOT_ALLOWED" and field:
            # Set argument to None when not allowed
            args[field] = None

        elif intent_type == "ARRAY_ITEM_TYPE_VIOLATION" and field:
            # List with wrong item type
            item_schema = schema.get("items", {})
            item_type = item_schema.get("type", "")
            if item_type == "string":
                args[field] = [12345]  # int instead of str
            elif item_type in ("integer", "number"):
                args[field] = ["wrong_type"]  # str instead of int
            else:
                args[field] = [self.INVALID_TYPE]

        elif intent_type == "DICT_KEY_TYPE_VIOLATION" and field:
            # Dict with wrong key type (Python dicts can have various key types)
            args[field] = {12345: "value"}  # int key when string expected

        elif intent_type == "DICT_VALUE_TYPE_VIOLATION" and field:
            # Dict with wrong value type
            args[field] = {"key": self.INVALID_TYPE}

        elif intent_type == "UNION_NO_MATCH" and field:
            # Value that doesn't match any union variant
            args[field] = {"__INVALID_UNION_VARIANT__": True}

        # --- 3. Constraint Violations ---
        elif intent_type == "BOUNDARY_MIN_MINUS_ONE" and field:
            val = self._calculate_boundary_value(intent_type, schema)
            args[field] = val

        elif intent_type == "BOUNDARY_MAX_PLUS_ONE" and field:
            val = self._calculate_boundary_value(intent_type, schema)
            args[field] = val

        elif intent_type == "STRING_TOO_SHORT" and field:
            min_len = schema.get("minLength", 1)
            args[field] = "x" * max(0, min_len - 1)

        elif intent_type == "STRING_TOO_LONG" and field:
            max_len = schema.get("maxLength", 100)
            args[field] = "x" * (max_len + 100)

        elif intent_type == "PATTERN_MISMATCH" and field:
            args[field] = "!!!invalid_pattern!!!"

        elif intent_type == "ENUM_MISMATCH" and field:
            args[field] = "INVALID_ENUM_VALUE"

        elif intent_type == "NOT_MULTIPLE_OF" and field:
            multiple = schema.get("multipleOf", 2)
            args[field] = multiple + 1  # Not a multiple

        # --- 4. Data Edge Cases ---
        elif intent_type == "EMPTY_STRING" and field:
            args[field] = ""

        elif intent_type == "WHITESPACE_ONLY" and field:
            args[field] = "   "

        elif intent_type == "ZERO_VALUE" and field:
            args[field] = 0

        elif intent_type == "NEGATIVE_VALUE" and field:
            args[field] = -1

        elif intent_type == "EMPTY_COLLECTION" and field:
            # Empty list or dict based on schema
            if schema.get("type") == "array":
                args[field] = []
            else:
                args[field] = {}

        # --- 5. Complex Objects ---
        elif intent_type == "OBJECT_MISSING_FIELD" and field:
            # Remove a required field from a nested object
            if field in args and isinstance(args[field], dict):
                # Remove first key from the object
                if args[field]:
                    first_key = next(iter(args[field]))
                    del args[field][first_key]

        elif intent_type == "OBJECT_EXTRA_FIELD" and field:
            # Add an unexpected field to a nested object
            if field in args and isinstance(args[field], dict):
                args[field]["__unexpected_field__"] = "unexpected"
            else:
                args["__unexpected_field__"] = "unexpected"

        # --- 6. Security Fuzzing ---
        elif intent_type == "SQL_INJECTION" and field:
            args[field] = "' OR '1'='1"

        elif intent_type == "XSS_INJECTION" and field:
            args[field] = "<script>alert(1)</script>"

        elif intent_type == "PATH_TRAVERSAL" and field:
            args[field] = "../../etc/passwd"

        elif intent_type == "COMMAND_INJECTION" and field:
            args[field] = "; cat /etc/passwd"

        # --- 7. Python Runtime Specifics ---
        elif intent_type == "MUTABLE_DEFAULT_TRAP" and field:
            # Marker to test mutable default argument anti-pattern
            args[field] = {"__mutable_default_marker__": True}

        return args

    def _set_invalid_type(self, args: dict, field: str, schema: dict):
        """Sets an invalid type value based on what type is expected."""
        expected_type = schema.get("type", "string")
        
        if expected_type == "integer":
            args[field] = "not_an_integer"
        elif expected_type == "number":
            args[field] = "not_a_number"
        elif expected_type == "string":
            args[field] = 12345  # int instead of str
        elif expected_type == "boolean":
            args[field] = "not_a_boolean"
        elif expected_type == "array":
            args[field] = "not_an_array"
        elif expected_type == "object":
            args[field] = "not_an_object"
        else:
            args[field] = self.INVALID_TYPE

    def _calculate_boundary_value(self, intent_type: str, schema: dict) -> Any:
        """Calculate boundary values for schema constraints."""
        if not schema:
            return "INVALID"
        
        stype = schema.get("type")
        
        if intent_type == "BOUNDARY_MIN_MINUS_ONE":
            mn = schema.get("minimum", 0)
            return (mn - 1) if stype == "integer" else (mn - 0.01)
        
        if intent_type == "BOUNDARY_MAX_PLUS_ONE":
            mx = schema.get("maximum", 100)
            return (mx + 1) if stype == "integer" else (mx + 0.01)
        
        return "INVALID"

    def generate_positional_args(self, args: dict, param_order: List[str]) -> tuple:
        """
        Converts keyword arguments to positional arguments based on parameter order.
        
        Args:
            args: Dictionary of argument name -> value
            param_order: List of parameter names in positional order
            
        Returns:
            Tuple of positional arguments
        """
        positional = []
        for param in param_order:
            if param in args:
                positional.append(args[param])
            else:
                break  # Stop at first missing positional arg
        return tuple(positional)

    def generate_keyword_args(self, args: dict, param_order: List[str]) -> dict:
        """
        Extracts keyword-only arguments (those not in the positional order).
        
        Args:
            args: Dictionary of argument name -> value
            param_order: List of parameter names in positional order
            
        Returns:
            Dictionary of keyword arguments
        """
        positional_set = set(param_order)
        return {k: v for k, v in args.items() if k not in positional_set}

===========================

====================
path: src\generators\payloads_generator\python_mutator\__init__.py


===========================

====================
path: src\generators\payloads_generator\typescript_mutator\mutator.py

"""
TypeScript Function Mutator

Handles all mutation logic for TypeScript function parameters.
This mutator is specifically designed for TypeScript function testing
based on type annotations, interfaces, and generics.
"""

from typing import Any, Dict, List, Optional
from copy import deepcopy


class TypeScriptMutator:
    """
    Handles all mutation logic for TypeScript function parameters.
    Generates mutated argument sets for testing TypeScript functions.
    """

    INVALID_TYPE = "__INVALID_TYPE__"

    # --- Interface compatibility with OpenAPIMutator ---
    def mutate_body(
        self, payload: dict, intent_type: str, target: str, field: str, schema: dict
    ):
        """Alias for mutate_args to maintain interface compatibility."""
        return self.mutate_args(payload, intent_type, target, field, schema)

    def mutate_headers(self, headers: dict, intent_type: str, field: str):
        """No-op for TypeScript functions (no HTTP headers)."""
        pass

    def mutate_path_params(
        self, path_params: dict, intent_type: str, field: str, schema: dict, ir: dict
    ):
        """No-op for TypeScript functions (no URL path params)."""
        pass

    def mutate_query_params(
        self, payload: dict, intent_type: str, field: str, schema: dict
    ) -> bool:
        """No-op for TypeScript functions (no query params)."""
        return True

    # --- TypeScript-specific mutation logic ---
    def mutate_args(
        self,
        args: dict,
        intent_type: str,
        target: str,
        field: str,
        schema: dict,
    ) -> dict:
        """
        Applies mutations to TypeScript function parameters.
        
        Args:
            args: Dictionary of parameter name -> value
            intent_type: The type of mutation to apply
            target: The target path (e.g., "body.param_name")
            field: The specific field/parameter to mutate
            schema: The schema for the field being mutated
            
        Returns:
            The mutated arguments dictionary
        """
        # Fallback: if field is missing, extract from target
        if not field and target:
            field = target.split(".")[-1].replace("[]", "")

        # --- 1. Structural Mutations ---
        if intent_type == "REQUIRED_ARG_MISSING" and field:
            # Remove a required parameter
            args.pop(field, None)

        elif intent_type == "UNEXPECTED_ARGUMENT":
            # Add an unexpected parameter
            args["__unexpected_param__"] = "unexpected_value"

        elif intent_type == "TOO_MANY_POS_ARGS":
            # Marker to indicate too many arguments
            args["__extra_positional__"] = ["extra_arg_1", "extra_arg_2"]

        # --- 2. Type Violations ---
        elif intent_type == "TYPE_VIOLATION" and field:
            # Set parameter to wrong type
            self._set_invalid_type(args, field, schema)

        elif intent_type == "NULL_NOT_ALLOWED" and field:
            # Set parameter to null when not allowed (undefined in TS context)
            args[field] = None

        elif intent_type == "ARRAY_ITEM_TYPE_VIOLATION" and field:
            # Array with wrong item type
            item_schema = schema.get("items", {})
            item_type = item_schema.get("type", "")
            if item_type == "string":
                args[field] = [12345]  # number instead of string
            elif item_type in ("integer", "number"):
                args[field] = ["wrong_type"]  # string instead of number
            else:
                args[field] = [self.INVALID_TYPE]

        elif intent_type == "UNION_NO_MATCH" and field:
            # Value that doesn't match any union type
            args[field] = {"__INVALID_UNION_VARIANT__": True}

        # --- 3. TypeScript-Specific Types ---
        elif intent_type == "GENERIC_TYPE_VIOLATION" and field:
            # Violate a generic type constraint
            args[field] = {"__GENERIC_VIOLATION__": True}

        elif intent_type == "INTERFACE_MISSING_PROPERTY" and field:
            # Remove a required property from an interface
            if field in args and isinstance(args[field], dict):
                if args[field]:
                    first_key = next(iter(args[field]))
                    del args[field][first_key]

        elif intent_type == "READONLY_MUTATION" and field:
            # Marker to test readonly property mutation
            args[field] = {"__READONLY_MUTATION__": True}

        # --- 4. Constraint Violations ---
        elif intent_type == "BOUNDARY_MIN_MINUS_ONE" and field:
            val = self._calculate_boundary_value(intent_type, schema)
            args[field] = val

        elif intent_type == "BOUNDARY_MAX_PLUS_ONE" and field:
            val = self._calculate_boundary_value(intent_type, schema)
            args[field] = val

        elif intent_type == "STRING_TOO_SHORT" and field:
            min_len = schema.get("minLength", 1)
            args[field] = "x" * max(0, min_len - 1)

        elif intent_type == "STRING_TOO_LONG" and field:
            max_len = schema.get("maxLength", 100)
            args[field] = "x" * (max_len + 100)

        elif intent_type == "PATTERN_MISMATCH" and field:
            args[field] = "!!!invalid_pattern!!!"

        elif intent_type == "ENUM_MISMATCH" and field:
            args[field] = "INVALID_ENUM_VALUE"

        # --- 5. Data Edge Cases ---
        elif intent_type == "EMPTY_STRING" and field:
            args[field] = ""

        elif intent_type == "WHITESPACE_ONLY" and field:
            args[field] = "   "

        elif intent_type == "ZERO_VALUE" and field:
            args[field] = 0

        elif intent_type == "NEGATIVE_VALUE" and field:
            args[field] = -1

        elif intent_type == "EMPTY_COLLECTION" and field:
            if schema.get("type") == "array":
                args[field] = []
            else:
                args[field] = {}

        # --- 6. Complex Objects ---
        elif intent_type == "OBJECT_MISSING_FIELD" and field:
            if field in args and isinstance(args[field], dict):
                if args[field]:
                    first_key = next(iter(args[field]))
                    del args[field][first_key]

        elif intent_type == "OBJECT_EXTRA_FIELD" and field:
            if field in args and isinstance(args[field], dict):
                args[field]["__unexpected_field__"] = "unexpected"
            else:
                args["__unexpected_field__"] = "unexpected"

        # --- 7. Security Fuzzing ---
        elif intent_type == "SQL_INJECTION" and field:
            args[field] = "' OR '1'='1"

        elif intent_type == "XSS_INJECTION" and field:
            args[field] = "<script>alert(1)</script>"

        elif intent_type == "PATH_TRAVERSAL" and field:
            args[field] = "../../etc/passwd"

        elif intent_type == "COMMAND_INJECTION" and field:
            args[field] = "; cat /etc/passwd"

        return args

    def _set_invalid_type(self, args: dict, field: str, schema: dict):
        """Sets an invalid type value based on what type is expected."""
        expected_type = schema.get("type", "string")
        
        if expected_type == "integer" or expected_type == "number":
            args[field] = "not_a_number"
        elif expected_type == "string":
            args[field] = 12345  # number instead of string
        elif expected_type == "boolean":
            args[field] = "not_a_boolean"
        elif expected_type == "array":
            args[field] = "not_an_array"
        elif expected_type == "object":
            args[field] = "not_an_object"
        else:
            args[field] = self.INVALID_TYPE

    def _calculate_boundary_value(self, intent_type: str, schema: dict) -> Any:
        """Calculate boundary values for schema constraints."""
        if not schema:
            return "INVALID"
        
        stype = schema.get("type")
        
        if intent_type == "BOUNDARY_MIN_MINUS_ONE":
            mn = schema.get("minimum", 0)
            return (mn - 1) if stype == "integer" else (mn - 0.01)
        
        if intent_type == "BOUNDARY_MAX_PLUS_ONE":
            mx = schema.get("maximum", 100)
            return (mx + 1) if stype == "integer" else (mx + 0.01)
        
        return "INVALID"

===========================

====================
path: src\generators\payloads_generator\typescript_mutator\__init__.py


===========================

====================
path: src\llm_enhancer\circuit_breaker.py

from testsuitegen.src.exceptions.exceptions import LLMError
from testsuitegen.src.config.settings import CIRCUIT_BREAKER_FAILURE_THRESHOLD


class LLMCircuitBreaker:
    """
    Prevents cascading failures by stopping LLM calls after N consecutive failures.
    """

    def __init__(self, failure_threshold: int = None):
        self.failure_threshold = failure_threshold or CIRCUIT_BREAKER_FAILURE_THRESHOLD
        self.consecutive_failures = 0
        self.is_open = False  # Circuit is OPEN (blocking) or CLOSED (allowing)

    def record_success(self):
        self.consecutive_failures = 0
        self.is_open = False
        print("Circuit Breaker: Success recorded. Circuit is CLOSED.")

    def record_failure(self):
        self.consecutive_failures += 1
        if self.consecutive_failures >= self.failure_threshold:
            self.is_open = True
            print(
                f"Circuit Breaker TRIPPED: {self.consecutive_failures} consecutive failures. "
                "Stopping LLM calls for this session."
            )
            raise LLMError(
                f"Circuit Breaker Tripped: LLM failed {self.consecutive_failures} times consecutively."
            )

    def check_state(self):
        if self.is_open:
            raise LLMError(
                "Circuit Breaker is OPEN. Blocking LLM call to save resources."
            )


circuit_breaker = LLMCircuitBreaker()

===========================

====================
path: src\llm_enhancer\client.py

# llm_enhancer/client.py

from typing import Optional
from testsuitegen.src.config.settings import LLMProviders
from testsuitegen.src.llm_enhancer.providers.config import ProviderConfig
from testsuitegen.src.llm_enhancer.providers.factory import (
    ProviderFactory,
    BaseLLMProvider,
)


# Global provider cache to avoid recreating providers
_provider_cache = {}


def _clone_config_with_model(
    base_config: ProviderConfig, model_override: str
) -> ProviderConfig:
    return ProviderConfig(
        name=base_config.name,
        model=model_override or base_config.model,
        temperature=base_config.temperature,
        max_tokens=base_config.max_tokens,
        timeout=base_config.timeout,
        base_url=base_config.base_url,
    )


def _get_provider(
    provider_enum: LLMProviders, model_override: str = None
) -> BaseLLMProvider:
    """Get or create a provider instance keyed by provider name and model."""

    provider_name = provider_enum.value.name
    cache_key = f"{provider_name}:{model_override or provider_enum.value.model}"

    if cache_key not in _provider_cache:
        config = (
            _clone_config_with_model(provider_enum.value, model_override)
            if model_override
            else provider_enum.value
        )

        _provider_cache[cache_key] = ProviderFactory.create_provider(
            provider_config=config
        )

    return _provider_cache[cache_key]


def llm_generate(
    prompt: str,
    provider: Optional[str] = None,
    model_override: Optional[str] = None,
    max_retries: int = 3,
    **kwargs,
) -> str:
    """Generate text using the selected LLM provider.

    Args:
        prompt: Input prompt
        provider: Provider name to use (None = use default provider)
        max_retries: Maximum retry attempts
        **kwargs: Additional provider-specific arguments (e.g., response_format)

    Returns:
        Generated text

    Raises:
        ValueError: If provider is unknown or not configured
        Exception: If generation fails
    """
    # Resolve provider
    if provider:
        provider_enum = LLMProviders.get_by_name(provider)
        if not provider_enum:
            raise ValueError(f"Unknown provider: {provider}")
    else:
        provider_enum = LLMProviders.get_default_provider()

    if not provider_enum:
        raise ValueError("No LLM provider configured")

    if not provider_enum.value.is_available:
        raise ValueError(
            f"Provider '{provider_enum.value.name}' is not properly configured"
        )

    provider_instance = _get_provider(provider_enum, model_override=model_override)
    return provider_instance.generate(prompt, max_retries=max_retries, **kwargs)

===========================

====================
path: src\llm_enhancer\__init__.py


===========================

====================
path: src\llm_enhancer\payload_enhancer\enhancer.py

# llm_enhancer/payload_enhancer/enhancer.py

import json
import time

from testsuitegen.src.llm_enhancer.client import llm_generate
from testsuitegen.src.llm_enhancer.payload_enhancer.prompts import (
    ENHANCE_PAYLOAD_PROMPT,
)
from testsuitegen.src.llm_enhancer.payload_enhancer.validator import (
    validate_payload_structure,
)
from testsuitegen.src.llm_enhancer.circuit_breaker import circuit_breaker
from testsuitegen.src.config.settings import MAX_LLM_RETRIES, EXPONENTIAL_BACKOFF_BASE
from testsuitegen.src.exceptions.exceptions import LLMError, LLMFatalError


def enhance_payload(
    payload: dict,
    operation_id: str = None,
    intent: str = "HAPPY_PATH",
    schema_info: str = None,
    provider: str = None,
    model: str = None,
    max_retries: int = None,
    **kwargs,
) -> dict:
    """Enhance payload with realistic values using LLM with Resilience Layer.

    Args:
        payload: Original payload dictionary
        operation_id: API operation identifier
        intent: Test intent type
        schema_info: Additional schema context
        provider: LLM provider to use (None = use default)
        max_retries: Number of retry attempts on transient errors

    Returns:
        Enhanced payload (or original if enhancement fails)

    RULES:
    - Only enhances HAPPY_PATH intents (and optionally VALID_BOUNDARY, VALID_ENUM)
    - Does NOT add, remove, or rename keys
    - Does NOT change data types
    - Does NOT change array sizes
    - Falls back to original payload if structure changes

    RESILIENCE:
    - Circuit Breaker: Stops LLM calls after repeated failures
    - Exponential Backoff: Retries with increasing delays (2s, 4s, 8s)
    - Strict JSON Validation: Rejects non-JSON responses
    """

    # Only enhance valid/happy path intents
    if intent not in ["HAPPY_PATH", "VALID_BOUNDARY", "VALID_ENUM"]:
        return payload

    # Use config default if not specified
    if max_retries is None:
        max_retries = MAX_LLM_RETRIES

    # 1. Check Circuit Breaker before attempting
    try:
        circuit_breaker.check_state()
    except LLMError as e:
        print(
            f"Circuit Breaker blocking LLM call for {operation_id}. Returning raw payload."
        )
        return payload

    # Build schema context if available
    schema_context = ""
    if schema_info:
        schema_context = f"\nSchema Context:\n{schema_info}"

    prompt = (
        ENHANCE_PAYLOAD_PROMPT.replace("{operation_id}", operation_id or "unknown")
        .replace("{intent}", intent)
        .replace("{schema_info}", schema_context)
        .replace("{payload}", json.dumps(payload, indent=2))
    )
    print(
        f"Sending payload to LLM for enhancement with {provider or 'default'}:{model or 'default'}..."
    )

    # 2. Exponential Backoff Retry Loop
    for attempt in range(1, max_retries + 1):
        try:
            print(f"LLM Attempt {attempt}/{max_retries} for {operation_id}...")

            # Progressive Backoff Temperature: Increase temp by 0.1 for each retry to break loops
            base_temp = 0.01
            if attempt > 1:
                kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)
            # Call LLM
            enhanced_text = llm_generate(
                prompt, provider=provider, model_override=model, **kwargs
            )

            # 3. STRICT JSON VALIDATION - Check for non-JSON headers (Hallucination)
            enhanced_text = enhanced_text.strip()

            # Remove markdown code blocks if present
            if enhanced_text.startswith("```json"):
                enhanced_text = enhanced_text[7:]  # Remove ```json
            elif enhanced_text.startswith("```"):
                enhanced_text = enhanced_text[3:]  # Remove ```

            if enhanced_text.endswith("```"):
                enhanced_text = enhanced_text[:-3]  # Remove closing ```

            enhanced_text = enhanced_text.strip()

            # Check if response starts with valid JSON
            if not enhanced_text.startswith(("{", "[")):
                print(
                    f"   LLM returned non-JSON header (Hallucination). Retrying... Attempt {attempt}"
                )
                raise ValueError("Response did not start with JSON object/array")

            # Parse JSON
            try:
                enhanced = json.loads(enhanced_text)
            except json.JSONDecodeError as e:
                print(
                    f"   LLM returned invalid JSON: {e}. Retrying... Attempt {attempt}"
                )
                raise ValueError(f"Invalid JSON returned: {e}")

            # Handle case where enhanced is an array of objects with a 'payload' key
            if (
                isinstance(enhanced, list)
                and len(enhanced) > 0
                and isinstance(payload, dict)
            ):
                first_item = enhanced[0]
                if isinstance(first_item, dict) and "payload" in first_item:
                    enhanced = first_item["payload"]

            # Validate structure
            if not validate_payload_structure(payload, enhanced):
                print(f"   Structure validation failed. Retrying... Attempt {attempt}")
                raise ValueError(f"Structure changed for operation {operation_id}")

            # Check if placeholders were actually replaced
            enhanced_str = json.dumps(enhanced)
            if "__PLACEHOLDER_" in enhanced_str:
                print(
                    f"   LLM did not replace placeholders. Retrying... Attempt {attempt}"
                )
                raise ValueError("Placeholders not replaced")

            # 4. SUCCESS: Record Success and Return
            circuit_breaker.record_success()
            print(f"   LLM Enhancement successful for {operation_id}")
            return enhanced

        except ValueError as ve:
            # Validation errors (JSON, structure, placeholders)
            print(f"   Validation Error: {ve}")
            if attempt == max_retries:
                print(f"   Max retries ({max_retries}) reached for validation errors.")
                circuit_breaker.record_failure()
                return payload
            time.sleep(2**attempt)  # Exponential backoff: 2s, 4s, 8s

        except LLMFatalError as lf:
            # Non-retryable policy/config error from provider - abort immediately
            print(
                f"   Non-retryable LLM error: {lf}. Aborting enhancement for {operation_id}."
            )
            circuit_breaker.record_failure()
            return payload

        except Exception as e:
            # Transient network/API errors
            print(f"   LLM API Error (Attempt {attempt}): {e}")
            if attempt == max_retries:
                print(f"   Max retries ({max_retries}) reached for API errors.")
                circuit_breaker.record_failure()
                return payload
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)  # Exponential backoff

    # Should not reach here, but just in case
    return payload

===========================

====================
path: src\llm_enhancer\payload_enhancer\prompts.py

ENHANCE_PAYLOAD_PROMPT = """
<role>
You are a Strict JSON Editor. Your job is to find and replace specific placeholder text strings within a given JSON object. You are NOT a generator; you do not create new structures.
</role>

<context>
Operation ID: {operation_id}
Test Intent: {intent}

<schema_info>
{schema_info}
</schema_info>
</context>

<task>
1. Look at the <input_payload>.
2. Find every string value that matches the pattern `__PLACEHOLDER_*__`.
3. Replace those specific strings with realistic values.
4. Return the modified JSON.
</task>

<critical_rules>
1. INPUT PAYLOAD IS THE TRUTH:
   - The <input_payload> is the **only** definition of structure.
   - If the input has an empty object `{}`, the output MUST be `{}`.
   - If the input has an empty array `[]`, the output MUST be `[]`.
   - Do NOT add keys, properties, or array items that are missing from the input.

2. SCHEMA WARNING:
   - The <schema_info> above contains definitions for "required" fields and complex structures.
   - **IGNORE** the schema regarding structure.
   - Use the schema ONLY to infer formats (e.g., knowing "street" implies a text address).
   - Even if the schema says a field is "required", if it is not in the input payload, DO NOT ADD IT.

3. STRICT REPLACEMENT:
   - ONLY replace strings that start with `__PLACEHOLDER_` and end with `__`.
   - Do not touch `__INVALID_TYPE__`, `__NULL__`, or other markers.

4. PATTERN CONSTRAINTS:
   - If the <schema_info> specifies a `pattern` regex for a field, the replacement value MUST match that pattern.
   - Example: If `username` has pattern `^[a-zA-Z0-9_]+$`, do NOT use dots or special characters.

5. NUMERIC PRECISION:
   - If the placeholder ends with `_INTEGER_...`, output a WHOLE NUMBER (no decimals).
   - If the placeholder ends with `_NUMBER_...`, output a FLOAT/DECIMAL (e.g., 10.5, 0.99) unless the context strictly implies an integer count.
   - Respect the implied type of the field if known.

6. DEFINITIVE BOUNDARY VIOLATIONS:
   - When generating "INVALID" or "NEGATIVE" values for boundary tests:
   - **MaxLength:** Exceed by at least 5 characters (not just 1) to avoid off-by-one ambiguities.
   - **MinLength:** Use extremely short values (e.g., 1 char or empty string) if minLength > 1.
   - **Minimum/Maximum:** Exceed by a significant margin (e.g., +10 or -10) unless strict boundary testing is requested.
   - **Pattern:** Use values that clearly violate the regex (e.g., "INVALID_CHARS_!@#" for alphanumeric).
</critical_rules>

<pattern_dictionary>
1.  __PLACEHOLDER_STRING_[Field]__       -> Contextual text (e.g., "Standard Plan", "UUID-1234").
2.  __PLACEHOLDER_EMAIL_[Field]__       -> Valid email (e.g., `alicesmith@domain.com`).
3.  __PLACEHOLDER_UUID_[Field]__        -> UUID v4 format (e.g., `f47ac10b-58cc-4372-a567-0e02b2c3d479`).
4.  __PLACEHOLDER_URI_[Field]__         -> Valid URL (e.g., `https://api.example.com/v1/resource`).
5.  __PLACEHOLDER_DATE_[Field]__        -> YYYY-MM-DD (e.g., `2024-11-20`).
6.  __PLACEHOLDER_DATETIME_[Field]__    -> ISO 8601 (e.g., `2024-11-20T14:30:00Z`).
7.  __PLACEHOLDER_INTEGER_[Field]__     -> Realistic integer (e.g., 42, 100, 5000).
8.  __PLACEHOLDER_INTEGER_MIN[N]_[Field]__ -> Integer >= N.
9.  __PLACEHOLDER_INTEGER_MAX[N]_[Field]__ -> Integer <= N.
10. __PLACEHOLDER_NUMBER_[Field]__      -> Realistic float (e.g., 19.99, 3.1415).
11. __PLACEHOLDER_BOOLEAN_[Field]__     -> `true` or `false`.
</pattern_dictionary>

<examples>

Example 1: Empty Objects Must Stay Empty (Schema Ignorance)
Input:  {{"reference_id": "__PLACEHOLDER_STRING_reference_id__", "sender": {}, "recipient": {}}}
Output: {{"reference_id": "REF-999", "sender": {}, "recipient": {}}}

Example 2: Nested Structures
Input:  {{"address": {{"street": "__PLACEHOLDER_STRING_street__", "zip": "__PLACEHOLDER_INTEGER_zip__"}}, "history": []}}
Output: {{"address": {{"street": "123 Innovation Drive", "zip": 94043}}, "history": []}}

Example 3: Strict Types (No quotes on numbers)
Input:  {{"quantity": "__PLACEHOLDER_INTEGER_quantity__"}}
Output: {{"quantity": 15}}

</examples>

<input_payload>
{payload}
</input_payload>

<output_format>
ABSOLUTE SILENCE RULES:
- The output must be valid JSON only.
- The **VERY FIRST** character of your response must be `{` or `[`.
- The **VERY LAST** character of your response must be `}` or `]`.
- NO markdown formatting (no ```json ... ```, no **bold**).
- NO introductory text (e.g., do not say "Here is the JSON").
- NO explanatory text (e.g., do not say "I replaced the placeholder").
- NO headers or titles.
- Failure to adhere to the "First/Last Character" rule is a failure.
</output_format>
"""

===========================

====================
path: src\llm_enhancer\payload_enhancer\validator.py

def validate_payload_structure(original: dict, enhanced: dict) -> bool:
    """
    Validate that payload structure remains identical.

    Checks:
    - Same keys
    - Same types (with special handling for placeholders)
    - Same array lengths
    - Recursive validation for nested dicts/lists
    - Empty dicts must stay empty
    """
    # Check if both dictionaries have the same keys
    if original.keys() != enhanced.keys():
        return False

    for k in original:
        orig_val = original[k]
        enh_val = enhanced[k]

        # Special handling for placeholder strings
        if isinstance(orig_val, str) and orig_val.startswith("__PLACEHOLDER_"):
            # Allow any type replacement for placeholders
            continue

        # Check if the types of the original and enhanced values match
        is_orig_num = isinstance(orig_val, (int, float)) and not isinstance(
            orig_val, bool
        )
        is_enh_num = isinstance(enh_val, (int, float)) and not isinstance(enh_val, bool)

        if is_orig_num and is_enh_num:
            continue
        elif type(orig_val) != type(enh_val):
            return False

        # Recursively validate nested dictionaries
        if isinstance(orig_val, dict):
            # Allow empty original dicts to be populated
            if not orig_val:
                continue
            if not validate_payload_structure(orig_val, enh_val):
                return False

        # Validate lists: ensure same length and recursively validate elements
        if isinstance(orig_val, list):
            # if len(orig_val) != len(enh_val):
            #     return False
            for o, e in zip(orig_val, enh_val):
                # Ensure empty dictionaries in lists remain empty
                if isinstance(o, dict) and len(o) == 0:
                    # if not (isinstance(e, dict) and len(e) == 0):
                    #     return False
                    continue
                # Check if the types of the elements match
                if type(o) != type(e):
                    return False
                # Recursively validate nested dictionaries in lists
                if isinstance(o, dict):
                    if not validate_payload_structure(o, e):
                        return False
    return True

===========================

====================
path: src\llm_enhancer\payload_enhancer\__init__.py

"""Payload Enhancer - Enhances test payloads with realistic data."""

from .enhancer import enhance_payload
from .prompts import ENHANCE_PAYLOAD_PROMPT
from .validator import validate_payload_structure

__all__ = ["enhance_payload", "ENHANCE_PAYLOAD_PROMPT", "validate_payload_structure"]

===========================

====================
path: src\llm_enhancer\providers\airllm.py

"""AirLLM provider for running large models with low memory."""

import time
from typing import Optional
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider


class AirLLMProvider(BaseLLMProvider):
    """AirLLM provider for memory-efficient inference of large models."""

    def __init__(
        self,
        model_path: str,
        model: Optional[str] = None,
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
        compression: Optional[str] = None,
        layer_shards_saving_path: Optional[str] = None,
    ):
        """Initialize AirLLM provider.

        Args:
            model_path: HuggingFace model repo ID or local path
            model: Model identifier (optional, for compatibility)
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
            compression: Compression mode ('4bit', '8bit', or None)
            layer_shards_saving_path: Path to save split model layers
        """
        super().__init__(
            model=model or model_path,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        self.model_path = model_path
        self.compression = compression
        self.layer_shards_saving_path = layer_shards_saving_path
        self._model = None
        self._tokenizer = None

    def _get_model(self):
        """Lazy initialization of AirLLM model."""
        if self._model is None:
            try:
                from airllm import AutoModel

                kwargs = {}
                if self.compression:
                    kwargs["compression"] = self.compression
                if self.layer_shards_saving_path:
                    kwargs["layer_shards_saving_path"] = self.layer_shards_saving_path

                self._model = AutoModel.from_pretrained(self.model_path, **kwargs)
                self._tokenizer = self._model.tokenizer
            except ImportError:
                raise ImportError(
                    "airllm package not installed. Run: pip install airllm"
                )
            except Exception as e:
                raise Exception(f"Failed to load AirLLM model: {e}")
        return self._model

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using AirLLM.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts

        Returns:
            Generated text
        """
        model = self._get_model()

        for attempt in range(max_retries):
            try:
                # Tokenize input
                input_tokens = self._tokenizer(
                    prompt,
                    return_tensors="pt",
                    return_attention_mask=False,
                    truncation=True,
                    max_length=4096,  # Context window
                    padding=False,
                )

                # Generate response
                generation_output = model.generate(
                    input_tokens["input_ids"].cuda(),
                    max_new_tokens=self.max_tokens,
                    temperature=self.temperature,
                    use_cache=True,
                    return_dict_in_generate=True,
                )

                # Decode output
                output = self._tokenizer.decode(
                    generation_output.sequences[0], skip_special_tokens=True
                )

                # Remove the input prompt from output if present
                if output.startswith(prompt):
                    output = output[len(prompt) :].strip()

                return output

            except Exception as e:
                error_msg = str(e).lower()
                # Retry on temporary errors
                if attempt < max_retries - 1 and (
                    "timeout" in error_msg
                    or "connection" in error_msg
                    or "cuda" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                raise Exception(f"AirLLM generation failed: {e}")

        raise Exception("AirLLM generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if AirLLM is available."""
        try:
            import airllm

            return bool(self.model_path)
        except ImportError:
            return False

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "airllm"

===========================

====================
path: src\llm_enhancer\providers\base.py

"""Base provider interface for LLM providers."""

from abc import ABC, abstractmethod
from typing import Optional


class BaseLLMProvider(ABC):
    """Abstract base class for LLM providers."""

    def __init__(
        self,
        model: Optional[str] = None,
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
    ):
        """Initialize the provider with common settings.

        Args:
            model: Model name to use (provider-specific)
            temperature: Sampling temperature (0.0 to 1.0)
            max_tokens: Maximum tokens in response
            timeout: Request timeout in seconds
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

    @abstractmethod
    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text from the LLM.

        Args:
            prompt: The input prompt
            max_retries: Maximum number of retry attempts for transient errors

        Returns:
            Generated text response

        Raises:
            Exception: If generation fails after all retries
        """
        pass

    @property
    @abstractmethod
    def is_available(self) -> bool:
        """Check if the provider is properly configured and available.

        Returns:
            True if provider can be used, False otherwise
        """
        pass

    @property
    @abstractmethod
    def provider_name(self) -> str:
        """Get the name of this provider.

        Returns:
            Provider name (e.g., "gemini", "groq", "lmstudio")
        """
        pass

===========================

====================
path: src\llm_enhancer\providers\config.py

"""
Provider configuration module.

Contains the ProviderConfig dataclass used by all LLM providers.
"""

import os
from dataclasses import dataclass
from typing import Optional


@dataclass
class ProviderConfig:
    """
    Configuration for a single LLM provider.

    Attributes:
        name: Provider identifier (e.g., 'gemini', 'groq')
        model: Model name/identifier for the provider
        temperature: Sampling temperature (0.0-1.0, lower = more deterministic)
        max_tokens: Maximum tokens in response
        timeout: Request timeout in seconds
        base_url: Optional base URL for self-hosted providers
    """

    name: str
    model: str
    temperature: float
    max_tokens: int
    timeout: int
    base_url: Optional[str] = None

    def get_api_key(self) -> Optional[str]:
        """
        Retrieve API key from environment variables.

        Returns:
            API key string if configured, None otherwise
        """
        if self.name == "gemini":
            return os.getenv("GEMINI_API_KEY")
        elif self.name == "groq":
            return os.getenv("GROQ_API_KEY")
        elif self.name == "lmstudio":
            return os.getenv("LMSTUDIO_BASE_URL")
        elif self.name == "vllm":
            # For vLLM, we treat the base_url (if hardcoded) as the 'key' for availability
            return self.base_url or os.getenv("VLLM_BASE_URL")
        elif self.name == "airllm":
            return os.getenv("AIRLLM_MODEL_PATH")
        return None

    @property
    def is_available(self) -> bool:
        """
        Check if provider is properly configured and available.

        Returns:
            True if API key/configuration is present, False otherwise
        """
        key = self.get_api_key()
        return (key is not None and len(key) > 0) or (
            self.base_url is not None and len(self.base_url) > 0
        )

===========================

====================
path: src\llm_enhancer\providers\factory.py

"""Provider factory for creating LLM provider instances."""

from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider
from testsuitegen.src.llm_enhancer.providers.gemini import GeminiProvider
from testsuitegen.src.llm_enhancer.providers.groq import GroqProvider
from testsuitegen.src.llm_enhancer.providers.lmstudio import LMStudioProvider
from testsuitegen.src.llm_enhancer.providers.vllm import VLLMProvider
from testsuitegen.src.llm_enhancer.providers.airllm import AirLLMProvider
from testsuitegen.src.llm_enhancer.providers.config import ProviderConfig


class ProviderFactory:
    """Factory for creating and managing LLM providers."""

    @staticmethod
    def create_provider(provider_config: ProviderConfig) -> BaseLLMProvider:
        """Create an LLM provider instance from ProviderConfig.

        Args:
            provider_config: Provider configuration from enum

        Returns:
            Configured provider instance

        Raises:
            ValueError: If provider_type is unknown or not configured
        """
        provider_name = provider_config.name.lower()
        api_key = provider_config.get_api_key()

        if provider_name == "gemini":
            if not api_key:
                raise ValueError("Gemini provider requires GEMINI_API_KEY in .env")
            return GeminiProvider(
                api_key=api_key,
                model=provider_config.model,
                temperature=provider_config.temperature,
                max_tokens=provider_config.max_tokens,
                timeout=provider_config.timeout,
            )

        elif provider_name == "groq":
            if not api_key:
                raise ValueError("Groq provider requires GROQ_API_KEY in .env")
            return GroqProvider(
                api_key=api_key,
                model=provider_config.model,
                temperature=provider_config.temperature,
                max_tokens=provider_config.max_tokens,
                timeout=provider_config.timeout,
            )

        elif provider_name == "lmstudio":
            if not api_key:
                raise ValueError("LM Studio requires LMSTUDIO_BASE_URL in .env")
            return LMStudioProvider(
                base_url=provider_config.base_url or api_key,
                model=provider_config.model,
                api_key="not-needed",
                temperature=provider_config.temperature,
                max_tokens=provider_config.max_tokens,
                timeout=provider_config.timeout,
            )

        elif provider_name == "vllm":
            # vLLM setup (base_url from config or env)
            return VLLMProvider(
                base_url=provider_config.base_url
                or api_key,  # Use base_url from config
                model=provider_config.model,
                api_key="not-needed",  # vLLM typically doesn't check keys
                temperature=provider_config.temperature,
                max_tokens=provider_config.max_tokens,
                timeout=provider_config.timeout,
            )

        elif provider_name == "airllm":
            if not api_key:
                raise ValueError("AirLLM requires AIRLLM_MODEL_PATH in .env")
            return AirLLMProvider(
                model_path=api_key,
                model=provider_config.model,
                temperature=provider_config.temperature,
                max_tokens=provider_config.max_tokens,
                timeout=provider_config.timeout,
            )

        else:
            raise ValueError(
                f"Unknown provider: {provider_name}. Supported: gemini, groq, lmstudio, vllm, airllm"
            )

===========================

====================
path: src\llm_enhancer\providers\gemini.py

"""Google Gemini provider using official SDK."""

import time
from typing import Optional
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider


class GeminiProvider(BaseLLMProvider):
    """Google Gemini LLM provider."""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.0-flash",
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
    ):
        """Initialize Gemini provider.

        Args:
            api_key: Google Gemini API key
            model: Model name (default: gemini-2.0-flash)
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
        """
        super().__init__(
            model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout
        )
        self.api_key = api_key
        self._client = None

    def _get_client(self):
        """Lazy initialization of Gemini client."""
        if self._client is None:
            try:
                import google.generativeai as genai

                genai.configure(api_key=self.api_key)
                self._client = genai.GenerativeModel(self.model)
            except ImportError:
                raise ImportError(
                    "google-generativeai SDK not installed. Run: pip install google-generativeai"
                )
        return self._client

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using Gemini.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts

        Returns:
            Generated text
        """
        client = self._get_client()

        for attempt in range(max_retries):
            try:
                response = client.generate_content(
                    prompt,
                    generation_config={
                        "temperature": self.temperature,
                        "max_output_tokens": self.max_tokens,
                    },
                )
                return response.text
            except Exception as e:
                error_msg = str(e).lower()
                # Retry on rate limits or server errors
                if attempt < max_retries - 1 and (
                    "503" in error_msg or "429" in error_msg or "rate" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                raise Exception(f"Gemini generation failed: {e}")

        raise Exception("Gemini generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if Gemini is available."""
        return bool(self.api_key)

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "gemini"

===========================

====================
path: src\llm_enhancer\providers\groq.py

"""Groq provider using official SDK."""

import time
from typing import Optional
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider


class GroqProvider(BaseLLMProvider):
    """Groq LLM provider."""

    def __init__(
        self,
        api_key: str,
        model: str = "llama-3.3-70b-versatile",
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
    ):
        """Initialize Groq provider.

        Args:
            api_key: Groq API key
            model: Model name (default: llama-3.3-70b-versatile)
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
        """
        super().__init__(
            model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout
        )
        self.api_key = api_key
        self._client = None

    def _get_client(self):
        """Lazy initialization of Groq client."""
        if self._client is None:
            try:
                from groq import Groq

                self._client = Groq(api_key=self.api_key)
            except ImportError:
                raise ImportError("groq SDK not installed. Run: pip install groq")
        return self._client

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using Groq.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts

        Returns:
            Generated text
        """
        client = self._get_client()

        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout,
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e).lower()
                # Retry on rate limits or server errors
                if attempt < max_retries - 1 and (
                    "503" in error_msg or "429" in error_msg or "rate" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                raise Exception(f"Groq generation failed: {e}")

        raise Exception("Groq generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if Groq is available."""
        return bool(self.api_key)

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "groq"

===========================

====================
path: src\llm_enhancer\providers\lmstudio.py

"""LM Studio provider using OpenAI-compatible SDK."""

import time
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider


class LMStudioProvider(BaseLLMProvider):
    """LM Studio local LLM provider (OpenAI-compatible)."""

    def __init__(
        self,
        base_url: str = "http://localhost:1234/v1",
        model: str = "local-model",
        api_key: str = "not-needed",
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
    ):
        """Initialize LM Studio provider.

        Args:
            base_url: LM Studio server URL (default: http://localhost:1234/v1)
            model: Model identifier (use server's loaded model name)
            api_key: API key (not needed for local LM Studio)
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
        """
        super().__init__(
            model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout
        )
        self.base_url = base_url
        self.api_key = api_key
        self._client = None

    def _get_client(self):
        """Lazy initialization of OpenAI client for LM Studio."""
        if self._client is None:
            try:
                from openai import OpenAI

                self._client = OpenAI(base_url=self.base_url, api_key=self.api_key)
            except ImportError:
                raise ImportError("openai SDK not installed. Run: pip install openai")
        return self._client

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using LM Studio.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts
            **kwargs: Extra arguments for the client (e.g. response_format)

        Returns:
            Generated text
        """
        client = self._get_client()

        # Extract extra params for OpenAI client (e.g. response_format)
        extra_params = {
            k: v
            for k, v in kwargs.items()
            if k
            in ["response_format", "functions", "function_call", "tools", "tool_choice"]
        }

        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout,
                    **extra_params,
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e).lower()
                # Retry on connection errors or server errors
                if attempt < max_retries - 1 and (
                    "connection" in error_msg
                    or "timeout" in error_msg
                    or "503" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                raise Exception(f"LM Studio generation failed: {e}")

        raise Exception("LM Studio generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if LM Studio is available.

        Note: We assume availability if base_url is set.
        Actual server availability is checked on first request.
        """
        return bool(self.base_url)

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "lmstudio"

===========================

====================
path: src\llm_enhancer\providers\openrouter.py

"""OpenRouter provider using OpenAI SDK (OpenAI-compatible API)."""

import time
from typing import Optional
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider


class OpenRouterProvider(BaseLLMProvider):
    """OpenRouter LLM provider using native SDK."""

    def __init__(
        self,
        api_key: str,
        model: str = "meta-llama/llama-3.1-8b-instruct:free",
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
        base_url: str = "https://openrouter.ai/api/v1",
    ):
        """Initialize OpenRouter provider.

        Args:
            api_key: OpenRouter API key (get from openrouter.ai/settings/keys)
            model: Model identifier (e.g., 'meta-llama/llama-3.1-8b-instruct:free',
                   'nvidia/nemotron-3-nano-30b-a3b:free', 'qwen/qwen3-next-80b-a3b-instruct:free')
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
            base_url: OpenRouter API base URL
        """
        super().__init__(
            model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout
        )
        self.api_key = api_key
        self.base_url = base_url
        self._client = None

    def _get_client(self):
        """Lazy initialization of OpenAI client for OpenRouter."""
        if self._client is None:
            try:
                from openai import OpenAI

                self._client = OpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                )
            except ImportError:
                raise ImportError("openai SDK not installed. Run: pip install openai")
        return self._client

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using OpenRouter.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts

        Returns:
            Generated text
        """
        client = self._get_client()

        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout,
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e).lower()
                # Retry on rate limits, connection errors, or server errors
                if attempt < max_retries - 1 and (
                    "rate" in error_msg
                    or "429" in error_msg
                    or "503" in error_msg
                    or "connection" in error_msg
                    or "timeout" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                raise Exception(f"OpenRouter generation failed: {e}")

        raise Exception("OpenRouter generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if OpenRouter is available."""
        return bool(self.api_key)

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "openrouter"

===========================

====================
path: src\llm_enhancer\providers\vllm.py

"""vLLM provider using OpenAI-compatible SDK."""

import time
from testsuitegen.src.llm_enhancer.providers.base import BaseLLMProvider
from testsuitegen.src.exceptions.exceptions import LLMFatalError


class VLLMProvider(BaseLLMProvider):
    """vLLM provider (OpenAI-compatible)."""

    def __init__(
        self,
        base_url: str,
        model: str,
        api_key: str = "not-needed",
        temperature: float = 0.01,
        max_tokens: int = 8000,
        timeout: int = 90,
    ):
        """Initialize vLLM provider.

        Args:
            base_url: vLLM server URL
            model: Model identifier
            api_key: API key (usually ignored by vLLM, but passed for compatibility)
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            timeout: Request timeout
        """
        super().__init__(
            model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout
        )
        self.base_url = base_url
        self.api_key = api_key
        self._client = None

    def _get_client(self):
        """Lazy initialization of OpenAI client for vLLM."""
        if self._client is None:
            try:
                from openai import OpenAI

                self._client = OpenAI(base_url=self.base_url, api_key=self.api_key)
            except ImportError:
                raise ImportError("openai SDK not installed. Run: pip install openai")
        return self._client

    def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> str:
        """Generate text using vLLM.

        Args:
            prompt: Input prompt
            max_retries: Maximum retry attempts
            **kwargs: Extra arguments for the client

        Returns:
            Generated text
        """
        client = self._get_client()

        # Extract extra params for OpenAI client
        extra_params = {
            k: v
            for k, v in kwargs.items()
            if k
            in [
                "response_format",
                "functions",
                "function_call",
                "tools",
                "tool_choice",
                "extra_body",
            ]
        }

        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout,
                    **extra_params,
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = str(e).lower()
                # Retry on connection errors or transient server errors
                if attempt < max_retries - 1 and (
                    "connection" in error_msg
                    or "timeout" in error_msg
                    or "503" in error_msg
                    or "502" in error_msg
                ):
                    wait_time = 2**attempt  # Exponential backoff
                    time.sleep(wait_time)
                    continue
                # Treat other errors (including 404 / bad route) as fatal
                raise LLMFatalError(f"vLLM generation failed: {e}")

        raise LLMFatalError("vLLM generation failed after all retries")

    @property
    def is_available(self) -> bool:
        """Check if vLLM is available.

        Assumes availability if base_url is configured.
        """
        return bool(self.base_url)

    @property
    def provider_name(self) -> str:
        """Get provider name."""
        return "vllm"

===========================

====================
path: src\llm_enhancer\providers\__init__.py


===========================

====================
path: src\llm_enhancer\python_enhancer\__init__.py


===========================

====================
path: src\llm_enhancer\python_enhancer\ir_enhancer\enhancer.py

# llm_enhancer/ir_enhancer/enhancer.py

import json
import time

from testsuitegen.src.llm_enhancer.client import llm_generate
from testsuitegen.src.llm_enhancer.python_enhancer.ir_enhancer.prompts import (
    ENHANCE_IR_PROMPT,
)
from testsuitegen.src.llm_enhancer.python_enhancer.ir_enhancer.validator import (
    validate_ir_enhancement_flexible,
)
from testsuitegen.src.config.settings import (
    LLM_ENABLED,
    MAX_LLM_RETRIES,
    EXPONENTIAL_BACKOFF_BASE,
)
from testsuitegen.src.llm_enhancer.circuit_breaker import circuit_breaker
from testsuitegen.src.exceptions.exceptions import LLMError


def enhance_ir_schema(
    ir_operation: dict,
    source_code: str,
    types: list,
    provider: str = None,
    model: str = None,
    max_retries: int = None,
) -> dict:
    """
    Uses LLM to inject constraints into the IR based on code logic with Resilience Layer.

    Args:
        ir_operation: The single operation dictionary from the IR
        source_code: The full python source code (or specific function text)
        provider: LLM provider to use
        model: Specific model to use (overrides provider default)
        max_retries: Number of retry attempts on transient errors
    """
    if not LLM_ENABLED:
        return ir_operation

    # Check if schema exists
    if (
        "body" not in ir_operation["inputs"]
        or "schema" not in ir_operation["inputs"]["body"]
    ):
        return ir_operation

    operation_id = ir_operation.get("id", "unknown_function")

    # Use config default if not specified
    if max_retries is None:
        max_retries = MAX_LLM_RETRIES

    # 1. Check Circuit Breaker before attempting
    try:
        circuit_breaker.check_state()
    except LLMError as e:
        print(
            f"      ⚠ Circuit Breaker blocking LLM call for {operation_id}. Returning original IR."
        )
        return ir_operation

    schema = ir_operation["inputs"]["body"]["schema"]
    schema_json = json.dumps(schema, indent=2)
    types_json = json.dumps(types, indent=2) if types else "[]"

    # Prepare prompt
    prompt = (
        ENHANCE_IR_PROMPT.replace("{schema}", schema_json)
        .replace("{function_name}", operation_id)
        .replace("{code}", source_code)
        .replace("{types}", types_json)
    )

    # 2. Exponential Backoff Retry Loop
    for attempt in range(1, max_retries + 1):
        try:
            print(f"      LLM Attempt {attempt}/{max_retries} for {operation_id}...")

            kwargs = {"chat_template_kwargs": {"enable_thinking": False}}
            kwargs["extra_body"] = {"chat_template_kwargs": {"enable_thinking": False}}

            # Progressive Backoff Temperature: Increase temp by 0.1 for each retry to break loops
            # Attempt 1: Default (0.01)
            # Attempt 2: 0.11
            # Attempt 3: 0.21 ...
            base_temp = 0.01
            if attempt > 1:
                kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)

            kwargs["generate_cfg"] = (
                {
                    # Add: When the response content is `<think>this is the thought</think>this is the answer;
                    # Do not add: When the response has been separated by reasoning_content and content.
                    "thought_in_content": False,
                },
            )

            enhanced_text = llm_generate(
                prompt, provider=provider, model_override=model, **kwargs
            )

            # 3. STRICT JSON VALIDATION - Clean and validate
            enhanced_text = enhanced_text.strip()

            # Remove markdown code blocks
            if enhanced_text.startswith("```json"):
                enhanced_text = enhanced_text[7:]
            elif enhanced_text.startswith("```"):
                enhanced_text = enhanced_text[3:]

            if enhanced_text.endswith("```"):
                enhanced_text = enhanced_text[:-3]

            enhanced_text = enhanced_text.strip()

            # Check for valid JSON start
            if not enhanced_text.startswith("{"):
                # Try to find JSON object
                start = enhanced_text.find("{")
                end = enhanced_text.rfind("}")
                if start != -1 and end != -1:
                    enhanced_text = enhanced_text[start : end + 1]
                else:
                    print(
                        f"      ⚠ LLM returned non-JSON response. Retrying... Attempt {attempt}"
                    )
                    raise ValueError("Response did not contain valid JSON object")

            # Parse JSON
            try:
                enhanced_schema = json.loads(enhanced_text)
            except json.JSONDecodeError as e:
                try:
                    import re

                    repaired_text = re.sub(
                        r'\\(?![/u"\\bfnrt])', r"\\\\", enhanced_text
                    )
                    enhanced_schema = json.loads(repaired_text)
                    print(
                        f"      ✨ JSON repaired successfully (fixed invalid escapes)"
                    )
                except Exception as repair_error:
                    print(
                        f"      ⚠ LLM returned invalid JSON: {e}. Repair failed: {repair_error}. Retrying... Attempt {attempt}"
                    )
                    raise ValueError(f"Invalid JSON returned: {e}")

            # Validate structure using FLEXIBLE validator
            if not validate_ir_enhancement_flexible(schema, enhanced_schema):
                print(
                    f"      ⚠ Structure validation failed. Retrying... Attempt {attempt}"
                )
                raise ValueError("LLM changed schema structure")

            # 4. SUCCESS: Apply enhancements
            # Extract metadata if provided by LLM
            if "metadata" in enhanced_schema:
                ir_operation["metadata"] = enhanced_schema.pop("metadata")

            # Update Schema
            ir_operation["inputs"]["body"]["schema"] = enhanced_schema

            circuit_breaker.record_success()
            print(f"      ✨ Schema enhanced successfully with type resolution")
            return ir_operation

        except ValueError as ve:
            # Validation errors
            print(f"      ⚠ Validation Error: {ve}")
            if attempt == max_retries:
                print(
                    f"      ❌ Max retries ({max_retries}) reached for validation errors."
                )
                circuit_breaker.record_failure()
                return ir_operation
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)  # Exponential backoff

        except Exception as e:
            # Transient errors
            print(f"      ⚠ LLM API Error (Attempt {attempt}): {e}")
            if attempt == max_retries:
                print(f"      ❌ Max retries ({max_retries}) reached for API errors.")
                circuit_breaker.record_failure()
                return ir_operation
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)  # Exponential backoff

    return ir_operation

===========================

====================
path: src\llm_enhancer\python_enhancer\ir_enhancer\prompts.py

ENHANCE_IR_PROMPT = """
/no_think
You are an expert Semantic Analyzer for Python code and JSON Schema generation.
Your task is to enrich and correct a "Basic JSON Schema" based on the provided "Python Source Code".

INPUTS:
1. Python Code (Function Source)
2. Basic JSON Schema (Generated by a fast AST Parser)
3. Type Definitions (Extracted from parser)

Function Name: {function_name}

The Basic Schema contains placeholders for complex types it could not resolve (e.g., `{"type": "object", "description": "Complex type: User"}`) and lacks semantic constraints.

=====================================
TASK:
=====================================

1. **Resolve Complex Types & Recursion:**
   - Locate classes (Pydantic Models, Dataclasses, TypedDict) defined in the source code AND the provided "Type Definitions".
   - Replace placeholders like `{"description": "Complex type: ClassName"}` with the full schema of that class.
   - **IMPORTANT:** Always add `"x-class-type": "ClassName"` to preserve the original type name.
   - **Crucial:** If a field is a container of complex types (e.g., `List[User]`, `Dict[str, Config]`), you must recursively expand the schema inside the `items` (for List) or `additionalProperties` (for Dict) fields.

2. **Resolve Enums:**
   - Locate `Enum` definitions in the code.
   - Replace placeholders with `{"type": "string", "enum": ["VAL1", "VAL2"], "x-enum-type": "EnumName", "description": "Enum: EnumName"}`.
   - **CRITICAL:** Always include `"x-enum-type": "EnumName"` to preserve the original Enum type name for display purposes.

3. **Handle Return Types:**
   - Check the function's return annotation. If the schema has a "return" or "response" key that is a complex type or placeholder, resolve it using the same logic as input parameters.

4. **Extract Constraints:**
   - **Pydantic/Dataclasses:** Look for `Field(min_length=5, max_length=10)`, `gt=0`, `lt=100`, `regex="..."`. Map these to `minLength`, `maxLength`, `minimum`, `maximum`, and `pattern` respectively.
   - **Typing:** Look for `typing.Annotated` constraints.
   - **Logic/Docstrings:** If explicit type constraints are missing, infer them from validation logic.
     - `if count % 5 != 0:` -> Add `"multipleOf": 5` to the schema.
     - `if x < 10:` -> Add `"maximum": 9` (int) or `"exclusiveMaximum": 10`.
     - `if not x.startswith(...):` -> Add `"pattern": "^..."`.
   - **Arithmetic:** specificially look for `%` modulo checks and map to `multipleOf`.
   - **Internal Whitelists:** Check for lists defined inside the function used for validation (e.g., `allowed_actions = ["list", "show"]`). Add these as `"enum": [...]` to the schema.
   - **Validation Checks:**
     - `if not table.isalnum():` -> Add `"pattern": "^[a-zA-Z0-9]+$"` and description "Alphanumeric only".
     - `dangerous_patterns = [...]` -> Description should note "Disallowed patterns: ...".
   - **Dictionary Keys:**
     - If a Dict argument has its keys checked (e.g., `if "name" not in profile` or `required = ["name", "email"]` loop), convert the `additionalProperties` schema to a fixed `properties` object with those keys marked as `required`.
   - **Graceful Handling:**
     - If the code checks a condition but returns a default value instead of raising an Error (e.g., `if not x: return "default"`), **DO NOT** add a `minLength` or `required` constraint that would force a test failure. Instead, add a description: "Empty input handled gracefully".

5. **Handle Unions and Optionals:**
   - If a type is `Optional[X]` or `X | None`, set the type as the resolved type of X and add `"nullable": true` (or use `anyOf` if strictly following JSON Schema Draft 7, but `nullable` is preferred for simplicity here).
   - If a type is `Union[A, B]`, generate an `anyOf` array containing resolved schemas for A and B.

6. **Extract Decorators:**
   - Inspect the target function definition {function_name}.
   - If decorators exist (e.g., `@auth_required`, `@router.get("/api")`), extract them.
   - **Add to Root:** Add a `"metadata": {"decorators": ["@name1(args)", ...]}` field at the ROOT level of the JSON output. Include arguments if present.

7. **Fallback for Missing Definitions:**
   - If a type is referenced in the signature but its definition is NOT found in the provided code snippet, **DO NOT** invent fields. Keep the original description or mark it as `{"type": "object", "description": "External type: TypeName"}`.

=====================================
RULES:
=====================================
- **DO NOT** remove existing properties found by the AST parser.
- **DO NOT** change the type of primitive fields (int, float, str, bool) unless the AST marked them as "Any".
- **PRESERVE** the structure of the AST schema (properties, required list) while enriching leaf nodes.
- **ENSURE** valid JSON Schema syntax for all resolved types.

=====================================
EXAMPLES:
=====================================

Input Schema:
```json
{
  "type": "object",
  "properties": {
    "status": {"type": "object", "description": "Complex type: OrderStatus"},
    "items": {"type": "array", "items": {"type": "object", "description": "Complex type: Product"}},
    "user": {"type": "object", "description": "Complex type: UserContext"}
  },
  "required": ["status"]
}
```

Code contains:
```python
class OrderStatus(Enum):
    PENDING = "pending"
    SHIPPED = "shipped"

@dataclass
class Product:
    id: int
    name: Annotated[str, Field(min_length=3)]

@dataclass
class UserContext:
    user_id: str

@auth_required(role="admin")
def process_order(status: OrderStatus, items: List[Product], user: UserContext) -> bool:
    ...
```

Expected Output:
```json
{
  "type": "object",
  "properties": {
    "status": {
      "type": "string",
      "enum": ["pending", "shipped"],
      "x-enum-type": "OrderStatus",
      "description": "Enum: OrderStatus"
    },
    "items": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": {"type": "integer"},
          "name": {"type": "string", "minLength": 3}
        },
        "required": ["id", "name"],
        "x-class-type": "Product",
        "description": "Class: Product"
      }
    },
    "user": {
      "type": "object",
      "properties": {
        "user_id": {"type": "string"}
      },
      "required": ["user_id"],
      "x-class-type": "UserContext",
      "description": "Class: UserContext"
    }
  },
  "required": ["status"],
  "metadata": {
    "decorators": ["@auth_required(role='admin')"]
  }
}
```

=====================================
EXECUTION
=====================================
# Python Code:
{code}

# Type Definitions:
{types}

# Current Schema (AST):
{schema}

Return ONLY the updated JSON Schema as valid JSON.
NO explanations. NO markdown code blocks (```json). NO additional text.
Just the raw JSON object.
"""

===========================

====================
path: src\llm_enhancer\python_enhancer\ir_enhancer\validator.py

# llm_enhancer/ir_enhancer/validator.py


def validate_ir_enhancement_flexible(original: dict, enhanced: dict) -> bool:
    """
    Validates IR enhancement with support for Type Resolution.

    Allowed:
    - Adding constraints (min/max, pattern, etc.)
    - Resolving "Complex type" objects into detailed schemas with properties
    - Changing generic object to specific enum
    - Expanding placeholder objects

    Forbidden:
    - Removing properties defined in AST
    - Changing primitive types (e.g., int -> bool) unless original was Any/Object

    Args:
        original: Original schema from AST
        enhanced: Enhanced schema from LLM

    Returns:
        True if enhancement is valid
    """
    if "properties" in original:
        if "properties" not in enhanced:
            return False  # Should not lose properties container

        for prop_name, orig_prop in original["properties"].items():
            if prop_name not in enhanced["properties"]:
                print(f"      [Validation] Missing property: {prop_name}")
                return False

            enh_prop = enhanced["properties"][prop_name]

            # Allow Object -> Detailed Object resolution
            if orig_prop.get("type") == "object" and "description" in orig_prop:
                if "Complex type" in orig_prop["description"]:
                    continue  # Allow total replacement of complex placeholders

            # Allow Object -> Enum (for enum types marked as objects)
            if (
                orig_prop.get("type") == "object"
                and enh_prop.get("type") == "string"
                and "enum" in enh_prop
            ):
                continue  # Allow enum resolution

            # Recursive check for nested schemas
            if not validate_ir_enhancement_flexible(orig_prop, enh_prop):
                return False

    return True

===========================

====================
path: src\llm_enhancer\python_enhancer\ir_enhancer\__init__.py

"""IR Schema Enhancer - Enhances Python function schemas with LLM."""

from .enhancer import enhance_ir_schema
from .prompts import ENHANCE_IR_PROMPT
from .validator import validate_ir_enhancement_flexible

__all__ = ["enhance_ir_schema", "ENHANCE_IR_PROMPT", "validate_ir_enhancement_flexible"]

===========================

====================
path: src\llm_enhancer\python_enhancer\test_suite_enhancer\enhancer.py

# llm_enhancer/test_enhancer/enhancer.py

import sys
import time

sys.path.append("..")
from testsuitegen.src.llm_enhancer.client import llm_generate
from testsuitegen.src.llm_enhancer.python_enhancer.test_suite_enhancer.prompts import (
    ENHANCE_CODE_UNIT_PROMPT,
    ENHANCE_CODE_API_PROMPT,
)
from testsuitegen.src.llm_enhancer.python_enhancer.test_suite_enhancer.validator import (
    validate_no_logic_change,
)
from testsuitegen.src.llm_enhancer.circuit_breaker import circuit_breaker
from testsuitegen.src.config.settings import (
    LLM_ENABLED,
    MAX_LLM_RETRIES,
    EXPONENTIAL_BACKOFF_BASE,
)
from testsuitegen.src.exceptions.exceptions import LLMError, LLMFatalError


def _clean_llm_response(response: str) -> str:
    """Clean LLM response from markdown formatting and common issues.

    Args:
        response: Raw LLM response

    Returns:
        Cleaned Python code
    """
    # Remove markdown code blocks
    if response.startswith("```python"):
        response = response[9:]
    elif response.startswith("```typescript"):
        response = response[13:]
    elif response.startswith("```ts"):
        response = response[5:]
    elif response.startswith("```javascript"):
        response = response[13:]
    elif response.startswith("```js"):
        response = response[5:]
    elif response.startswith("```"):
        response = response[3:]

    if response.endswith("```"):
        response = response[:-3]  # Remove closing ```

    # Strip whitespace
    response = response.strip()

    # Handle common LLM formatting issues
    lines = response.split("\n")
    cleaned_lines = []

    for line in lines:
        # Skip empty lines at start/end
        if not line.strip() and (not cleaned_lines or line == lines[-1]):
            continue
        cleaned_lines.append(line)

    return "\n".join(cleaned_lines)


def _is_beneficial_only_change(original: str, enhanced: str) -> bool:
    """Check if the enhancement only adds beneficial changes (fixtures, comments, etc.)."""
    # Simple heuristic: if enhanced is longer and contains fixture-related keywords,
    # and doesn't contain forbidden patterns, consider it beneficial

    beneficial_keywords = [
        "@pytest.fixture",
        "test_data_setup",
        "USE_CREATED_RESOURCE",
        "def setup_",
        "patch(",
        "MagicMock",
    ]
    forbidden_patterns = [
        r"expected_status.*=",
        r"assert.*!=",
        r"@pytest.mark.parametrize.*\[",
    ]

    # Check for beneficial additions
    has_beneficial = any(keyword in enhanced for keyword in beneficial_keywords)

    # Check for forbidden changes
    has_forbidden = any(pattern in enhanced for pattern in forbidden_patterns)

    # If it has beneficial changes and no forbidden ones, and is reasonably longer (added content)
    length_increase = len(enhanced) - len(original)
    reasonable_addition = (
        length_increase > 50 and length_increase < 2000
    )  # Reasonable bounds

    return has_beneficial and not has_forbidden and reasonable_addition


def enhance_code(
    code: str,
    provider: str = None,
    model: str = None,
    max_retries: int = None,
    test_type: str = "api",
) -> str:
    """Enhance test code with better formatting and docstrings using Resilience Layer.

    Args:
        code: Original test code
        provider: LLM provider to use (None = use default)
        max_retries: Number of retry attempts on transient errors
        test_type: "api" or "unit" to select enhancement strategy

    Returns:
        Enhanced code (or original if enhancement fails)

    RESILIENCE:
    - Circuit Breaker: Stops LLM calls after repeated failures
    - Exponential Backoff: Retries with increasing delays (2s, 4s, 8s)
    - Strict Validation: Rejects code with logic changes
    """
    if not LLM_ENABLED:
        return code

    # Use config default if not specified
    if max_retries is None:
        max_retries = MAX_LLM_RETRIES

    # 1. Check Circuit Breaker before attempting
    try:
        circuit_breaker.check_state()
    except LLMError as e:
        print(
            f"⚠ Circuit Breaker blocking LLM call for test code enhancement. Returning original code."
        )
        return code

    print(
        f"▶ Enhancing code with LLM ({test_type} mode) using provider: {provider or 'default'}:{model or 'default'}..."
    )

    # Prepare prompt based on test type
    if test_type == "unit":
        prompt_template = ENHANCE_CODE_UNIT_PROMPT
    else:
        prompt_template = ENHANCE_CODE_API_PROMPT

    prompt = prompt_template.replace("{code}", code)

    # 2. Exponential Backoff Retry Loop
    for attempt in range(1, max_retries + 1):
        try:
            print(
                f"   LLM Attempt {attempt}/{max_retries} for test code enhancement..."
            )

            # Progressive Backoff Temperature: Increase temp by 0.1 for each retry to break loops
            base_temp = 0.01
            kwargs = {}
            if attempt > 1:
                kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)

            # Call LLM
            raw_enhanced = llm_generate(
                prompt, provider=provider, model_override=model, **kwargs
            )

            # 3. STRICT VALIDATION - Clean and validate
            enhanced = _clean_llm_response(raw_enhanced)

            # Check if response looks like Python code
            if not enhanced or len(enhanced.strip()) < 10:
                print(
                    f"   ⚠ LLM returned empty or too short response. Retrying... Attempt {attempt}"
                )
                raise ValueError("Empty or invalid response from LLM")

            # Check for common hallucination patterns
            if enhanced.strip().startswith(
                ("Here", "Sure", "I'll", "Let me", "The code")
            ):
                print(
                    f"   ⚠ LLM returned text instead of code. Retrying... Attempt {attempt}"
                )
                raise ValueError("LLM returned explanation text instead of code")

            # Validate no logic changes
            try:
                validate_no_logic_change(original=code, enhanced=enhanced)
            except RuntimeError as validation_error:
                # Check if it's beneficial changes only
                if _is_beneficial_only_change(code, enhanced):
                    print(
                        f"   ⚠ Validation warning (but allowing beneficial changes): {validation_error}"
                    )
                else:
                    print(
                        f"   ⚠ Logic validation failed. Retrying... Attempt {attempt}"
                    )
                    raise ValueError(f"Logic change detected: {validation_error}")

            # 4. SUCCESS: Record Success and Return
            circuit_breaker.record_success()
            print(f"   ✨ Test code enhancement successful")
            return enhanced

        except ValueError as ve:
            # Validation errors
            print(f"   ⚠ Validation Error: {ve}")
            if attempt == max_retries:
                print(
                    f"   ❌ Max retries ({max_retries}) reached for validation errors."
                )
                circuit_breaker.record_failure()
                return code
            time.sleep(2**attempt)  # Exponential backoff

        except LLMFatalError as lf:
            # Non-retryable policy/config error from provider - abort immediately
            print(f"   ⚠ Non-retryable LLM error: {lf}. Aborting code enhancement.")
            circuit_breaker.record_failure()
            return code

        except Exception as e:
            # Transient errors
            print(f"   ⚠ LLM API Error (Attempt {attempt}): {e}")
            if attempt == max_retries:
                print(f"   ❌ Max retries ({max_retries}) reached for API errors.")
                circuit_breaker.record_failure()
                return code
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)  # Exponential backoff

    # Should not reach here, but just in case
    return code

===========================

====================
path: src\llm_enhancer\python_enhancer\test_suite_enhancer\prompts.py

ENHANCE_CODE_API_PROMPT = """
You are a test code enhancer specializing in API contract testing with pytest.

YOUR TASK: Improve readability, implement accurate test data setup using data from test parameters, and fix common issues WITHOUT changing test logic.

IMPORTANT: Return ONLY clean Python code without markdown formatting, code blocks, or explanations.

CRITICAL - NEVER MODIFY:
- DO NOT change test logic or assertions
- DO NOT add or remove test cases from parametrize
- DO NOT rename test functions or classes
- DO NOT modify payload structure or expected status codes
- DO NOT change API endpoint URLs or HTTP methods
- DO NOT change the @pytest.mark.parametrize decorator or its parameters
- DO NOT change test IDs or expected_status values
- DO NOT modify the BASE_URL, ENDPOINT, or METHOD constants

REQUIRED FIXES & IMPROVEMENTS:


1. **Test Data Setup with Source Awareness (CRITICAL - MANDATORY):**
     - **MANDATORY RULE:** If ANY parametrized test case contains `USE_CREATED_RESOURCE` (or variants like `USE_CREATED_RESOURCE_*`) in ANY path parameter value, you MUST implement resource creation in the `test_data_setup` fixture. This is REQUIRED - do not skip this.
     - **CRITICAL:** Scan the entire @pytest.mark.parametrize decorator for any path_params dict that contains "USE_CREATED_RESOURCE" as a value. If found, you MUST create the resource.
     - Example of what to look for in parametrize:
       ```
       @pytest.mark.parametrize("intent, payload, path_params, expected_status", [
           pytest.param("HAPPY_PATH", {...}, {"user_id": "USE_CREATED_RESOURCE"}, 200, id="HAPPY_PATH"),
       ])
       ```
       In this case, you MUST create a user resource in test_data_setup and return {"placeholders": {"USE_CREATED_RESOURCE": created_user_id}}
     - Read the entire source code and scan ALL `@pytest.mark.parametrize` decorators for `USE_CREATED_RESOURCE` in path_params.
     - For each unique placeholder (e.g., `USE_CREATED_RESOURCE`, `USE_CREATED_RESOURCE_ACTIVE`), create ONE resource using a HAPPY_PATH payload from the parametrize data.
     - Derive the create endpoint by removing path params from ENDPOINT (e.g., if ENDPOINT is "/users/{user_id}", create at "/users").
     - POST the payload, check for 200/201, extract the ID from response.json()["id"].
     - The fixture MUST yield a dict like: `{"created_resources": [resource_dict], "placeholders": {"USE_CREATED_RESOURCE": created_id}}`
     - If creation fails, yield `{"created_resources": [], "placeholders": {}}` and skip tests that need it.
     - Add teardown to delete created resources.
     - **INJECTION MANDATORY:** In EVERY test function body, immediately after `created_resources = test_data_setup["created_resources"]`, add code to replace placeholders in path_params with the mapped IDs from `test_data_setup["placeholders"]`. Use the example code provided - do not modify parametrize.
     - If no USE_CREATED_RESOURCE is found, the fixture can be minimal, but still check.
*** End Patch

    Example Pattern (Single Resource):
   ```python
   @pytest.fixture(scope="class")
   def test_data_setup(api_client):
       # Extract payload from the logic or define a valid base payload matching the schema
       create_payload = {
           "name": "Test Resource",
           "status": "active" # Matches the HAPPY_PATH requirement
       }
       response = api_client.post(f"{BASE_URL}/endpoint", json=create_payload)
       resource_data = response.json() if response.status_code in [200, 201] else {}
       
       yield resource_data
       
       # Cleanup
       if "id" in resource_data:
           api_client.delete(f"{BASE_URL}/endpoint/{resource_data['id']}")
   ```

   Example Pattern (Multiple Resources / Variants):
   ```python
   @pytest.fixture(scope="class")
   def test_data_setup(api_client):
       created_ids = {}
       
       # Create Active Variant
       res1 = api_client.post(f"{BASE_URL}/users", json={"name": "Active User", "status": "active"})
       if res1.status_code == 201: created_ids["active_user_id"] = res1.json()["id"]
       
       # Create Inactive Variant
       res2 = api_client.post(f"{BASE_URL}/users", json={"name": "Inactive User", "status": "inactive"})
       if res2.status_code == 201: created_ids["inactive_user_id"] = res2.json()["id"]

       yield created_ids
       
       # Cleanup all
       for uid in created_ids.values():
           api_client.delete(f"{BASE_URL}/users/{uid}")
   ```


2. **Parameter Replacement Logic:**
   - Always ensure `path_params` is a dict in the generated code. If it is None or missing, set it to an empty dict.
   - For endpoints with path parameters (e.g., user_id, transfer_id), always create the required resource(s) in `test_data_setup` and use their IDs for the path parameters in the test cases.
   - Map placeholders to the IDs provided by `test_data_setup`.
   - If `test_data_setup` returns a dictionary of IDs (due to variants), use conditional logic to select the right one based on the test case ID or parameters.

    Example (Single ID):
   ```python
   if not path_params or not isinstance(path_params, dict):
       path_params = {}
   if "user_id" in path_params and path_params["user_id"] == "USE_CREATED_RESOURCE" and "id" in test_data_setup:
       path_params["user_id"] = test_data_setup["id"]
   ```

   Example (Multiple IDs/Variants):
   ```python
   if not path_params or not isinstance(path_params, dict):
       path_params = {}
   # Logic to pick the correct ID based on the test scenario
   if "user_id" in path_params:
       pid = path_params["user_id"]
       if pid == "USE_CREATED_RESOURCE_ACTIVE":
           path_params["user_id"] = test_data_setup.get("active_user_id")
       elif pid == "USE_CREATED_RESOURCE_INACTIVE":
           path_params["user_id"] = test_data_setup.get("inactive_user_id")
   ```

3. **Code Quality:**
   - Improve comments and docstrings
   - Fix formatting and indentation
   - Add descriptive variable names
   - Keep all existing functionality intact
   
4. **Context Awareness:**
   - Identify resource type from endpoint (users, shipments, products, etc.)
   - Infer required fields from operation name and schema
   - Ensure the fixture payload strictly adheres to the schema constraints defined in the test cases

5. **PLACEHOLDER REPLACEMENT (CRITICAL):**
   - Look for the comment "# PLACEHOLDER_FOR_TEST_DATA_SETUP_CODE"
   - REPLACE the entire section from "# === LLM ENHANCEMENT REQUIRED ===" to "# PLACEHOLDER_FOR_TEST_DATA_SETUP_CODE" with actual test data creation code
   - DO NOT leave the placeholder comment in the final code
   - If no test data setup is needed, replace with just a comment explaining why

EXAMPLES OF ALLOWED CHANGES:

GOOD - Add test data setup fixture matching parametrize data:
```python
@pytest.fixture(scope="class")
def test_data_setup(api_client):
    # Payload matches the specific requirements found in parametrize HAPPY_PATH
    payload = {
        "title": "Test Task for Setup",
        "description": "Created by test fixture",
        "priority": "high" # Specific requirement from test case
    }
    response = api_client.post(f"{BASE_URL}/tasks", json=payload)
    task_data = response.json() if response.status_code == 201 else {}
    
    yield task_data
    
    if "id" in task_data:
        api_client.delete(f"{BASE_URL}/tasks/{task_data['id']}")
```

GOOD - Add parameter replacement logic for variants:
```python
# Handle different resource requirements for different test cases
if path_params.get("task_id") == "USE_CREATED_RESOURCE_HIGH_PRIORITY":
    path_params["task_id"] = test_data_setup.get("high_priority_id")
elif path_params.get("task_id") == "USE_CREATED_RESOURCE_LOW_PRIORITY":
    path_params["task_id"] = test_data_setup.get("low_priority_id")
```

FORBIDDEN:
- Changing test case parameters (user_id values, expected statuses, payloads in parametrize)
- Removing or adding test cases
- Modifying assertions or expected behaviors
- Changing BASE_URL, ENDPOINT, or METHOD constants
- Modifying @pytest.mark.parametrize decorators
- Changing test function names or class names
- Using generic hardcoded data in the fixture when specific data exists in parametrize

OUTPUT FORMAT:
Return ONLY valid Python code.
NO explanations before or after.
NO markdown code blocks.
Just the raw Python code.

Code to enhance:
----------------
{code}
----------------
"""

ENHANCE_CODE_UNIT_PROMPT = """
You are a test code enhancer specializing in Python unit testing with pytest.

YOUR TASK: Improve readability, implement accurate test data setup using data from test parameters, and fix common issues WITHOUT changing test logic.

IMPORTANT: Return ONLY clean Python code without markdown formatting, code blocks, or explanations.

CRITICAL - NEVER MODIFY:
- DO NOT change test logic or assertions
- DO NOT add or remove test cases from parametrize
- DO NOT rename test functions or classes
- DO NOT change specified function calls or method signatures
- DO NOT change test IDs or expected_status/return values
- DO NOT modify payload structure or expected results

REQUIRED FIXES & IMPROVEMENTS:

1. **Test Data Setup & Mocking (CRITICAL):**
   - **Object Initialization:** If the function under test defines complex arguments (Classes, Pydantic Models, Dataclasses), the setup must initialize them correctly.
     - Convert dictionaries from `parametrize` payload into actual Object instances if the function signature requires it.
   - **Mocking Dependencies:** If the function uses external dependencies (e.g., database, external APIs) and `mock` or `patch` is used, ensure the mocks are configured to return data consistent with the `parametrize` payload/intent.
   
   Example (Object Initialization):
   ```python
   # In test function body, before calling the function under test
   if "user_context" in args and isinstance(args["user_context"], dict):
       # Dynamic checks to avoid import errors (ensure imports are added if needed)
       if "UserContext" in globals():
           args["user_context"] = UserContext(**args["user_context"])
   ```

   - **Argument Instantiation (Enums/Classes):** 
     - If the function signature expects an Enum or Class instance, but the `parametrize` arguments provide a dictionary or string (from JSON), YOU MUST Instantiate the object before calling the function.
     - **CRITICAL for ENUMS:** `kwargs["status"]` = `Status(kwargs["status"])` if `status` is a string like "active".
   - Ensure the function is called directly with the correct arguments: `result = function_name(**args)`.
   - Preserve all existing assertions.

3. **Code Quality:**
   - Improve comments and docstrings.
   - Fix formatting and indentation.
   - Add descriptive variable names.
   - Keep all existing functionality intact.

EXAMPLES OF ALLOWED CHANGES:

GOOD - Python Unit Test Mocking:
```python
@pytest.fixture
def mock_db():
    with patch("src.database.get_connection") as mock:
        yield mock

def test_process_order(mock_db, intent, args, expected_result):
    # Setup mock return value based on intent
    if intent == "DB_ERROR":
        mock_db.side_effect = Exception("DB Connection Failed")
    
    # Convert dict to Enum if function expects Enum
    if "status" in args and isinstance(args["status"], str) and "OrderStatus" in globals():
        args["status"] = OrderStatus(args["status"])
        
    result = process_order(**args)
    assert result == expected_result
```

BAD - DO NOT change expected values:
```python
# DON'T DO THIS:
expected_result = True  # Was False, now changed
```

FORBIDDEN:
- Changing test case parameters (values, expected results)
- Removing or adding test cases
- Modifying assertions
- Changing function signatures
- Adding API client logic or HTTP calls (this is pure unit testing)

OUTPUT FORMAT:
Return ONLY valid Python code.
NO explanations before or after.
NO markdown code blocks.
Just the raw Python code.

Code to enhance:
----------------
{code}
----------------
"""

===========================

====================
path: src\llm_enhancer\python_enhancer\test_suite_enhancer\validator.py

# llm_enhancer/test_enhancer/validator.py

import ast


def validate_no_logic_change(original: str, enhanced: str):
    """Validate that LLM only made safe enhancements, not harmful logic changes."""

    # Check for obviously harmful changes by comparing key patterns
    harmful_patterns = [
        r"expected_status.*[^=]=[^=]",  # API tests: expected statuses
        r"expected_result.*[^=]=[^=]",  # Unit tests: expected return values
        r"expected_value.*[^=]=[^=]",  # Unit tests: expected values
        r"assert.*!=",  # Changing assertions to not-equal
        r"assert.*status_code.*[^2]..",  # Changing status code assertions
        r"@pytest.mark.parametrize.*\[.*\]",  # Modifying parametrize decorators
        r'id=.*[^"]',  # Changing test IDs
    ]

    for pattern in harmful_patterns:
        orig_count = len([line for line in original.split("\n") if pattern in line])
        enhanced_count = len([line for line in enhanced.split("\n") if pattern in line])

        if orig_count != enhanced_count:
            raise RuntimeError(
                f"LLM modified test logic (pattern: {pattern}) — enhancement rejected"
            )

    # Validate test_data_setup fixture structure for API tests
    if "def test_data_setup" in original:
        # Check that the fixture still yields a dict with "created_resources" key
        if 'yield {"created_resources": created_resources}' not in enhanced:
            raise RuntimeError(
                "LLM modified test_data_setup fixture structure — enhancement rejected. "
                "Fixture must yield {'created_resources': created_resources}"
            )

    # Allow enhancements that add fixtures, comments, or formatting
    return True

===========================

====================
path: src\llm_enhancer\python_enhancer\test_suite_enhancer\__init__.py


===========================

====================
path: src\llm_enhancer\typescript_enhancer\__init__.py


===========================

====================
path: src\llm_enhancer\typescript_enhancer\ir_enhancer\enhancer.py

# llm_enhancer/typescript_enhancer/ir_enhancer/enhancer.py

import json
import time
import logging

from testsuitegen.src.llm_enhancer.client import llm_generate
from testsuitegen.src.llm_enhancer.typescript_enhancer.ir_enhancer.prompts import (
    ENHANCE_IR_PROMPT_TS,
)
from testsuitegen.src.llm_enhancer.typescript_enhancer.ir_enhancer.validator import (
    validate_ir_enhancement_flexible,
)
from testsuitegen.src.config.settings import (
    LLM_ENABLED,
    MAX_LLM_RETRIES,
    EXPONENTIAL_BACKOFF_BASE,
)
from testsuitegen.src.llm_enhancer.circuit_breaker import circuit_breaker
from testsuitegen.src.exceptions.exceptions import LLMError

logger = logging.getLogger(__name__)


def enhance_ir_schema_ts(
    ir_operation: dict,
    source_code: str,
    types: list,
    provider: str = None,
    model: str = None,
    max_retries: int = None,
    **kwargs,
) -> dict:
    """
    Uses LLM to inject constraints into the IR based on TS code logic with Resilience Layer.
    """
    if not LLM_ENABLED:
        return ir_operation

    # Check if schema exists
    if (
        "body" not in ir_operation["inputs"]
        or "schema" not in ir_operation["inputs"]["body"]
    ):
        return ir_operation

    operation_id = ir_operation.get("id", "unknown_function")

    # Use config default if not specified
    if max_retries is None:
        max_retries = MAX_LLM_RETRIES

    # 1. Check Circuit Breaker before attempting
    try:
        circuit_breaker.check_state()
    except LLMError as e:
        logger.warning(
            f"      Circuit Breaker blocking LLM call for {operation_id}. Returning original IR."
        )
        return ir_operation

    schema = ir_operation["inputs"]["body"]["schema"]
    schema_json = json.dumps(schema, indent=2)
    types_json = json.dumps(types, indent=2) if types else "[]"

    # Prepare prompt with TS Template
    prompt = (
        ENHANCE_IR_PROMPT_TS.replace("{schema}", schema_json)
        .replace("{function_name}", operation_id)
        .replace("{code}", source_code)
        .replace("{types}", types_json)
    )

    # 2. Exponential Backoff Retry Loop
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(
                f"      LLM Attempt {attempt}/{max_retries} for {operation_id} (TS)..."
            )

            # kwargs = {"chat_template_kwargs": {"enable_thinking": False}}
            # kwargs["extra_body"] = {"chat_template_kwargs": {"enable_thinking": False}}
            # kwargs["generate_cfg"] = ({"thought_in_content": False},)

            # Progressive Backoff Temperature: Increase temp by 0.1 for each retry to break loops
            base_temp = 0.01
            if attempt > 1:
                kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)

            enhanced_text = llm_generate(
                prompt, provider=provider, model_override=model, **kwargs
            )

            # 3. STRICT JSON VALIDATION - Clean and validate
            enhanced_text = enhanced_text.strip()
            if enhanced_text.startswith("```json"):
                enhanced_text = enhanced_text[7:]
            elif enhanced_text.startswith("```"):
                enhanced_text = enhanced_text[3:]
            if enhanced_text.endswith("```"):
                enhanced_text = enhanced_text[:-3]
            enhanced_text = enhanced_text.strip()

            if not enhanced_text.startswith("{"):
                start = enhanced_text.find("{")
                end = enhanced_text.rfind("}")
                if start != -1 and end != -1:
                    enhanced_text = enhanced_text[start : end + 1]
                else:
                    logger.warning(
                        f"      LLM returned non-JSON response. Retrying... Attempt {attempt}"
                    )
                    raise ValueError("Response did not contain valid JSON object")

            try:
                enhanced_schema = json.loads(enhanced_text)
            except json.JSONDecodeError as e:
                # Attempt simple repair
                try:
                    import re

                    repaired_text = re.sub(
                        r'\\(?![/u"\\bfnrt])', r"\\\\", enhanced_text
                    )
                    enhanced_schema = json.loads(repaired_text)
                except Exception:
                    logger.warning(f"      Invalid JSON. Retrying... Attempt {attempt}")
                    if attempt <= max_retries:
                        base_temp = 0.01
                        kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)
                        prompt += f"\n\nInvalid JSON returned: {e}"
                        continue
                    raise ValueError(f"Invalid JSON returned: {e}")

            # Validate structure (Reuse python validator as structure rules are same)
            if not validate_ir_enhancement_flexible(schema, enhanced_schema):
                logger.warning(
                    f"      Structure validation failed. Retrying... Attempt {attempt}"
                )
                raise ValueError("LLM changed schema structure")

            # 4. SUCCESS: Apply enhancements
            if "metadata" in enhanced_schema:
                ir_operation["metadata"] = enhanced_schema.pop("metadata")

            ir_operation["inputs"]["body"]["schema"] = enhanced_schema

            circuit_breaker.record_success()
            logger.info(f"      Schema enhanced successfully (TS)")
            return ir_operation

        except ValueError as ve:
            logger.warning(f"      Validation Error: {ve}")
            if attempt == max_retries:
                circuit_breaker.record_failure()
                return ir_operation
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)

        except Exception as e:
            logger.error(f"      LLM API Error (Attempt {attempt}): {e}")
            if attempt == max_retries:
                circuit_breaker.record_failure()
                return ir_operation
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)

    return ir_operation

===========================

====================
path: src\llm_enhancer\typescript_enhancer\ir_enhancer\prompts.py

ENHANCE_IR_PROMPT_TS = """

You are an expert Semantic Analyzer for Typescript code and JSON Schema generation.
Your task is to enrich and correct a "Basic JSON Schema" based on the provided "Typescript Source Code".

INPUTS:
1. Typescript Code (Function Source)
2. Basic JSON Schema (Generated by a fast AST Parser)
3. Type Definitions (Extracted from parser)

Function Name: {function_name}

The Basic Schema contains placeholders for complex types it could not resolve (e.g., `{"type": "object", "description": "Complex type: User"}`) and lacks semantic constraints.

=====================================
TASK:
=====================================

1. **Resolve Complex Types & Recursion:**
   - Locate classes (Pydantic Models, Dataclasses, TypedDict) defined in the source code AND the provided "Type Definitions".
   - Replace placeholders like `{"description": "Complex type: ClassName"}` with the full schema of that class.
   - **IMPORTANT:** Always add `"x-class-type": "ClassName"` to preserve the original type name.
   - **Crucial:** If a field is a container of complex types (e.g., `List[User]`, `Dict[str, Config]`), you must recursively expand the schema inside the `items` (for List) or `additionalProperties` (for Dict) fields.

2. **Resolve Enums:**
   - Locate `Enum` definitions in the code.
   - Replace placeholders with `{"type": "string", "enum": ["VAL1", "VAL2"], "x-enum-type": "EnumName", "description": "Enum: EnumName"}`.
   - **CRITICAL:** Always include `"x-enum-type": "EnumName"` to preserve the original Enum type name for display purposes.

3. **Handle Return Types:**
   - Check the function's return annotation. If the schema has a "return" or "response" key that is a complex type or placeholder, resolve it using the same logic as input parameters.

4. **Extract Constraints:**
   - **Pydantic/Dataclasses:** Look for `Field(min_length=5, max_length=10)`, `gt=0`, `lt=100`, `regex="..."`. Map these to `minLength`, `maxLength`, `minimum`, `maximum`, and `pattern` respectively.
   - **Typing:** Look for `typing.Annotated` constraints.
   - **Logic/Docstrings:** If explicit type constraints are missing, infer them from validation logic.
     - `if count % 5 != 0:` -> Add `"multipleOf": 5` to the schema.
     - `if x < 10:` -> Add `"maximum": 9` (int) or `"exclusiveMaximum": 10`.
     - `if not x.startswith(...):` -> Add `"pattern": "^..."`.
   - **Arithmetic:** specificially look for `%` modulo checks and map to `multipleOf`.
   - **Internal Whitelists:** Check for lists defined inside the function used for validation (e.g., `allowed_actions = ["list", "show"]`). Add these as `"enum": [...]` to the schema.
   - **Validation Checks:**
     - `if not table.isalnum():` -> Add `"pattern": "^[a-zA-Z0-9]+$"` and description "Alphanumeric only".
     - `dangerous_patterns = [...]` -> Description should note "Disallowed patterns: ...".
   - **Dictionary Keys:**
     - If a Dict argument has its keys checked (e.g., `if "name" not in profile` or `required = ["name", "email"]` loop), convert the `additionalProperties` schema to a fixed `properties` object with those keys marked as `required`.
   - **Graceful Handling:**
     - If the code checks a condition but returns a default value instead of raising an Error (e.g., `if not x: return "default"`), **DO NOT** add a `minLength` or `required` constraint that would force a test failure. Instead, add a description: "Empty input handled gracefully".

5. **Handle Unions and Optionals:**
   - If a type is `Optional[X]` or `X | None`, set the type as the resolved type of X and add `"nullable": true` (or use `anyOf` if strictly following JSON Schema Draft 7, but `nullable` is preferred for simplicity here).
   - If a type is `Union[A, B]`, generate an `anyOf` array containing resolved schemas for A and B.

6. **Extract Decorators:**
   - Inspect the target function definition {function_name}.
   - If decorators exist (e.g., `@auth_required`, `@router.get("/api")`), extract them.
   - **Add to Root:** Add a `"metadata": {"decorators": ["@name1(args)", ...]}` field at the ROOT level of the JSON output. Include arguments if present.

7. **Fallback for Missing Definitions:**
   - If a type is referenced in the signature but its definition is NOT found in the provided code snippet, **DO NOT** invent fields. Keep the original description or mark it as `{"type": "object", "description": "External type: TypeName"}`.

=====================================
RULES:
=====================================
- **DO NOT** remove existing properties found by the AST parser.
- **DO NOT** change the type of primitive fields (int, float, str, bool) unless the AST marked them as "Any".
- **PRESERVE** the structure of the AST schema (properties, required list) while enriching leaf nodes.
- **ENSURE** valid JSON Schema syntax for all resolved types.

=====================================
EXAMPLES:
=====================================

Input Schema:
```json
{
  "type": "object",
  "properties": {
    "status": {"type": "object", "description": "Complex type: OrderStatus"},
    "items": {"type": "array", "items": {"type": "object", "description": "Complex type: Product"}},
    "user": {"type": "object", "description": "Complex type: UserContext"}
  },
  "required": ["status"]
}
```

Code contains:
```Typescript
class OrderStatus(Enum):
    PENDING = "pending"
    SHIPPED = "shipped"

@dataclass
class Product:
    id: int
    name: Annotated[str, Field(min_length=3)]

@dataclass
class UserContext:
    user_id: str

@auth_required(role="admin")
def process_order(status: OrderStatus, items: List[Product], user: UserContext) -> bool:
    ...
```

Expected Output:
```json
{
  "type": "object",
  "properties": {
    "status": {
      "type": "string",
      "enum": ["pending", "shipped"],
      "x-enum-type": "OrderStatus",
      "description": "Enum: OrderStatus"
    },
    "items": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": {"type": "integer"},
          "name": {"type": "string", "minLength": 3}
        },
        "required": ["id", "name"],
        "x-class-type": "Product",
        "description": "Class: Product"
      }
    },
    "user": {
      "type": "object",
      "properties": {
        "user_id": {"type": "string"}
      },
      "required": ["user_id"],
      "x-class-type": "UserContext",
      "description": "Class: UserContext"
    }
  },
  "required": ["status"],
  "metadata": {
    "decorators": ["@auth_required(role='admin')"]
  }
}
```

=====================================
EXECUTION
=====================================
# Typescript Code:
{code}

# Type Definitions:
{types}

# Current Schema (AST):
{schema}

Return ONLY the updated JSON Schema as valid JSON.
NO explanations. NO markdown code blocks (```json). NO additional text.
Just the raw JSON object.
"""

===========================

====================
path: src\llm_enhancer\typescript_enhancer\ir_enhancer\validator.py

def validate_ir_enhancement_flexible(original: dict, enhanced: dict) -> bool:
    """
    Validates IR enhancement with support for Type Resolution.

    Allowed:
    - Adding constraints (min/max, pattern, etc.)
    - Resolving "Complex type" objects into detailed schemas with properties
    - Changing generic object to specific enum
    - Expanding placeholder objects

    Forbidden:
    - Removing properties defined in AST
    - Changing primitive types (e.g., int -> bool) unless original was Any/Object

    Args:
        original: Original schema from AST
        enhanced: Enhanced schema from LLM

    Returns:
        True if enhancement is valid
    """
    if "properties" in original:
        if "properties" not in enhanced:
            return False  # Should not lose properties container

        for prop_name, orig_prop in original["properties"].items():
            if prop_name not in enhanced["properties"]:
                print(f"      [Validation] Missing property: {prop_name}")
                return False

            enh_prop = enhanced["properties"][prop_name]

            # Allow Object -> Detailed Object resolution
            if orig_prop.get("type") == "object" and "description" in orig_prop:
                if "Complex type" in orig_prop["description"]:
                    continue  # Allow total replacement of complex placeholders

            # Allow Object -> Enum (for enum types marked as objects)
            if (
                orig_prop.get("type") == "object"
                and enh_prop.get("type") == "string"
                and "enum" in enh_prop
            ):
                continue  # Allow enum resolution

            # Recursive check for nested schemas
            if not validate_ir_enhancement_flexible(orig_prop, enh_prop):
                return False

    return True

===========================

====================
path: src\llm_enhancer\typescript_enhancer\ir_enhancer\__init__.py


===========================

====================
path: src\llm_enhancer\typescript_enhancer\test_suite_enhancer\enhancer.py

import sys
import time

sys.path.append("..")
from testsuitegen.src.llm_enhancer.client import llm_generate
from testsuitegen.src.llm_enhancer.typescript_enhancer.test_suite_enhancer.prompts import (
    ENHANCE_CODE_UNIT_PROMPT,
    ENHANCE_CODE_API_PROMPT,
)
from testsuitegen.src.llm_enhancer.typescript_enhancer.test_suite_enhancer.validator import (
    validate_no_logic_change,
)
from testsuitegen.src.llm_enhancer.circuit_breaker import circuit_breaker
from testsuitegen.src.config.settings import (
    LLM_ENABLED,
    MAX_LLM_RETRIES,
    EXPONENTIAL_BACKOFF_BASE,
)
from testsuitegen.src.exceptions.exceptions import LLMError, LLMFatalError


def _clean_llm_response(response: str) -> str:
    """Clean LLM response from markdown formatting and common issues."""
    # Remove markdown code blocks
    if response.startswith("```typescript"):
        response = response[13:]
    elif response.startswith("```ts"):
        response = response[5:]
    elif response.startswith("```javascript"):
        response = response[13:]
    elif response.startswith("```js"):
        response = response[5:]
    elif response.startswith("```"):
        response = response[3:]

    if response.endswith("```"):
        response = response[:-3]  # Remove closing ```

    # Strip whitespace
    response = response.strip()

    # Handle common LLM formatting issues
    lines = response.split("\n")
    cleaned_lines = []

    for line in lines:
        # Skip empty lines at start/end
        if not line.strip() and (not cleaned_lines or line == lines[-1]):
            continue
        cleaned_lines.append(line)

    return "\n".join(cleaned_lines)


def enhance_code(
    code: str,
    provider: str = None,
    model: str = None,
    max_retries: int = None,
    test_type: str = "api",
) -> str:
    """Enhance TypeScript test code."""
    if not LLM_ENABLED:
        return code

    # Use config default if not specified
    if max_retries is None:
        max_retries = MAX_LLM_RETRIES

    # 1. Check Circuit Breaker before attempting
    try:
        circuit_breaker.check_state()
    except LLMError as e:
        print(
            f"⚠ Circuit Breaker blocking LLM call for test code enhancement. Returning original code."
        )
        return code

    print(
        f"▶ Enhancing TypeScript code with LLM ({test_type} mode) using provider: {provider or 'default'}:{model or 'default'}..."
    )

    # Prepare prompt based on test type
    if test_type == "unit":
        prompt_template = ENHANCE_CODE_UNIT_PROMPT
    else:
        prompt_template = ENHANCE_CODE_API_PROMPT

    prompt = prompt_template.replace("{code}", code)

    # 2. Exponential Backoff Retry Loop
    for attempt in range(1, max_retries + 1):
        try:
            print(
                f"   LLM Attempt {attempt}/{max_retries} for test code enhancement..."
            )

            # Progressive Backoff Temperature: Increase temp by 0.1 for each retry to break loops
            base_temp = 0.01
            kwargs = {}
            if attempt > 1:
                kwargs["temperature"] = base_temp + 0.1 * (attempt - 1)

            # Call LLM
            raw_enhanced = llm_generate(
                prompt, provider=provider, model_override=model, **kwargs
            )

            # 3. STRICT VALIDATION - Clean and validate
            enhanced = _clean_llm_response(raw_enhanced)

            # Check if response looks like code (length check)
            if not enhanced or len(enhanced.strip()) < 10:
                print(
                    f"   ⚠ LLM returned empty or too short response. Retrying... Attempt {attempt}"
                )
                raise ValueError("Empty or invalid response from LLM")

            # Check for common hallucination patterns
            if enhanced.strip().startswith(
                ("Here", "Sure", "I'll", "Let me", "The code")
            ):
                print(
                    f"   ⚠ LLM returned text instead of code. Retrying... Attempt {attempt}"
                )
                raise ValueError("LLM returned explanation text instead of code")

            # Validate no logic changes
            try:
                validate_no_logic_change(original=code, enhanced=enhanced)
            except RuntimeError as validation_error:
                print(f"   ⚠ Logic validation failed. Retrying... Attempt {attempt}")
                raise ValueError(f"Logic change detected: {validation_error}")

            # 4. SUCCESS: Record Success and Return
            circuit_breaker.record_success()
            print(f"   ✨ Test code enhancement successful")
            return enhanced

        except ValueError as ve:
            # Validation errors
            print(f"   ⚠ Validation Error: {ve}")
            if attempt == max_retries:
                circuit_breaker.record_failure()
                return code
            time.sleep(2**attempt)

        except LLMFatalError as lf:
            print(f"   ⚠ Non-retryable LLM error: {lf}. Aborting code enhancement.")
            circuit_breaker.record_failure()
            return code

        except Exception as e:
            # Transient errors
            print(f"   ⚠ LLM API Error (Attempt {attempt}): {e}")
            if attempt == max_retries:
                circuit_breaker.record_failure()
                return code
            time.sleep(EXPONENTIAL_BACKOFF_BASE**attempt)

    return code

===========================

====================
path: src\llm_enhancer\typescript_enhancer\test_suite_enhancer\prompts.py

ENHANCE_CODE_API_PROMPT = """
You are a test code enhancer specializing in API contract testing with Jest and TypeScript.

YOUR TASK: Improve code quality, implement accurate test data setup, and fix TypeScript issues WITHOUT changing test logic.

IMPORTANT: Return ONLY clean TypeScript code without markdown formatting, code blocks, or explanations.

CRITICAL - NEVER MODIFY:
- DO NOT change test logic or assertions.
- DO NOT add or remove test cases or `testCases` array entries.
- DO NOT rename test descriptions or `it.each` arguments.
- DO NOT modify payload structure or expected status codes.
- DO NOT change API endpoint URLs or HTTP methods.
- DO NOT change `BASE_URL`, `ENDPOINT`, or `METHOD` constants logic (but ensure they are scoped correctly).

REQUIRED FIXES & IMPROVEMENTS:

1. **TypeScript & Scoping (CRITICAL):**
   - Ensure `const testCases`, `BASE_URL`, `ENDPOINT`, and `METHOD` are defined INSIDE the `describe` block to prevent "Cannot redeclare block-scoped variable" errors.
   - Use `const init: any = { ... }` when creating the fetch options object to prevent "Property 'body' does not exist" errors.
   - Use native `fetch` (Node.js 18+). DO NOT import `node-fetch`.

2. **Test Data Setup:**
   - If `testCases` requires dynamic data setup (e.g., creating resources), implement a `beforeAll` block inside the `describe`.
   - Use strict typing for data where possible, but use `any` if strict types block compilation of generated code.

3. **Code Quality:**
   - Improve comments and docstrings (JsDoc).
   - Fix indentation and formatting.

OUTPUT FORMAT:
Return ONLY valid TypeScript code.
NO explanations.
NO markdown code blocks.

Code to enhance:
----------------
{code}
----------------
"""

ENHANCE_CODE_UNIT_PROMPT = """
You are a test code enhancer specializing in TypeScript unit testing with Jest.

YOUR TASK: Improve readability and mock setup using data from test parameters WITHOUT changing test logic.

IMPORTANT: Return ONLY clean TypeScript code without markdown formatting.

CRITICAL - NEVER MODIFY:
- DO NOT change test logic or assertions
- DO NOT add or remove test cases
- DO NOT rename functions
- DO NOT change expected values

REQUIRED FIXES & IMPROVEMENTS:

1. **Mocking & Setup:**
   - Use `jest.mock(...)` or `jest.spyOn(...)` correctly.
   - Ensure mocks return data that matches the structure expected by the function under test, implied by `testCases` payloads.

2. **TypeScript:**
   - Fix `any` types where specific interfaces can be inferred, but default to `any` if unsure to avoid breaking compilation.
   - Ensure imports are correct for the module under test.

OUTPUT FORMAT:
Return ONLY valid TypeScript code.
NO explanations.
NO markdown code blocks.

Code to enhance:
----------------
{code}
----------------
"""

===========================

====================
path: src\llm_enhancer\typescript_enhancer\test_suite_enhancer\validator.py

def validate_no_logic_change(original: str, enhanced: str):
    """Validate that LLM only made safe enhancements, not harmful logic changes."""

    # TypeScript specific harmful patterns
    harmful_patterns = [
        r"expectedStatus.*:",  # Changed expected status in JSON object
        r"expectedResult.*:",  # Changed expected result
        r"expect\(.*\.toBe\(",  # Changed assertions
        r"describe\(",  # Should not remove/change describe blocks essentially
        r"const BASE_URL",  # Constants
        r"const ENDPOINT",
        r"const METHOD",
    ]

    for pattern in harmful_patterns:
        orig_count = len([line for line in original.split("\n") if pattern in line])
        enhanced_count = len([line for line in enhanced.split("\n") if pattern in line])

        if orig_count != enhanced_count:
            # For Jest/TS, exact line validation is tricky due to formatting changes (e.g. object keys).
            # But critical logic like 'expectedStatus: 200' should appear same number of times.
            raise RuntimeError(
                f"LLM modified test logic (pattern: {pattern}) — enhancement rejected"
            )

    # Allow enhancements that add fixtures, comments, or formatting
    return True

===========================

====================
path: src\llm_enhancer\typescript_enhancer\test_suite_enhancer\__init__.py


===========================

====================
path: src\parsers\validator.py

"""Pre-parsing validation logic for input files."""

import json
import ast
from pathlib import Path
from typing import Union

from testsuitegen.src.exceptions.exceptions import InvalidSpecError, FileError


def validate_input_spec(file_path: Union[str, Path]):
    """
    Performs structural validation on OpenAPI or Python files.

    Args:
        file_path: Path to the specification file.

    Raises:
        FileError: If file doesn't exist or extension is wrong.
        InvalidSpecError: If the file structure is malformed.
    """
    path = Path(file_path)

    # 1. File Existence & Type Check
    if not path.exists():
        raise FileError(
            f"File not found: {file_path}", "Please check the path and try again."
        )

    suffix = path.suffix.lower()
    if suffix not in [".json", ".yaml", ".yml", ".py"]:
        raise FileError(
            f"Unsupported file format: {suffix}",
            "Supported formats are: .json, .yaml, .yml, .py",
        )

    # 2. Read content safely
    try:
        content = path.read_text(encoding="utf-8")
    except Exception as e:
        raise FileError(f"Could not read file: {file_path}", str(e))

    # 3. Validate Structure based on type
    if suffix == ".py":
        _validate_python_structure(path, content)
    else:
        _validate_openapi_structure(path, content, suffix)


def _validate_openapi_structure(path: Path, content: str, suffix: str):
    """Validates OpenAPI YAML/JSON structure."""
    try:
        if suffix == ".json":
            data = json.loads(content)
        else:
            # Basic YAML import (assuming pyyaml is installed)
            import yaml

            data = yaml.safe_load(content)
    except json.JSONDecodeError as e:
        raise InvalidSpecError(
            f"Invalid JSON syntax in {path.name}",
            f"Line {e.lineno}, Column {e.colno}: {e.msg}",
        )
    except Exception as e:
        # Handle YAML errors or other parsing issues
        if "yaml" in str(type(e).__module__):
            # YAML error
            problem_mark = e.problem_mark if hasattr(e, "problem_mark") else None
            location = (
                f"Line {problem_mark.line}" if problem_mark else "Unknown location"
            )
            problem_desc = str(e.problem) if hasattr(e, "problem") else "Syntax error"
            raise InvalidSpecError(
                f"Invalid YAML syntax in {path.name}", f"{location}: {problem_desc}"
            )
        raise InvalidSpecError(f"Failed to parse OpenAPI file", str(e))

    # Structural Checks (Deep enough to catch basic issues, fast enough to run instantly)
    if not isinstance(data, dict):
        raise InvalidSpecError(
            "Root element must be an object (dictionary)",
            "The file provided is likely a list or raw value.",
        )

    # Check for OpenAPI version (v3.x or v2.0)
    if "openapi" not in data and "swagger" not in data:
        raise InvalidSpecError(
            "Missing 'openapi' or 'swagger' key at root level",
            "This file does not appear to be a valid OpenAPI specification.",
        )

    # Check for paths
    if "paths" not in data:
        raise InvalidSpecError(
            "Missing 'paths' key at root level",
            "An OpenAPI spec must define paths/endpoints to generate tests.",
        )


def _validate_python_structure(path: Path, content: str):
    """Validates Python syntax using AST."""
    try:
        ast.parse(content, filename=str(path))
    except SyntaxError as e:
        raise InvalidSpecError(
            f"Python syntax error in {path.name}", f"Line {e.lineno}: {e.msg}"
        )

===========================

====================
path: src\parsers\__init__.py


===========================

====================
path: src\parsers\openapi_parser\parser.py

import yaml


class Parser:

    __current_spec = None  # for $ref resolution
    HTTP_METHODS = {"get", "post", "put", "delete", "patch", "options", "head"}

    def __init__(self, raw_spec: str) -> None:
        self._spec = yaml.safe_load(raw_spec)
        self.__current_spec = self._spec
        self._operations = []

    def parse(self):

        operation_data = {
            "title": self._spec.get("info", {}).get("title", "API"),
            "version": self._spec.get("info", {}).get("version", "1.0.0"),
        }

        if self._spec.get("info", {}).get("description") != "":
            operation_data["description"] = self._spec.get("info", {}).get(
                "description", ""
            )

        paths = self._spec.get("paths", {})

        for path, methods in paths.items():
            for method, operation in methods.items():
                if method.lower() not in self.HTTP_METHODS:
                    continue

                operation_id = operation.get(
                    "operationId",
                    f"{method}_{path}".replace("/", "_")
                    .replace("{", "")
                    .replace("}", ""),
                )

                self._operations.append(
                    {
                        "id": operation_id,
                        "kind": "http",
                        "method": method.upper(),
                        "path": path,
                        "inputs": self.__parse_inputs(operation),
                        "outputs": self.__parse_outputs(operation),
                        "errors": self.__parse_errors(operation),
                    }
                )

        operation_data["operations"] = self._operations

        return operation_data

    def __parse_inputs(self, operation: dict) -> dict:

        parameters = operation.get("parameters", [])

        path_params = []
        query_params = []
        header_params = []

        for parameter in parameters:
            # Get schema and normalize it to resolve $ref and extract other properties
            raw_schema = parameter.get("schema", {"type": "object"})
            normalized_schema = self.__normalize_schema(raw_schema)

            params = {
                "name": parameter.get("name"),
                "required": parameter.get("required", False),
                "schema": normalized_schema,
            }

            location = parameter.get("in")
            if location == "path":
                path_params.append(params)
            elif location == "query":
                query_params.append(params)
            elif location == "header":
                header_params.append(params)

        body = self.__parse_request_body(operation.get("requestBody"))

        return {
            "path": path_params,
            "query": query_params,
            "headers": header_params,
            "body": body,
        }

    def __parse_outputs(self, operation: dict) -> dict:

        outputs = []

        for status, resp in operation.get("responses", {}).items():

            if not status.isdigit():
                continue

            code = int(status)
            if code >= 500:
                continue

            content = resp.get("content", {})
            json_body = content.get("application/json", {})

            if json_body:
                outputs.append(
                    {
                        "status": code,
                        "description": resp.get("description", ""),
                        "content_type": "application/json",
                        "schema": self.__normalize_schema(json_body.get("schema", {})),
                    }
                )
            else:
                outputs.append(
                    {
                        "status": code,
                        "description": resp.get("description", ""),
                        "content_type": None,
                        "schema": None,
                    }
                )

        return outputs

    def __parse_errors(self, operation: dict) -> list:
        errors = []

        for status, resp in operation.get("responses", {}).items():

            if not status.isdigit():
                continue

            code = int(status)
            if code < 400 or code >= 500:
                continue

            content = resp.get("content", {})
            json_body = content.get("application/json", {})

            if json_body:
                errors.append(
                    {
                        "status": code,
                        "description": resp.get("description", "http_error"),
                        "schema": self.__normalize_schema(json_body.get("schema", {})),
                        "content_type": "application/json",
                    }
                )
            else:
                errors.append(
                    {
                        "status": code,
                        "description": resp.get("description", ""),
                        "schema": None,
                        "content_type": None,
                    }
                )

        return errors

    def __normalize_schema(self, schema: dict) -> dict:

        if not isinstance(schema, dict):
            return {"type": "object", "nullable": False}

        # Keep original reference for preserving constraint keywords
        original_schema = dict(schema)
        schema = dict(schema)

        if "$ref" in schema:
            ref_path = schema["$ref"]
            resolved = (
                self._resolve_ref(ref_path, self.__current_spec)
                if self.__current_spec
                else {}
            )
            # Merge resolved schema with any additional properties in the original schema
            schema.pop("$ref")
            resolved = dict(resolved)
            resolved.update(schema)
            schema = resolved

        # Default nullable
        schema["nullable"] = schema.get("nullable", False)

        # Normalize properties
        if "properties" in schema:
            schema["properties"] = {
                k: self.__normalize_schema(v) for k, v in schema["properties"].items()
            }

        # Normalize array items
        if "items" in schema:
            schema["items"] = self.__normalize_schema(schema["items"])

        # Normalize allOf (merge all schemas into one)
        if "allOf" in schema:
            merged = {}
            for sub_schema in schema["allOf"]:
                normalized_sub = self.__normalize_schema(sub_schema)
                # Merge properties
                if "properties" in normalized_sub:
                    if "properties" not in merged:
                        merged["properties"] = {}
                    merged["properties"].update(normalized_sub["properties"])
                # Merge required fields
                if "required" in normalized_sub:
                    if "required" not in merged:
                        merged["required"] = []
                    merged["required"].extend(normalized_sub["required"])
                # Merge other fields (type, etc.)
                for key, value in normalized_sub.items():
                    if key not in ["properties", "required", "allOf"]:
                        merged[key] = value
            # Remove allOf and replace with merged schema
            schema.pop("allOf")
            schema.update(merged)
            # Deduplicate required fields
            if "required" in schema:
                schema["required"] = list(set(schema["required"]))

        # Normalize anyOf
        if "anyOf" in schema:
            non_null = [s for s in schema["anyOf"] if s.get("type") != "null"]
            has_null = len(non_null) != len(schema["anyOf"])

            if len(non_null) == 1:
                merged = self.__normalize_schema(non_null[0])
                merged["nullable"] = has_null
                return merged

            return {
                "oneOf": [self.__normalize_schema(s) for s in non_null],
                "nullable": has_null,
            }
        # Normalize unions
        if "oneOf" in schema:
            schema["oneOf"] = [self.__normalize_schema(s) for s in schema["oneOf"]]

        # Normalize not (negation schema)
        if "not" in schema:
            schema["not"] = self.__normalize_schema(schema["not"])

        # Normalize discriminator mapping (resolve $ref in mapping values)
        if "discriminator" in schema and isinstance(schema["discriminator"], dict):
            discriminator = schema["discriminator"]
            if "mapping" in discriminator and isinstance(
                discriminator["mapping"], dict
            ):
                resolved_mapping = {}
                for key, ref_path in discriminator["mapping"].items():
                    if isinstance(ref_path, str) and ref_path.startswith("#/"):
                        # Resolve the reference and normalize the schema
                        resolved = (
                            self._resolve_ref(ref_path, self.__current_spec)
                            if self.__current_spec
                            else {}
                        )
                        resolved_mapping[key] = self.__normalize_schema(resolved)
                    else:
                        resolved_mapping[key] = ref_path
                discriminator["mapping"] = resolved_mapping

        # Normalize additionalProperties
        if "additionalProperties" in schema:
            if isinstance(schema["additionalProperties"], dict):
                schema["additionalProperties"] = self.__normalize_schema(
                    schema["additionalProperties"]
                )

        if "oneOf" in schema:
            schema.pop("type", None)
            return schema

        # Default type only for non-unions
        schema.setdefault("type", "object")

        # PRESERVE all OpenAPI constraint keywords
        # These are critical for test intent generation and payload validation
        preserved_keys = [
            # Enums and defaults
            "enum",
            "default",
            "example",
            # String constraints
            "minLength",
            "maxLength",
            "pattern",
            "format",
            # Numeric constraints
            "minimum",
            "maximum",
            "exclusiveMinimum",
            "exclusiveMaximum",
            "multipleOf",
            # Array constraints
            "minItems",
            "maxItems",
            "uniqueItems",
            # Object constraints
            "minProperties",
            "maxProperties",
            "required",
            # Access modifiers
            "readOnly",
            "writeOnly",
            # Metadata
            "deprecated",
            "title",
            "description",
            # Conditional logic (OpenAPI 3.0/3.1)
            "dependencies",
            "dependentRequired",
            "dependentSchemas",
        ]

        # Preserve these keywords from original schema if they exist
        for key in preserved_keys:
            if key in original_schema and key not in schema:
                schema[key] = original_schema[key]

        return schema

    def _resolve_ref(self, ref: str, spec: dict) -> dict:
        """
        Resolve a JSON reference like '#/components/schemas/UserCreate'
        """
        if not ref.startswith("#/"):
            return {}

        parts = ref[2:].split("/")  # Remove '#/' and split
        current = spec

        for part in parts:
            if isinstance(current, dict):
                current = current.get(part, {})
            else:
                return {}

        return current if isinstance(current, dict) else {}

    def __parse_request_body(self, request_body: dict | None) -> dict:
        if not request_body:
            return {}

        content = request_body.get("content", {})

        json_body = content.get("application/json")
        if not json_body:
            return {}

        schema = self.__normalize_schema(json_body.get("schema", {}))

        return {
            "content_type": "application/json",
            "required": request_body.get("required", False),
            "schema": schema,
        }


if __name__ == "__main__":

    import json

    filepath = [
        # "input_parser/openapi_parser/examples/openapi_easy.yaml",
        # "input_parser/openapi_parser/examples/openapi_medium.yaml",
        # "input_parser/openapi_parser/examples/openapi_hard.yaml",
        "test.yaml"
    ]

    for path in filepath:
        print(f"Parsing {path}...")
        with open(path, "r") as f:
            raw = f.read()

        parser = Parser(raw)

        parsed_spec = parser.parse()
        print(json.dumps(parsed_spec, indent=2))

===========================

====================
path: src\parsers\openapi_parser\__init__.py

# OpenAPI Parser module
# Parses OpenAPI specifications into IR operations

===========================

====================
path: src\parsers\openapi_parser\examples\openapi_easy.yaml

openapi: 3.0.3
info:
  title: User API (Easy)
  version: 1.0.0

paths:
  /user:
    post:
      summary: Create a user
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              additionalProperties: false
              required:
                - name
                - age
              properties:
                name:
                  type: string
                age:
                  type: integer
                active:
                  type: boolean
      responses:
        "200":
          description: User created
          content:
            application/json:
              schema:
                type: object
                properties:
                  id:
                    type: string

===========================

====================
path: src\parsers\openapi_parser\examples\openapi_hard.yaml

openapi: 3.0.3
info:
  title: Analytics API (Difficult)
  version: 1.0.0

paths:
  /analytics/report:
    post:
      summary: Submit analytics report
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              additionalProperties: false
              required:
                - user
                - metrics
              properties:
                user:
                  type: object
                  additionalProperties:
                    oneOf:
                      - type: string
                      - type: integer
                metrics:
                  type: array
                  minItems: 1
                  items:
                    type: object
                    additionalProperties: false
                    required:
                      - name
                      - values
                    properties:
                      name:
                        type: string
                      values:
                        type: array
                        items:
                          oneOf:
                            - type: number
                            - type: string
                context:
                  type: object
                  nullable: true
                  additionalProperties:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: boolean
      responses:
        "200":
          description: Report processed
          content:
            application/json:
              schema:
                type: object
                additionalProperties:
                  oneOf:
                    - type: string
                    - type: number
                    - type: object

===========================

====================
path: src\parsers\openapi_parser\examples\openapi_medium.yaml

openapi: 3.0.3
info:
  title: Order API (Medium)
  version: 1.0.0

paths:
  /order:
    post:
      summary: Create an order
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              additionalProperties: false
              required:
                - orderId
                - items
              properties:
                orderId:
                  type: string
                items:
                  type: array
                  minItems: 1
                  items:
                    type: object
                    additionalProperties: false
                    required:
                      - sku
                      - quantity
                    properties:
                      sku:
                        type: string
                      quantity:
                        type: integer
                discount:
                  oneOf:
                    - type: integer
                    - type: string
                metadata:
                  type: object
                  additionalProperties:
                    type: string
      responses:
        "200":
          description: Order accepted
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string

===========================

====================
path: src\parsers\python_parser\examples.py

# examples.py
# Collection of example functions demonstrating various Python type annotations
# From simple to complex, showcasing different data types and patterns

from typing import List, Dict, Union, Optional, Tuple, Set, FrozenSet, Any, Callable


# Example 1: Simple function with basic types
def example_1_simple_function(name: str, age: int) -> str:
    """Simple function with basic types."""
    return f"Hello {name}, you are {age} years old."


# Example 2: Function with optional parameters
def example_2_optional_function(name: str, age: Optional[int] = None) -> str:
    """Function with optional parameter."""
    if age:
        return f"Hello {name}, you are {age} years old."
    return f"Hello {name}."


# Example 3: Function with list
def example_3_list_function(items: List[str]) -> int:
    """Function that takes a list and returns its length."""
    return len(items)


# Example 4: Function with dictionary
def example_4_dict_function(data: Dict[str, int]) -> List[str]:
    """Function that processes a dictionary."""
    return [f"{k}: {v}" for k, v in data.items()]


# Example 5: Function with union types
def example_5_union_function(value: Union[str, int]) -> str:
    """Function that accepts string or int."""
    return str(value)


# Example 6: Function with tuple
def example_6_tuple_function(coords: Tuple[float, float]) -> float:
    """Function that calculates distance from origin."""
    x, y = coords
    return (x**2 + y**2) ** 0.5


# Example 7: Function with set
def example_7_set_function(tags: Set[str]) -> int:
    """Function that returns number of unique tags."""
    return len(tags)


# Example 8: Function with complex nested types
def example_8_complex_function(
    user: Dict[str, Union[str, int]],
    scores: List[Dict[str, float]],
    metadata: Optional[Dict[str, List[str]]] = None,
) -> Dict[str, Any]:
    """Complex function with nested types."""
    result = {
        "user": user,
        "average_score": (
            sum(score["value"] for score in scores) / len(scores) if scores else 0
        ),
        "metadata": metadata or {},
    }
    return result


# Example 9: Function with callable
def example_9_callback_function(
    data: List[int], processor: Callable[[int], int]
) -> List[int]:
    """Function that applies a callback to each item."""
    return [processor(item) for item in data]


# Example 10: Function with all basic types
def example_10_all_types_function(
    s: str, i: int, f: float, b: bool, bt: bytes
) -> Tuple[str, int, float, bool, str]:
    """Function demonstrating all basic Python types."""
    return s, i, f, b, bt.decode("utf-8")


# Example 11: Function with mutable defaults (should warn)
def example_11_mutable_defaults_function(
    items: List[int] = [], config: Dict[str, str] = {}
) -> Dict[str, Any]:
    """Function with mutable defaults - should generate warnings."""
    items.append(len(config))
    config["processed"] = "true"
    return {"items": items, "config": config}


# Example 12: Function with advanced unions
def example_12_advanced_union_function(
    value: Union[str, int, float, bool, None],
) -> str:
    """Function with union of multiple types including None."""
    if value is None:
        return "None"
    return f"Value: {value} (type: {type(value).__name__})"


# Example 13: Function with generic types
def example_13_generic_function(
    data: List[Dict[str, Union[int, str]]], mapping: Dict[str, Callable[[Any], Any]]
) -> List[Dict[str, Any]]:
    """Function with complex generic types."""
    result = []
    for item in data:
        processed = {}
        for key, value in item.items():
            if key in mapping:
                processed[key] = mapping[key](value)
            else:
                processed[key] = value
        result.append(processed)
    return result


# Example 14: Function with frozenset
def example_14_frozenset_function(tags: FrozenSet[str]) -> List[str]:
    """Function that works with frozenset."""
    return sorted(list(tags))


# Example 15: Function with variable-length tuple
def example_15_var_tuple_function(coords: Tuple[float, ...]) -> float:
    """Function with variable-length tuple."""
    return sum(x**2 for x in coords) ** 0.5


# Example 16: Extremely complex function
def example_16_extremely_complex_function(
    # Basic and collections (required)
    name: str,
    scores: List[float],
    metadata: Dict[str, Union[str, int, List[str]]],
    data: List[Dict[str, Union[int, str, List[Union[int, str]]]]],
    # Sets and tuples (with defaults)
    tags: Set[str] = set(),
    coordinates: Tuple[float, float, float] = (0.0, 0.0, 0.0),
    # Advanced types (optional)
    callback: Optional[Callable[[Dict[str, Any]], bool]] = None,
    config: Optional[Dict[str, Any]] = None,
    # Any and unions
    extra: Any = None,
    status: Union[str, int, None] = None,
) -> Dict[str, Union[str, float, List[Any], Dict[str, Any], Any]]:
    """Extremely complex function with all possible type constructs."""
    result = {
        "name": name,
        "average_score": sum(scores) / len(scores) if scores else 0,
        "metadata": metadata,
        "data_count": len(data),
        "unique_tags": len(tags),
        "distance": sum(x**2 for x in coordinates) ** 0.5,
        "extra": extra,
        "status": status,
    }

    if callback and config:
        result["validated"] = callback(config)

    return result


# Example 17: Function with class method (for testing self parameter)
class ExampleClass:
    def example_17_method_with_self(self, value: str) -> str:
        """Method that includes self parameter."""
        return f"Processed: {value}"


# Example 18: Function with *args and **kwargs (though not fully typed)
def example_18_args_kwargs_function(*args: int, **kwargs: str) -> Dict[str, Any]:
    """Function with *args and **kwargs."""
    return {"args_count": len(args), "kwargs": kwargs, "args_sum": sum(args)}


# Example 19: Recursive type (simulated with Any)
def example_19_recursive_function(data: Dict[str, Any]) -> Dict[str, Any]:
    """Function that could handle recursive structures."""
    # In practice, recursive types are hard to define in typing

    def r(d):
        if d == 0 or d == 1:
            return 1
        return r(d - 1) + r(d - 2)

    return r(data)


# Example 20: Function with literal types (using Union as approximation)
def example_20_literal_like_function(mode: Union[str, int]) -> str:
    """Function with literal-like types."""
    if isinstance(mode, str):
        return f"String mode: {mode}"
    return f"Int mode: {mode}"


# Example 21: Enum Definition
import enum
class Color(enum.Enum):
    RED = 1
    GREEN = 2
    BLUE = 3


# Example 22: Dataclass Definition
from dataclasses import dataclass
@dataclass
class UserInfo:
    id: int
    username: str
    email: str
    is_active: bool = True

# Example 23: Pydantic Model
from pydantic import BaseModel

class Product(BaseModel):
    name: str
    price: float
    tags: List[str]


if __name__ == "__main__":
    try:
        from testsuitegen.src.parsers.python_parser.parser import parse_python_function
    except ImportError:
        # Fallback if running from the directory itself (not recommended due to name conflict with stdlib parser)
        from parser import parse_python_function

    # Test all examples
    examples = [
        ("simple_function", example_1_simple_function),
        ("optional_function", example_2_optional_function),
        ("list_function", example_3_list_function),
        ("dict_function", example_4_dict_function),
        ("union_function", example_5_union_function),
        ("tuple_function", example_6_tuple_function),
        ("set_function", example_7_set_function),
        ("complex_function", example_8_complex_function),
        ("callback_function", example_9_callback_function),
        ("all_types_function", example_10_all_types_function),
        ("mutable_defaults_function", example_11_mutable_defaults_function),
        ("advanced_union_function", example_12_advanced_union_function),
        ("generic_function", example_13_generic_function),
        ("frozenset_function", example_14_frozenset_function),
        ("var_tuple_function", example_15_var_tuple_function),
        ("extremely_complex_function", example_16_extremely_complex_function),
        ("method_with_self", ExampleClass().example_17_method_with_self),
        ("args_kwargs_function", example_18_args_kwargs_function),
        ("recursive_function", example_19_recursive_function),
        ("literal_like_function", example_20_literal_like_function),
        ("color_enum", Color),
        ("user_info_dataclass", UserInfo),
        ("product_model", Product),
    ]

    import json

    for name, func in examples:
        try:
            parsed = parse_python_function(func)
            print(f"\n=== {name} ===")
            print(json.dumps(parsed, indent=2))
        except Exception as e:
            print(f"\n=== {name} ===")
            print(f"Error parsing {name}: {e}")

===========================

====================
path: src\parsers\python_parser\parser.py

import ast
import logging
import sys
import inspect
import textwrap

# Configure logging
logger = logging.getLogger(__name__)


class PythonParser:
    """
    Deterministic Parser for Python Source Code using AST.
    Converts Python source directly to TestSuiteGen IR.
    """

    def __init__(self, source_code: str):
        self.source_code = source_code
        self.type_registry = {}  # Maps type name -> parsed type definition
        try:
            self.tree = ast.parse(source_code)
        except SyntaxError as e:
            raise ValueError(f"Invalid Python syntax: {e}")

    def parse(self) -> dict:
        operations = []
        types = []

        # First pass: Parse all classes to build type registry
        for node in ast.walk(self.tree):
            if isinstance(node, ast.ClassDef):
                try:
                    type_def = self._parse_class(node)
                    if type_def:
                        types.append(type_def)
                        # Register the type for lookup during function parsing
                        self.type_registry[type_def["id"]] = type_def
                except Exception as e:
                    logger.warning(f"Skipping class '{node.name}': {e}")
                    continue

        # Second pass: Parse functions (can now resolve custom types via registry)
        for node in ast.walk(self.tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                try:
                    op = self._parse_function(node)
                    operations.append(op)
                except ValueError as e:
                    logger.warning(f"Skipping function '{node.name}': {e}")
                    continue

        return {"operations": operations, "types": types}

    def _parse_class(self, class_node) -> dict:
        """
        Parses a ClassDef into a Type Definition (Enum or Model).
        """
        class_name = class_node.name
        docstring = ast.get_docstring(class_node) or ""
        
        # Check Bases for Enum
        is_enum = any(self._get_name_from_node(base) == "Enum" for base in class_node.bases)
        
        # Check Decorators for Dataclasses
        decorators = []
        is_dataclass = False
        for dec in class_node.decorator_list:
            dec_name = ""
            if isinstance(dec, ast.Name):
                dec_name = dec.id
            elif isinstance(dec, ast.Call) and isinstance(dec.func, ast.Name):
                dec_name = dec.func.id
            
            if dec_name:
                decorators.append(f"@{dec_name}")
                if "dataclass" in dec_name:
                    is_dataclass = True

        if is_enum:
            values = []
            for item in class_node.body:
                if isinstance(item, ast.Assign):
                    for target in item.targets:
                        if isinstance(target, ast.Name):
                            # Enum member: NAME = VALUE
                            member_name = target.id
                            # Try to get simple value
                            member_value = None
                            if isinstance(item.value, ast.Constant):
                                member_value = item.value.value
                            values.append({"name": member_name, "value": member_value})
            
            return {
                "id": class_name,
                "kind": "enum",
                "description": docstring,
                "values": values
            }

        # Otherwise, treat as Model (Dataclass or Pydantic or TypedDict)
        properties = {}
        required = []
        
        for item in class_node.body:
            if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                # Field: name: type = default
                field_name = item.target.id
                field_schema = self._node_to_schema(item.annotation)
                properties[field_name] = field_schema
                
                # Check for default value
                if item.value is None:
                    # No default value -> required
                    # Check if type is Optional/Nullable
                    if not field_schema.get("nullable", False):
                        required.append(field_name)
        
        return {
            "id": class_name,
            "kind": "model",
            "description": docstring,
            "metadata": {"decorators": decorators, "is_dataclass": is_dataclass},
            "schema": {
                "type": "object",
                "properties": properties,
                "required": required
            }
        }

    def _parse_function(self, func_node) -> dict:
        """
        Converts a single function AST node into an IR Operation.
        """
        function_name = func_node.name
        docstring = ast.get_docstring(func_node) or ""

        is_async = isinstance(func_node, ast.AsyncFunctionDef)

        # Extract Decorators
        decorators = []
        for dec in func_node.decorator_list:
            if isinstance(dec, ast.Name):
                decorators.append(f"@{dec.id}")
            elif isinstance(dec, ast.Call) and isinstance(dec.func, ast.Name):
                decorators.append(f"@{dec.func.id}")

        # Validate Strict Rules
        if func_node.args.vararg or func_node.args.kwarg:
            raise ValueError("Functions with *args or **kwargs are not supported.")

        # 2. Parse Arguments -> JSON Schema Properties
        properties = {}
        required = []

        # Calculate where defaults start
        num_args = len(func_node.args.args)
        num_defaults = len(func_node.args.defaults)
        first_default_index = num_args - num_defaults

        for i, arg in enumerate(func_node.args.args):
            if arg.arg in ["self", "cls"]:
                continue

            # Strict Mode: Require Type Hints
            if not arg.annotation:
                raise ValueError(f"Argument '{arg.arg}' is missing a type hint.")

            # --- CORE CHANGE: Direct AST to Schema Mapping ---
            schema = self._node_to_schema(arg.annotation)
            properties[arg.arg] = schema

            # Handle Optional/Nullable logic for 'required' list
            is_optional_type = schema.get("nullable", False)
            has_default = i >= first_default_index

            if not has_default and not is_optional_type:
                required.append(arg.arg)

        body_schema = {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False,
        }

        # Parse Return Type
        return_schema = (
            self._node_to_schema(func_node.returns) if func_node.returns else {}
        )



        # 6. Construct IR
        return {
            "id": function_name,
            "kind": "function",
            "async": is_async,
            "description": docstring,
            "metadata": {"decorators": decorators},
            "inputs": {
                "path": [],
                "query": [],
                "headers": [],
                "body": {
                    "content_type": "application/json",
                    "required": True,
                    "schema": body_schema,
                },
            },
            "outputs": [
                {
                    "status": 200,
                    "content_type": "application/json",
                    "schema": return_schema,
                }
            ],
            "errors": [],  # Will be populated by LLM in Step 2
        }



    def _node_to_schema(self, node) -> dict:
        """
        Recursively converts an AST node into a JSON Schema dictionary.
        Handles Union, Optional, List, Dict, Tuple, Set, Literal, and more.
        """
        if node is None:
            return {}

        # 1. Basic Names (int, str, MyClass)
        if isinstance(node, ast.Name):
            return self._resolve_name(node.id)

        # 2. Attributes (typing.List, pydantic.BaseModel)
        if isinstance(node, ast.Attribute):
            return self._resolve_name(node.attr)

        # 3. Constants (Literal values)
        if isinstance(node, ast.Constant):
            return {"const": node.value}

        # 4. Subscripts (List[int], Union[A, B], Optional[T])
        if isinstance(node, ast.Subscript):
            container_name = self._get_name_from_node(node.value)
            
            # Unbox the slice
            if sys.version_info < (3, 9) and isinstance(node.slice, ast.Index):
                 slice_node = node.slice.value
            else:
                 slice_node = node.slice

            # --- List / Set / Iterable ---
            if container_name in ["List", "list", "Set", "set", "FrozenSet", "Iterable", "Sequence", "Deque"]:
                item_schema = self._node_to_schema(slice_node)
                return {"type": "array", "items": item_schema}

            # --- Tuple ---
            if container_name in ["Tuple", "tuple"]:
                if isinstance(slice_node, ast.Tuple):
                    items_schemas = [self._node_to_schema(elt) for elt in slice_node.elts]
                    return {
                        "type": "array",
                        "prefixItems": items_schemas,
                        "minItems": len(items_schemas),
                        "maxItems": len(items_schemas)
                    }
                else:
                    # Tuple[int] matches Tuple[int, ...] in some versions, or single item tuple
                    return {"type": "array", "items": self._node_to_schema(slice_node)}

            # --- Dict / Mapping ---
            if container_name in ["Dict", "dict", "Mapping", "MutableMapping"]:
                if isinstance(slice_node, ast.Tuple) and len(slice_node.elts) >= 2:
                    value_schema = self._node_to_schema(slice_node.elts[1])
                    return {"type": "object", "additionalProperties": value_schema}
                return {"type": "object", "additionalProperties": True}

            # --- Optional ---
            if container_name == "Optional":
                schema = self._node_to_schema(slice_node)
                schema["nullable"] = True
                return schema

            # --- Union ---
            if container_name == "Union":
                return self._handle_union_node(slice_node)

            # --- Literal ---
            if container_name == "Literal":
                return self._handle_literal_node(slice_node)
                
            # --- Type or ClassVar ---
            if container_name in ["Type", "ClassVar"]:
                return self._node_to_schema(slice_node)
            
            # --- Callable ---
            if container_name in ["Callable"]:
                return {"type": "object", "description": "Callable/Function"}

        # 5. Binary Operations (Python 3.10+ Union: int | str)
        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.BitOr):
            return self._handle_binop_union(node)

        return {"type": "object", "description": f"Complex/Unknown Type"}

    def _handle_literal_node(self, slice_node) -> dict:
        """Handles Literal['a', 'b', 1] -> enum schema."""
        values = []
        if isinstance(slice_node, ast.Tuple):
            for elt in slice_node.elts:
                if isinstance(elt, ast.Constant):
                    values.append(elt.value)
        elif isinstance(slice_node, ast.Constant):
            values.append(slice_node.value)
            
        if not values:
            return {}
            
        # Determine strict type if all match
        types = set(type(v) for v in values)
        schema_type = "string"
        if types == {int}: schema_type = "integer"
        elif types == {float}: schema_type = "number"
        elif types == {bool}: schema_type = "boolean"
        # Mixed types -> no 'type' field, just enum
        
        if len(types) == 1:
            return {"type": schema_type, "enum": values}
        return {"enum": values}


    def _resolve_name(self, name: str) -> dict:
        """Maps a type name to a schema."""

        name_lower = name.lower()
        if name_lower in ["int", "integer"]:
            return {"type": "integer"}
        if name_lower in ["float", "number"]:
            return {"type": "number"}
        if name_lower in ["str", "string"]:
            return {"type": "string"}
        if name_lower in ["bool", "boolean"]:
            return {"type": "boolean"}
        if name_lower == "none":
            return {"type": "null"}
        if name_lower in ["dict", "object"]:
            return {"type": "object"}
        if name_lower in ["list", "array"]:
            return {"type": "array"}
        if name_lower == "any":
            return {}

        # Check type registry for custom types (Enums, Models)
        if name in self.type_registry:
            type_def = self.type_registry[name]
            kind = type_def.get("kind", "model")
            
            if kind == "enum":
                # Return enum schema with string type and enum values
                values = type_def.get("values", [])
                enum_values = [v.get("value") or v.get("name") for v in values]
                return {
                    "type": "string",
                    "enum": enum_values,
                    "x-enum-type": name,  # Reference to original enum
                }
            else:
                # For models, return a $ref
                return {"$ref": f"#/types/{name}"}

        # Fallback for unknown types (likely imported from external modules)
        return {"type": "object", "description": f"Complex type: {name}"}

    def _handle_union_node(self, slice_node) -> dict:
        """Handles Union[A, B] content."""
        options = []

        # If multiple args, it's a Tuple
        if isinstance(slice_node, ast.Tuple):
            for elt in slice_node.elts:
                options.append(self._node_to_schema(elt))
        else:
            # Single arg Union (rare but valid)
            options.append(self._node_to_schema(slice_node))

        return self._finalize_union(options)

    def _handle_binop_union(self, node: ast.BinOp) -> dict:
        """Handles A | B | C recursively."""
        options = []

        def collect_options(n):
            if isinstance(n, ast.BinOp) and isinstance(n.op, ast.BitOr):
                collect_options(n.left)
                collect_options(n.right)
            else:
                options.append(self._node_to_schema(n))

        collect_options(node)
        return self._finalize_union(options)

    def _finalize_union(self, options: list) -> dict:
        """
        Cleans up a list of schemas into a oneOf or nullable schema.
        e.g. [int, None] -> {type: integer, nullable: true}
        """
        non_null_options = [o for o in options if o.get("type") != "null"]
        has_null = len(non_null_options) < len(options)

        if len(non_null_options) == 1:
            schema = non_null_options[0]
            if has_null:
                schema["nullable"] = True
            return schema

        schema = {"oneOf": non_null_options}
        if has_null:
            schema["nullable"] = True
        return schema

    def _get_name_from_node(self, node) -> str:
        """Helper to get string name from Name or Attribute node."""
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return node.attr
        return ""


if __name__ == "__main__":
    import json

    SAMPLE_CODE = """
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional, Dict

# 1. Enum Definition
class OrderStatus(Enum):
    PENDING = "pending"
    SHIPPED = "shipped"
    DELIVERED = "delivered"

# 2. Dataclass Definition
@dataclass
class UserContext:
    user_id: str
    roles: List[str]
    is_active: bool = True

# 3. Decorators (Metadata)
@auth_required
@logger(level="INFO")
async def process_complex_order(
    order_id: str,
    status: OrderStatus,
    user: UserContext,
    items: List[Dict[str, float]],
    discount: float = 0.0
) -> dict:
    '''
    Processes a high-value order asynchronously.

    Args:
        order_id: The unique order UUID.
        status: Current status of the order.
        user: The user context object.
        items: List of items. Must contain at least 1 item.
        discount: Discount percentage. Must be between 0.0 and 0.5 (50%).

    Returns:
        dict: The processed receipt.
    '''
    # Logic would go here...
    pass
    
def simple_function(a: int, b: str = "default") -> str:
    '''A simple synchronous function.'''
    return "a-b"


"""

    parser = PythonParser(SAMPLE_CODE)
    ir = parser.parse()

    print("✅ AST Parser Test")
    print(json.dumps(ir, indent=2))

def parse_python_function(func) -> dict:
    """
    Helper function to parse a Python function object directly.
    Used by examples.py
    """
    try:
        source = textwrap.dedent(inspect.getsource(func))
        parser = PythonParser(source)
        result = parser.parse()
        if result["operations"]:
            return result["operations"][0]
        if result["types"]:
             return result["types"][0]
        return {}
    except Exception as e:
        logger.error(f"Failed to parse function object: {e}")
        raise

===========================

====================
path: src\parsers\python_parser\__init__.py

# python_parser package

===========================

====================
path: src\parsers\typescript_parser\parser.py

"""
TypeScript Source Code Parser using Tree-Sitter

Parses TypeScript source code (.ts files) into TestSuiteGen IR format.
Uses tree-sitter for robust AST parsing, extracting signatures and type info.
"""

import logging
from typing import List, Dict, Any, Optional
from ...utils.tree_sitter_loader import get_parser

logger = logging.getLogger(__name__)

class TypeScriptParser:
    """
    Tree-Sitter Parser for TypeScript Source Code.
    """

    def __init__(self, source_code: str):
        self.source_code = source_code
        self.parser = get_parser("typescript")

    def parse(self) -> dict:
        """Parse TypeScript source code and return IR-compatible operations."""
        tree = self.parser.parse(bytes(self.source_code, "utf8"))
        root_node = tree.root_node
        
        operations = []
        
        # Traverse for function definitions
        # We look for: function_declaration, arrow_function, method_definition
        for node in self._traverse_tree(root_node):
            if node.type in ["function_declaration", "method_definition"]:
                op = self._parse_node(node)
                if op:
                    operations.append(op)
            elif node.type == "lexical_declaration":
                # Handle const foo = () => {}
                for child in node.children:
                    if child.type == "variable_declarator":
                        name_node = child.child_by_field_name("name")
                        value_node = child.child_by_field_name("value")
                        if value_node and value_node.type == "arrow_function":
                            op = self._parse_arrow_function(name_node, value_node)
                            if op:
                                operations.append(op)

        return {"operations": operations}

    def _traverse_tree(self, node):
        """Pre-order traversal."""
        yield node
        for child in node.children:
            yield from self._traverse_tree(child)

    def _get_text(self, node) -> str:
        """Get source text for a node."""
        if not node:
            return ""
        return self.source_code[node.start_byte:node.end_byte]

    def _parse_node(self, node) -> Optional[dict]:
        """Parse standard function entries."""
        name_node = node.child_by_field_name("name")
        if not name_node:
            return None
        
        func_name = self._get_text(name_node)
        
        # Check async
        is_async = False
        for child in node.children:
            if child.type == "async":
                is_async = True
                break

        # Parameters
        params_node = node.child_by_field_name("parameters")
        properties, required = self._parse_parameters(params_node)
        
        # Return Type
        return_type_node = node.child_by_field_name("return_type") # This is type_annotation
        # In tree-sitter typescript: 
        # return_type field exists for call_signature, but sometimes it captures `: Type` 
        # actually node.child_by_field_name("type") might match the return type annotation
        
        return_schema = {}
        if node.child_by_field_name("type"):
             # The : Type part
             return_annotation = node.child_by_field_name("type")
             # It wraps the actual type
             if return_annotation.children:
                 # Usually [0] is :, [1] is the type
                 real_type = return_annotation.children[-1]
                 return_schema = self._node_to_schema(real_type)
        elif return_type_node:
             # Some nodes use return_type field
             real_type = return_type_node.children[-1] if return_type_node.children else return_type_node
             return_schema = self._node_to_schema(real_type)

        # Get docstring (comments before the node)
        description = self._get_docstring(node)
        
        # Build Body Schema
        body_schema = {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False,
        }

        return {
            "id": func_name,
            "kind": "typescript_function",
            "async": is_async,
            "description": description or f"TypeScript function: {func_name}",
            "metadata": {},
            "inputs": {
                "path": [],
                "query": [],
                "headers": [],
                "body": {
                    "content_type": "application/json",
                    "required": True,
                    "schema": body_schema,
                },
            },
            "outputs": [
                {
                    "status": 200,
                    "content_type": "application/json",
                    "schema": return_schema,
                }
            ],
            "errors": [],
        }

    def _parse_arrow_function(self, name_node, arrow_node) -> dict:
        """Parse arrow functions explicitly."""
        func_name = self._get_text(name_node)
        
        is_async = False
        # Check children for 'async' keyword
        for child in arrow_node.children:
            if child.type == "async":
                is_async = True
        
        params_node = arrow_node.child_by_field_name("parameters")
        properties, required = self._parse_parameters(params_node)
        
        return_schema = {}
        # return_type field
        rt_node = arrow_node.child_by_field_name("return_type")
        if rt_node:
             real_type = rt_node.children[-1]
             return_schema = self._node_to_schema(real_type)

        # Docstring - usually attached to the variable declaration statement
        # We need to look up parent -> parent (lexical decl) -> prev sibling
        parent = arrow_node.parent # variable_declarator
        grandparent = parent.parent # lexical_declaration
        description = self._get_docstring(grandparent)

        body_schema = {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False,
        }
        
        return {
            "id": func_name,
            "kind": "typescript_function",
            "async": is_async,
            "description": description or f"TypeScript arrow function: {func_name}",
            "metadata": {},
            "inputs": {
                "path": [],
                "query": [],
                "headers": [],
                "body": {
                    "content_type": "application/json",
                    "required": True,
                    "schema": body_schema,
                },
            },
            "outputs": [
                {
                    "status": 200,
                    "content_type": "application/json",
                    "schema": return_schema,
                }
            ],
            "errors": [],
        }

    def _parse_parameters(self, params_node):
        """Returns (properties, required_list)"""
        properties = {}
        required = []
        
        if not params_node:
            return properties, required

        for child in params_node.children:
            if child.type in ["required_parameter", "optional_parameter"]:
                name_node = child.child_by_field_name("pattern") # 'pattern' holds identifier
                if not name_node: continue
                
                param_name = self._get_text(name_node)
                
                # Check optional
                is_optional = child.type == "optional_parameter" 
                
                # Type annotation
                type_node = child.child_by_field_name("type")
                schema = {}
                if type_node:
                    # type_node is usually type_annotation -> children[1] is usage
                    if type_node.children:
                        real_type = type_node.children[-1]
                        schema = self._node_to_schema(real_type)
                
                properties[param_name] = schema
                if not is_optional:
                    required.append(param_name)
                    
        return properties, required

    def _node_to_schema(self, type_node) -> dict:
        """Map TS type nodes to JSON schema"""
        if not type_node: return {}
        
        kind = type_node.type
        text = self._get_text(type_node)
        
        if kind == "predefined_type":
            if text == "string": return {"type": "string"}
            if text == "number": return {"type": "number"}
            if text == "boolean": return {"type": "boolean"}
            if text == "any": return {} 
            if text == "void": return {"type": "null"}
            
        if kind == "type_reference":
            # e.g. Promise<T> or Array<T> or User
            name_node = type_node.child_by_field_name("name")
            name = self._get_text(name_node)
            
            if name == "Array":
                args = type_node.child_by_field_name("type_arguments")
                if args and args.children:
                    # filtering checking children for type nodes
                    # standard struct: <, type, >
                    sub_types = [c for c in args.children if c.type not in ["<", ">", ","]]
                    if sub_types:
                        return {"type": "array", "items": self._node_to_schema(sub_types[0])}
            
            if name == "Promise":
                args = type_node.child_by_field_name("type_arguments")
                if args and args.children:
                    sub_types = [c for c in args.children if c.type not in ["<", ">", ","]]
                    if sub_types:
                        return self._node_to_schema(sub_types[0])

            # Fallback for named refs
            return {"type": "object", "description": f"Ref: {name}"}

        if kind == "array_type":
             # T[]
             elem = type_node.children[0]
             return {"type": "array", "items": self._node_to_schema(elem)}

        if kind == "union_type":
            # A | B
            # children: type, |, type
            options = []
            for child in type_node.children:
                if child.type == "|": continue
                options.append(self._node_to_schema(child))
            
            # Filter nulls
            non_null = [o for o in options if o.get("type") != "null"]
            has_null = len(non_null) < len(options)
            
            if len(non_null) == 1:
                s = non_null[0]
                if has_null: s["nullable"] = True
                return s
            
            s = {"oneOf": non_null}
            if has_null: s["nullable"] = True
            return s
            
        if kind == "object_type":
             # { name: string }
             props = {}
             # Iterate members
             for child in type_node.children:
                 if child.type in ["property_signature", "property_signature"]:
                     pname = self._get_text(child.child_by_field_name("name"))
                     ptype = child.child_by_field_name("type")
                     if ptype and ptype.children:
                         pschema = self._node_to_schema(ptype.children[-1])
                         props[pname] = pschema
             return {"type": "object", "properties": props}

        if kind == "tuple_type":
             # [string, number]
             items = []
             for child in type_node.children:
                 if child.type in ["[", "]", ","]: continue
                 items.append(self._node_to_schema(child))
             return {
                 "type": "array", 
                 "prefixItems": items,
                 "minItems": len(items),
                 "maxItems": len(items)
             }

        if kind == "literal_type":
             # "foo" or 123
             text = self._get_text(type_node)
             # Strip quotes
             if text.startswith("'") or text.startswith('"'):
                 return {"const": text[1:-1]}
             # Try number
             try:
                 if "." in text: return {"const": float(text)}
                 return {"const": int(text)}
             except:
                 return {"const": text}

        return {"type": "object", "description": f"TS Type: {kind}"}

    def _get_docstring(self, node) -> str:
        """Extract comments immediately preceding the node."""
        if not node: return ""
        
        # Look at previous siblings
        # Tree-sitter includes comments as nodes if enabled, or we scan raw source.
        # But commonly they are 'comment' nodes in the tree.
        
        # We search backward from node
        prev = node.prev_sibling
        comments = []
        while prev:
             if prev.type == "comment":
                 text = self._get_text(prev)
                 # Clean up // or /* */
                 cleaned = text.strip()
                 if cleaned.startswith("//"): cleaned = cleaned[2:].strip()
                 elif cleaned.startswith("/*"): cleaned = cleaned[2:-2].strip()
                 comments.insert(0, cleaned)
                 prev = prev.prev_sibling
             elif prev.type in ["export", "async"]: 
                 # Skip modifiers to find comments before them
                 prev = prev.prev_sibling
             elif str(prev.type).strip() == "":
                 # Whitespace/Text nodes? 
                 prev = prev.prev_sibling
             else:
                 break
                 
        return "\n".join(comments)

if __name__ == "__main__":
    import json
    code = '''
    /**
     * Greets the user.
     * @param name Name of user
     */
    export function greet(name: string): string { 
        return "Hi" 
    }
    
    // Process items
    const process = (items: number[]): void => {}
    '''
    
    p = TypeScriptParser(code)
    print(json.dumps(p.parse(), indent=2))

===========================

====================
path: src\parsers\typescript_parser\__init__.py


===========================

====================
path: src\testsuite\generator.py

import os
from collections import defaultdict
from typing import List, Dict
from jinja2 import Template
from testsuitegen.src.llm_enhancer.python_enhancer.test_suite_enhancer.enhancer import (
    enhance_code as enhance_code_python,
)
from testsuitegen.src.llm_enhancer.typescript_enhancer.test_suite_enhancer.enhancer import (
    enhance_code as enhance_code_ts,
)
from testsuitegen.src.testsuite.templates import (
    UNIT_TEST_TEMPLATE,
    API_TEST_TEMPLATE,
    OPENAPI_JEST_TEST_TEMPLATE,
    TYPESCRIPT_FUNCTION_TEST_TEMPLATE,
)


class TestSuiteGenerator:
    def __init__(
        self,
        output_dir: str = "generated_tests",
        llm_provider: str = None,
        llm_model: str = None,
    ):
        self.output_dir = output_dir
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        os.makedirs(self.output_dir, exist_ok=True)

    def generate_python_unit_tests(
        self, ir: dict, payloads: List[Dict], module_name: str
    ):
        """
        Generates unit tests for raw Python functions.
        """
        grouped = self._group_by_operation(payloads)

        for op_id, cases in grouped.items():
            # Render with all test cases (Happy Path + Edge Cases)
            template = Template(UNIT_TEST_TEMPLATE)
            code = template.render(
                module_path=module_name,
                function_name=op_id,
                operation_id=op_id,
                test_cases=cases,  # Pass all cases
            )

            self._write_file("unit", f"test_{op_id}.py", code, test_type="unit")

    def generate_typescript_tests(self, ir: dict, payloads: List[Dict]):
        """
        Generates unit tests for TypeScript functions.
        """
        grouped = self._group_by_operation(payloads)
        # Default import path; user will likely need to adjust this relative import
        module_path = "./src/index"

        for op_id, cases in grouped.items():
            template = Template(TYPESCRIPT_FUNCTION_TEST_TEMPLATE)
            code = template.render(
                module_path=module_path,
                function_name=op_id,
                operation_id=op_id,
                test_cases=cases,
            )

            self._write_file(
                "tests_ts", f"{op_id}.test.ts", code, test_type="unit", framework="jest"
            )

    def generate_api_tests(
        self, ir: dict, payloads: List[Dict], base_url: str = "http://localhost:8000"
    ):
        """
        Generates integration tests for HTTP APIs.
        """
        # Ensure base_url is a string and strip trailing slash for consistency
        base_url = str(base_url).rstrip("/")

        grouped = self._group_by_operation(payloads)

        # We need to look up path/method from the IR for each operation
        ops_map = {op["id"]: op for op in ir["operations"]}

        for op_id, cases in grouped.items():
            op_details = ops_map.get(op_id)
            if not op_details:
                continue

            # Extract error information
            errors = op_details.get("errors", [])
            error_codes = [e["status"] for e in errors]
            error_info = []
            for error in errors:
                error_data = {
                    "status": error["status"],
                    "description": error.get("description", "Error"),
                    "schema": error.get("schema"),
                }
                # Create a simplified schema summary for documentation
                if error.get("schema"):
                    error_data["schema_summary"] = self._summarize_schema(
                        error["schema"]
                    )
                error_info.append(error_data)

            # Extract path parameter names from operation inputs
            path_param_names = [
                p["name"] for p in op_details.get("inputs", {}).get("path", [])
            ]

            # Extract query parameter names
            query_param_names = [
                p["name"] for p in op_details.get("inputs", {}).get("query", [])
            ]

            # Extract body property names
            body_props = []
            body = op_details.get("inputs", {}).get("body", {})
            if body and body.get("schema"):
                body_props = list(body["schema"].get("properties", {}).keys())

            # --- PATCH: For GET with path params, ensure test_data_setup creates resource and test uses its ID ---
            method = op_details["method"].upper()
            has_path_params = bool(path_param_names)
            # Only patch for GET, HAPPY_PATH, and path params
            patched_cases = []
            for case in cases:
                patched_case = dict(case)
                if (
                    method == "GET"
                    and has_path_params
                    and case.get("intent", "").upper() == "HAPPY_PATH"
                ):
                    # Patch path_params to use a placeholder for created resource
                    patched_case = dict(case)
                    patched_case["path_params"] = {
                        k: "USE_CREATED_RESOURCE" for k in path_param_names
                    }
                patched_cases.append(patched_case)

            template = Template(API_TEST_TEMPLATE)
            code = template.render(
                base_url=base_url,
                path=op_details["path"],
                method=op_details["method"],
                operation_id=op_id,
                test_cases=patched_cases,
                error_codes=error_codes,
                error_info=error_info,
                path_param_names=path_param_names,
                query_param_names=query_param_names,
                body_props=body_props,
            )

            self._write_file("api", f"test_api_{op_id}.py", code, test_type="api")

    def generate_api_tests_jest(
        self, ir: dict, payloads: List[Dict], base_url: str = "http://localhost:8000"
    ):
        """
        Generates Jest-based integration tests for HTTP APIs.
        Also generates package.json and jest.config.js with TypeScript support.
        """
        # Ensure base_url is a string and strip trailing slash for consistency
        base_url = str(base_url).rstrip("/")

        grouped = self._group_by_operation(payloads)

        ops_map = {op["id"]: op for op in ir["operations"]}

        for op_id, cases in grouped.items():
            op_details = ops_map.get(op_id)
            if not op_details:
                continue

            errors = op_details.get("errors", [])
            error_codes = [e["status"] for e in errors]
            error_info = []
            for error in errors:
                error_data = {
                    "status": error["status"],
                    "description": error.get("description", "Error"),
                    "schema": error.get("schema"),
                }
                if error.get("schema"):
                    error_data["schema_summary"] = self._summarize_schema(
                        error["schema"]
                    )
                error_info.append(error_data)

            template = Template(OPENAPI_JEST_TEST_TEMPLATE)
            code = template.render(
                base_url=base_url,
                path=op_details["path"],
                method=op_details["method"],
                operation_id=op_id,
                test_cases=cases,
                error_codes=error_codes,
                error_info=error_info,
            )

            self._write_file(
                "api_jest",
                f"test_api_{op_id}.test.ts",
                code,
                test_type="api",
                framework="jest",
            )

        # Generate Jest configuration files for TypeScript support
        self._write_jest_config_files()

    def _summarize_schema(self, schema: dict) -> str:
        """
        Create a brief summary of a schema for documentation purposes.
        """
        if not schema:
            return "N/A"

        schema_type = schema.get("type", "object")

        if schema_type == "object":
            props = schema.get("properties", {})
            if props:
                prop_names = list(props.keys())[:3]  # Show first 3 properties
                prop_str = ", ".join(prop_names)
                if len(props) > 3:
                    prop_str += ", ..."
                return f"object with properties: {prop_str}"
            return "object"
        elif schema_type == "array":
            items_type = schema.get("items", {}).get("type", "any")
            return f"array of {items_type}"
        else:
            return schema_type

    def _group_by_operation(self, payloads: List[Dict]) -> Dict[str, List[Dict]]:
        groups = defaultdict(list)
        for p in payloads:
            groups[p["operation_id"]].append(p)
        return groups

    def _write_jest_config_files(self):
        """
        Generate package.json and jest.config.js with TypeScript support for Jest tests.
        These files are written to the api_jest subdirectory.
        """
        import json

        jest_dir = os.path.join(self.output_dir, "api_jest")
        os.makedirs(jest_dir, exist_ok=True)

        # Generate package.json with ts-jest and TypeScript dependencies
        # Uses native fetch (Node.js 18+) so no node-fetch dependency needed
        package_json = {
            "name": "testsuitegen-api-tests",
            "version": "1.0.0",
            "description": "Auto-generated API tests by TestSuiteGen",
            "scripts": {
                "test": "jest --detectOpenHandles",
                "test:verbose": "jest --verbose --detectOpenHandles",
            },
            "devDependencies": {
                "@types/jest": "^29.5.14",
                "@types/node": "^22.10.5",
                "jest": "^29.7.0",
                "ts-jest": "^29.2.5",
                "typescript": "^5.7.2",
            },
        }

        package_json_path = os.path.join(jest_dir, "package.json")
        with open(package_json_path, "w", encoding="utf-8") as f:
            json.dump(package_json, f, indent=2)
        print(f"[OK] Generated: {package_json_path}")

        # Generate jest.config.js with ts-jest preset
        jest_config = """/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  testMatch: ['**/*.test.ts'],
  moduleFileExtensions: ['ts', 'js', 'json'],
  transform: {
    '^.+\\.ts$': 'ts-jest',
  },
  testTimeout: 30000,
  verbose: true,
};
"""
        jest_config_path = os.path.join(jest_dir, "jest.config.js")
        with open(jest_config_path, "w", encoding="utf-8") as f:
            f.write(jest_config)
        print(f"[OK] Generated: {jest_config_path}")

        # Generate tsconfig.json for TypeScript configuration
        tsconfig = {
            "compilerOptions": {
                "target": "ES2020",
                "module": "commonjs",
                "lib": ["ES2020"],
                "strict": True,
                "esModuleInterop": True,
                "skipLibCheck": True,
                "forceConsistentCasingInFileNames": True,
                "resolveJsonModule": True,
                "declaration": False,
                "outDir": "./dist",
                "rootDir": ".",
            },
            "include": ["**/*.ts"],
            "exclude": ["node_modules", "dist"],
        }

        tsconfig_path = os.path.join(jest_dir, "tsconfig.json")
        with open(tsconfig_path, "w", encoding="utf-8") as f:
            json.dump(tsconfig, f, indent=2)
        print(f"[OK] Generated: {tsconfig_path}")

    def _write_file(
        self,
        subdir: str,
        filename: str,
        content: str,
        test_type: str = "api",
        framework: str = "pytest",
    ):
        dir_path = os.path.join(self.output_dir, subdir)
        os.makedirs(dir_path, exist_ok=True)
        filepath = os.path.join(dir_path, filename)

        content_to_write = content
        if self.llm_provider:
            print(
                f"Enhancing code {filename} with LLM using provider: {self.llm_provider}:{self.llm_model or 'default'}..."
            )
            try:
                if framework == "jest":
                    content_to_write = enhance_code_ts(
                        content,
                        provider=self.llm_provider,
                        model=self.llm_model,
                        test_type=test_type,
                    )
                else:
                    content_to_write = enhance_code_python(
                        content,
                        provider=self.llm_provider,
                        model=self.llm_model,
                        test_type=test_type,
                    )
            except Exception as e:
                # Rate limits or API failures: fall back to unenhanced code and disable LLM for subsequent files
                print(
                    f"LLM enhancement skipped: {e}. Writing unenhanced code and disabling LLM for remaining tests."
                )
                self.llm_provider = None

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content_to_write)

        # Format Python files with black after saving, only if syntax is valid
        if filename.endswith(".py"):
            try:
                import subprocess
                import py_compile

                try:
                    py_compile.compile(filepath, doraise=True)
                except py_compile.PyCompileError as ce:
                    print(
                        f"[WARN] Skipping black formatting for {filename}: invalid Python syntax: {ce}"
                    )
                else:
                    subprocess.run(["black", filepath], check=True)
            except Exception as e:
                print(f"[WARN] Could not format {filename} with black: {e}")

        print(f"[OK] Generated: {filepath}")

===========================

====================
path: src\testsuite\templates.py

# suite_gen/templates.py
# Jinja2 Templates for Test Suite Generation

# Template for testing raw Python functions (Unit Tests)
UNIT_TEST_TEMPLATE = """
import pytest
from {{ module_path }} import {{ function_name }}

# Auto-generated by TestSuiteGen
# Operation: {{ operation_id }}

class Test{{ operation_id | replace('_', '') }}:

    @pytest.mark.parametrize("intent, kwargs, expected_status", [
    {%- for case in test_cases %}
        pytest.param(
            "{{ case.intent }}",
            {{ case.payload }},
            {{ case.expected_status }},
            id="{{ case.intent }}"
        ),
    {%- endfor %}
    ])
    def test_{{ operation_id }}_contract(self, intent, kwargs, expected_status):
        \"\"\"
        Contract testing for {{ function_name }}.
        Intent: {intent}
        Expected: {expected_status} (400/422 = Exception)
        \"\"\"
        
        # Negative Tests (Expect Exceptions)
        if expected_status >= 400:
            # We expect TypeError for structural issues, ValueError for constraints, or AttributeError for None access
            with pytest.raises((ValueError, TypeError, AssertionError, AttributeError)):
                {{ function_name }}(**kwargs)
        
        # Happy Path (Expect Return Value)
        else:
            result = {{ function_name }}(**kwargs)
            assert result is not None
"""

# Template for testing HTTP APIs (Integration Tests) - Pytest (Function Based)
API_TEST_TEMPLATE = """
import pytest
import requests
import uuid
import re

BASE_URL = "{{ base_url }}"
ENDPOINT = "{{ path }}"
METHOD = "{{ method }}"

# Operation: {{ operation_id }}
# Error Codes Expected: {{ error_codes | join(', ') }}


@pytest.fixture(scope="module")
def api_client():
    '''Provides a configured API client for testing.'''
    session = requests.Session()
    session.headers.update({"Content-Type": "application/json"})
    yield session
    session.close()


@pytest.fixture(scope="module")
def test_data_setup(api_client):
    '''
    Setup fixture that creates test data before tests and cleans up after.
    This ensures each test run has a clean state.
    '''
    created_resources = []
    
    # === LLM ENHANCEMENT REQUIRED ===
    # REPLACE THIS SECTION with actual test data creation code
    # Analyze the test cases below and create prerequisite resources
    # 
    # Automatically create required resources for GET endpoints with path params and HAPPY_PATH intent
    
    # IMPORTANT: Replace everything from here to PLACEHOLDER_FOR_TEST_DATA_SETUP_CODE with actual code

    # DO NOT MODIFY THE YIELD STATEMENT BELOW - it must return a dict with "created_resources" key
    yield {"created_resources": created_resources}
    
    # Teardown: Clean up all created resources
    for resource in created_resources:
        try:
            api_client.delete(resource["endpoint"])
        except Exception as e:
            print(f"Cleanup warning: Could not delete {resource['type']} {resource['id']}: {e}")

@pytest.mark.parametrize("intent, payload, path_params, expected_status", [{%- for case in test_cases %}
    pytest.param(
        "{{ case.intent }}",
        {{ case.payload }},
        {{ case.path_params if case.path_params else '{}' }},
        {{ case.expected_status }},
        id="{{ case.intent }}"
    ),
{%- endfor %}
])
def test_{{ operation_id }}_contract(api_client, test_data_setup, intent, payload, path_params, expected_status):    
    \"\"\"
    Validates {{ method }} {{ path }} against the OpenAPI contract.
    Intent: {{"{intent}"}}
    Expected Status: {{"{expected_status}"}}
    
    {% if error_info %}
    Error Response Information:
    {%- for error in error_info %}
    - Status {{ error.status }}: {{ error.description }}
      {%- if error.schema %}
      Response Schema: {{ error.schema_summary }}
      {%- endif %}
    {%- endfor %}
    {% endif %}
    \"\"\"
    url = f"{BASE_URL}{ENDPOINT}"
    created_resources = test_data_setup["created_resources"]

    {% raw %}# Robust path parameter replacement
    # Always treat path_params as a dict
    if path_params is None:
      path_params = {}
    if not isinstance(path_params, dict):
      path_params = dict(path_params)

    # Replace path parameters with test data if available
    for param_name in re.findall(r'\{(\w+)\}', url):
      param_value = path_params.get(param_name)
      # Use created test resource ID if available and param_value is not set
      if param_value is None and created_resources:
        param_value = created_resources[0].get('id', 1)
      # Handle INVALID_TYPE for path parameters
      if param_value == "__INVALID_TYPE__":
        param_value = "invalid_string_value"
      # Fallback to a dummy value if still None
      if param_value is None:
        param_value = 1
      url = url.replace(f"{{{param_name}}}", str(param_value))
    
    # Extract query parameters from payload
    query_params = {}
    if payload and METHOD in ("GET", "DELETE"):
        # For GET/DELETE, move payload to query params
        query_params = payload.copy()
        payload = None
    
    # Generate unique data for creation tests to avoid conflicts
    if METHOD in ("POST", "PUT", "PATCH") and payload and intent == "HAPPY_PATH":
        if "username" in payload:
            payload["username"] = f"{payload['username']}_{uuid.uuid4().hex[:8]}"
        if "email" in payload:
            email_parts = payload["email"].split("@")
            payload["email"] = f"{email_parts[0]}_{uuid.uuid4().hex[:8]}@{email_parts[1]}"
    {% endraw %}
    
    response = api_client.request(
        method=METHOD,
        url=url,
        json=payload if payload else None,
        {% raw %}params=query_params if query_params else None,{% endraw %}
        headers={"Content-Type": "application/json"}
    )
    
    {% raw %}assert response.status_code == expected_status, \
        f"Failed Intent: {intent}. Expected {expected_status}, got {response.status_code}. Body: {response.text}"
    
    # Store created resource for cleanup if this is a creation test
    if response.status_code in (200, 201) and METHOD == "POST":
        try:
            resource_data = response.json()
            if isinstance(resource_data, dict):
                res_id = resource_data.get("id")
                if res_id:
                    # Append to the list defined in the fixture (lists are mutable)
                    created_resources.append({
                        "type": "test_resource",
                        "id": res_id,
                        "endpoint": f"{url}/{res_id}"
                    })
        except Exception:
            pass
    {% endraw %}
    
    # Validate error response structure if applicable
    if expected_status >= 400 and response.content:
        try:
            error_body = response.json()
            assert error_body is not None, "Error response should have a JSON body"
            # Additional validation can be added here based on error schema
        except ValueError:
            # Some error responses might not have JSON body
            pass

"""
# Template for testing HTTP APIs (Integration Tests) - Jest
# NOTE: This template intentionally avoids TypeScript-only syntax so it can run
# under a plain Jest setup without ts-jest/babel TypeScript presets.
OPENAPI_JEST_TEST_TEMPLATE = """
// Uses native fetch (Node.js 18+)

describe("{{ method }} {{ path }} ({{ operation_id }})", () => {
  const BASE_URL = "{{ base_url }}";
  const ENDPOINT = "{{ path }}";
  const METHOD = "{{ method }}";

  // Operation: {{ operation_id }}
  // Error Codes Expected: {{ error_codes | join(', ') }}

  const testCases = [
  {% for case in test_cases %}
    {
      intent: "{{ case.intent }}",
      payload: {{ case.payload }},
      pathParams: {{ case.path_params if case.path_params else '{}' }},
      expectedStatus: {{ case.expected_status }},
    },
  {% endfor %}
  ];

  it.each(testCases)("intent: %s", async (testCase) => {
    const { intent, payload, pathParams, expectedStatus } = testCase;

    let url = `${BASE_URL}${ENDPOINT}`;

    if (pathParams) {
      for (const [name, value] of Object.entries(pathParams)) {
        const paramValue =
          value === "__INVALID_TYPE__" ? "invalid_string_value" : value ?? 1;
        url = url.replace(`:${name}`, String(paramValue)).replace(
          `{${name}}`,
          String(paramValue),
        );
      }
    }

    const init: any = {
      method: METHOD,
      headers: { "Content-Type": "application/json" },
    };

    if (payload && (METHOD === "POST" || METHOD === "PUT" || METHOD === "PATCH")) {
      init.body = JSON.stringify(payload);
    }

    const response = await fetch(url, init);

    expect(response.status).toBe(
      expectedStatus,
    );

    if (expectedStatus >= 400) {
      try {
        const body = await response.json();
        expect(body).toBeDefined();
      } catch {
        // Some error responses might not have JSON body
      }
    }
  });
});

export {};
"""

# Template for testing TypeScript functions (Jest)
TYPESCRIPT_FUNCTION_TEST_TEMPLATE = """
import { {{ function_name }} } from "{{ module_path }}";

// Auto-generated by TestSuiteGen
// Operation: {{ operation_id }}

const testCases: any[] = [
{% for case in test_cases %}
  {
    intent: "{{ case.intent }}",
    args: {{ case.payload }},
    expectedStatus: {{ case.expected_status }},
  },
{% endfor %}
];

describe("{{ operation_id }}", () => {
  it.each(testCases)("intent: %s", async (testCase: any) => {
    const { intent, args, expectedStatus } = testCase;

    if (expectedStatus >= 400) {
      await expect(async () => {
        // @ts-ignore
        return await ({{ function_name }})(...Object.values(args));
      }).rejects.toThrow();
    } else {
      // @ts-ignore
      const result = await ({{ function_name }})(...Object.values(args));
      expect(result).toBeDefined();
    }
  });
});
"""

===========================

====================
path: src\testsuite\__init__.py


===========================

====================
path: src\utils\code_extractor.py

import ast
import logging
from typing import Optional
from testsuitegen.src.utils.tree_sitter_loader import get_parser
from tree_sitter import Language, Node
import tree_sitter_typescript

logger = logging.getLogger(__name__)


def extract_relevant_context(
    source_code: str, target_function_name: str, language: str = "python"
) -> str:
    """
    Extracts the target function code and all potential type definitions (Classes, Enums, Interfaces)
    from the source code to provide a focused context for the LLM.

    Args:
        source_code: Full source code.
        target_function_name: Name of the function to extract.
        language: "python" or "typescript".

    Returns:
        A string containing relevant definitions and the target function.
        Returns original source_code if extraction fails or function is not found.
    """
    if language == "python":
        return _extract_python_context(source_code, target_function_name)
    elif language == "typescript":
        return _extract_typescript_context(source_code, target_function_name)
    else:
        logger.warning(
            f"Unsupported language {language} for context extraction. Returning full source."
        )
        return source_code


def _extract_python_context(source_code: str, target_function_name: str) -> str:
    try:
        tree = ast.parse(source_code)
        relevant_nodes = []
        target_found = False
        source_lines = source_code.splitlines(keepends=True)

        for node in tree.body:
            # 1. Keep all Class Definitions (potential types/enums)
            if isinstance(node, ast.ClassDef):
                relevant_nodes.append(node)
            # 2. Keep the Target Function
            elif (
                isinstance(node, ast.FunctionDef) and node.name == target_function_name
            ):
                relevant_nodes.append(node)
                target_found = True
            # 3. Keep Assignments (potential Type Aliases or Constants)
            elif isinstance(node, (ast.Assign, ast.AnnAssign)):
                relevant_nodes.append(node)
            # 4. Keep Imports
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                relevant_nodes.append(node)

        if not target_found:
            logger.warning(
                f"Target function '{target_function_name}' not found in Python source. Returning full source."
            )
            return source_code

        extracted_code = []
        for node in relevant_nodes:
            if hasattr(node, "lineno") and hasattr(node, "end_lineno"):
                start = node.lineno - 1
                end = node.end_lineno
                extracted_code.append("".join(source_lines[start:end]))

        return "\n\n".join(extracted_code)

    except Exception as e:
        logger.error(f"Error parsing Python source: {e}")
        return source_code


# def _extract_typescript_context(source_code: str, target_function_name: str) -> str:
#     try:
#         parser = get_parser("typescript")

#         ts_language = Language(tree_sitter_typescript.language_typescript())

#         tree = parser.parse(bytes(source_code, "utf8"))
#         root_node = tree.root_node

#         # Query to find relevant top-level nodes
#         # We want: interfaces, type aliases, classes, imports, and the specific function/method
#         query_scm = """
#         (interface_declaration) @interface
#         (type_alias_declaration) @type
#         (class_declaration) @class
#         (enum_declaration) @enum
#         (import_statement) @import
#         (function_declaration name: (identifier) @func_name) @func
#         (export_statement
#             declaration: (function_declaration name: (identifier) @exp_func_name) @exp_func
#         )
#         """

#         query = ts_language.query(query_scm)
#         captures = query.captures(root_node)

#         relevant_ranges = []
#         target_found = False

#         # Helper to get code from node
#         def get_node_text(node: Node) -> str:
#             return source_code[node.start_byte : node.end_byte]

#         # First pass: Collect all types/imports, check for target function
#         nodes_to_keep = []

#         for node, capture_name in captures:
#             if capture_name in ["interface", "type", "class", "enum", "import"]:
#                 nodes_to_keep.append(node)

#             elif capture_name == "func_name":
#                 if get_node_text(node) == target_function_name:
#                     # Parent is the function declaration
#                     nodes_to_keep.append(node.parent)
#                     target_found = True

#             elif capture_name == "exp_func_name":
#                 if get_node_text(node) == target_function_name:
#                     # Grandparent is export -> function
#                     nodes_to_keep.append(
#                         node.parent.parent
#                     )  # @exp_func is export_statement
#                     target_found = True

#         if not target_found:
#             # Try searching methods inside classes if not found globally?
#             # For now, simplistic approach: if not found, return full source
#             logger.warning(
#                 f"Target function '{target_function_name}' not found in TS source. Returning full source."
#             )
#             return source_code

#         # De-duplicate nodes and sort by position
#         unique_nodes = sorted(list(set(nodes_to_keep)), key=lambda n: n.start_byte)

#         # Merge overlapping/adjacent ranges?
#         # Ideally just print them separated by newlines

#         extracted_parts = []
#         for node in unique_nodes:
#             extracted_parts.append(get_node_text(node))

#         return "\n\n".join(extracted_parts)

#     except Exception as e:
#         logger.error(f"Error parsing TypeScript source: {e}")
#         return source_code


def _extract_typescript_context(source_code: str, target_function_name: str) -> str:
    try:
        parser = get_parser("typescript")
        tree = parser.parse(bytes(source_code, "utf8"))
        root = tree.root_node

        def node_text(node):
            return source_code[node.start_byte : node.end_byte]

        nodes_to_keep = []
        target_found = False

        # Top-level pass: pick imports, types, classes, enums, top-level functions, exports
        for child in root.children:
            t = child.type

            # Keep imports, interfaces, type aliases, classes, enums
            if t in (
                "import_statement",
                "interface_declaration",
                "type_alias_declaration",
                "class_declaration",
                "enum_declaration",
            ):
                nodes_to_keep.append(child)

            # Top-level function
            elif t == "function_declaration":
                # find identifier child
                id_node = next(
                    (c for c in child.children if c.type == "identifier"), None
                )
                if id_node and node_text(id_node) == target_function_name:
                    nodes_to_keep.append(child)
                    target_found = True

            # exported declarations: could be "export_statement" with a declaration child
            elif t == "export_statement":
                # keep exported types/classes/etc for context
                for named in child.named_children:
                    if named.type in (
                        "interface_declaration",
                        "class_declaration",
                        "enum_declaration",
                        "type_alias_declaration",
                        "function_declaration",
                    ):
                        # if it's a function, check name; otherwise keep it for context
                        if named.type == "function_declaration":
                            id_node = next(
                                (c for c in named.children if c.type == "identifier"),
                                None,
                            )
                            if id_node and node_text(id_node) == target_function_name:
                                nodes_to_keep.append(
                                    child
                                )  # include the whole export_statement
                                target_found = True
                            else:
                                # keep other exported functions/types for context
                                nodes_to_keep.append(named)
                        else:
                            nodes_to_keep.append(named)

        # Second pass: search for the function as a method inside classes; if found, include the class
        for child in root.children:
            if child.type == "class_declaration":
                # class body typically is a child named 'class_body' or has method_definition nodes inside
                for member in child.named_children:
                    # method definition types may vary between grammars: 'method_definition', 'public_field_definition', etc.
                    if member.type in (
                        "method_definition",
                        "function",
                        "method_signature",
                    ):
                        # method name can be 'property_identifier' or 'identifier' or similar
                        name_node = next(
                            (
                                c
                                for c in member.children
                                if c.type in ("property_identifier", "identifier")
                            ),
                            None,
                        )
                        if name_node and node_text(name_node) == target_function_name:
                            nodes_to_keep.append(child)
                            target_found = True
                            break
                if target_found:
                    # no need to search other classes once found (optional)
                    pass

        if not target_found:
            logger.warning(
                f"Target function '{target_function_name}' not found in TS source. Returning full source."
            )
            return source_code

        # Deduplicate by (start_byte, end_byte) and sort by position
        seen_ranges = set()
        unique_nodes = []
        for n in nodes_to_keep:
            key = (n.start_byte, n.end_byte)
            if key not in seen_ranges:
                seen_ranges.add(key)
                unique_nodes.append(n)

        unique_nodes.sort(key=lambda n: n.start_byte)

        # Extract text parts
        extracted_parts = [node_text(n) for n in unique_nodes]

        return "\n\n".join(extracted_parts)

    except Exception as e:
        logger.error(f"Error parsing TypeScript source: {e}")
        return source_code

===========================

====================
path: src\utils\tree_sitter_loader.py

import tree_sitter_typescript
import tree_sitter_python
from tree_sitter import Language, Parser


def get_parser(language_name: str) -> Parser:
    """
    Returns a configured Tree-Sitter parser for the specified language.
    """
    if language_name == "typescript":
        language = Language(tree_sitter_typescript.language_typescript())
    elif language_name == "python":
        language = Language(tree_sitter_python.language())
    else:
        raise ValueError(f"Unsupported language for tree-sitter: {language_name}")

    parser = Parser(language)
    return parser

===========================

